{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __future__ import should always be first\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize     # for common color-scale\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Gymnasium & Minigrid imports\n",
    "import gymnasium as gym  # Correct way to import Gymnasium\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from gymnasium.utils.play import play\n",
    "import pandas as pd\n",
    "# Visualization imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from moviepy import VideoFileClip, TextClip, CompositeVideoClip\n",
    "from moviepy import ImageSequenceClip\n",
    "\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            size=10, \n",
    "            agent_start_pos=(1, 8), \n",
    "            agent_start_dir=0, \n",
    "            max_steps=256, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = (8, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        #create gird\n",
    "        self.grid = Grid(width, height)\n",
    "        #place barrier\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "        #place goal\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "        #place walls\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "        #place agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"find the shortest path\"\n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode = None)\n",
    "#env.reset needed to bypass the step counter function which is otherwise needed but doesn't exist in SimpleEnv\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model State Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "\n",
    "class ModelState:\n",
    "    #tables and arrays\n",
    "    Q_table: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    Pi_a_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    P_s_given_s_a: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    P_s_by_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    allowed_state_idx: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    #scalars\n",
    "    beta: Optional[float] = None\n",
    "    num_actions: Optional[int] = None\n",
    "    num_states: Optional[int] = None\n",
    "\n",
    "    #lists\n",
    "    info_to_go_term_training: List[float] = field(default_factory=list)\n",
    "    pi_analysis_term_training: List[float] = field(default_factory=list)\n",
    "\n",
    "    #list for P_s\n",
    "    positions_directions_list_ps: List[float] = field(default_factory=list)\n",
    "    positions_directions_list_neglogps: List[float] = field(default_factory=list)\n",
    "\n",
    "    #dictionaries\n",
    "    transition_counts: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate state instance, can also import from another file if needed\n",
    "state = ModelState()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SandBox State\n",
    "for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_state = copy.deepcopy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMixin:\n",
    "    def position_to_state_index(self, tuple_position = None): \n",
    "        grid_width = self.env.grid.width -2\n",
    "        if tuple_position is None:\n",
    "            direction = self.env.agent_dir\n",
    "            x, y = self.env.agent_pos\n",
    "\n",
    "        else:\n",
    "            if not isinstance(tuple_position, tuple) or len(tuple_position) != 3:\n",
    "                raise ValueError(f\"Invalid position format: {tuple_position}\")\n",
    "            x, y, direction = tuple_position\n",
    "        return np.int64(((y-1) * grid_width + (x-1)) * 4 + direction) \n",
    "    \n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width-2\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x+1, y+1, direction\n",
    "    \n",
    "    def find_state_indexes(self, env): #took out that in accepts env, bc redundant\n",
    "        \"\"\"Counts all states except walls and barriers\"\"\"\n",
    "        state_indexes_list = []\n",
    "        for x in range(1, env.grid.width-1):\n",
    "            for y in range(1, env.grid.height-1):\n",
    "                if env.grid.get(x, y) is None: #grabs all empty spaces\n",
    "                    for direction in range(4):\n",
    "                        state_index = self.position_to_state_index((x, y, direction))\n",
    "                        state_indexes_list.append(state_index)\n",
    "        return state_indexes_list  \n",
    "    \n",
    "    def next_state_index(self, current_state_idx, action):\n",
    "        # Convert current state index into (x, y, direction)\n",
    "        x, y, direction = self.state_index_to_position(current_state_idx )\n",
    "        \n",
    "        if action == 0:  # Turn left\n",
    "            direction = (direction - 1) % 4\n",
    "        elif action == 1:  # Turn right\n",
    "            direction = (direction + 1) % 4\n",
    "        elif action == 2:  # Move forward\n",
    "            if direction == 0:   # looking right increase x\n",
    "                x += 1\n",
    "            elif direction == 1: # looking down increase y\n",
    "                y += 1\n",
    "            elif direction == 2: # facing left: decrease x\n",
    "                x -= 1\n",
    "            elif direction == 3: # looking up: decrease y\n",
    "                y -= 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Convert the new (x, y, direction) back to a state index.\n",
    "        next_state = self.position_to_state_index((x, y, direction))\n",
    "        if next_state not in self.state.allowed_state_idx:\n",
    "            # If the next state is not allowed, return the current state\n",
    "            return current_state_idx\n",
    "        return next_state\n",
    "    \n",
    "    def find_all_next_states(self):\n",
    "        \"\"\"Find all possible next states for each state and action.\"\"\"\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            for action in range(self.state.num_actions):\n",
    "                next_state = self.next_state_index(state, action)\n",
    "                self.state.P_s_given_s_a[state, action, next_state] = 1\n",
    "            \n",
    "\n",
    "    def find_connected_states(self):\n",
    "        \"\"\"\n",
    "        Build and return a connectivity matrix P_s_by_s where the element at [state, next_state]\n",
    "        is given by the probability from self.Pi_a_s for the action that leads from state to next_state.\n",
    "        \"\"\"\n",
    "        # Reset the connectivity matrix at the beginning.\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states))\n",
    "        # Loop over all states.\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            # Loop over all actions for the state.\n",
    "            for action in range(self.state.num_actions):\n",
    "                # Find the next state: assume a deterministic transition where exactly one entry is 1.\n",
    "                next_state = np.argmax(self.state.P_s_given_s_a[state, action, :])\n",
    "                # Set the connectivity matrix: you might choose to sum if multiple actions lead to the same state.\n",
    "                self.state.P_s_by_s[state, next_state] += self.state.Pi_a_s[state, action]\n",
    "        \n",
    "\n",
    "    def calculate_pi(self):\n",
    "        \"\"\"Calculate the policy from the Q-table.\"\"\"\n",
    "        denominator = np.sum((np.exp(self.state.beta * self.Q_table)), axis = 1, keepdims=True) # e^(beta * Q(s,a)) summed over all actions for each state\n",
    "        numerator = np.exp(self.state.beta * self.Q_table) # e^(beta * Q(s,a))\n",
    "        pi = numerator/denominator\n",
    "        \n",
    "        assert np.all(np.isclose(np.sum(pi, axis=1), np.ones(pi.shape[0]), atol=1e-5)), \"Policy does not sum to 1 for all states.\"\n",
    "        return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Various run functions, simple run, run with collecting stats, and run multiple betas\"\"\"\n",
    "\n",
    "class QTrainer(GridMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        state: ModelState,\n",
    "        learning_rate = 0.9,\n",
    "        discount_factor = 0.9,\n",
    "        epochs = 200,\n",
    "        beta = 5\n",
    "         \n",
    "    ):\n",
    "        self.env = env # MiniGrid environment called from the class Minigrid\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.state.num_actions)) # Q table has goal states in it as well\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.zeros(self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        #these lists and in state to be used for plotting\n",
    "        self.state.info_to_go_term_training = []\n",
    "        self.state.pi_analysis_term_training = []\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "\n",
    "        self.steps = 1\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "            # print(f\"Epoch {epoch}: Starting state: {current_state}\")\n",
    "            # print(f\"Epoch {epoch}: Starting position: {self.state_index_to_position(current_state)}\")\n",
    "\n",
    "            epoch_info_to_go = []\n",
    "            epoch_pi_analysis = []\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "                # print(f\"next_state: {next_state}\")\n",
    "                # print(f\"next  step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "                #reward (-1 for each step thats not the gaol)\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "\n",
    "\n",
    "\n",
    "                #calculate the policy from Q table\n",
    "                self.state.Pi_a_s = self.calculate_pi()\n",
    "\n",
    "                #calculate the P_s \n",
    "                self.find_connected_states() #call this to initialize the self.P(s'|s) matrix\n",
    "                self.Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                self.P_s = np.dot(initial_s .T, self.Ps_s_matrix)   #define P(s)\n",
    "                assert np.isclose(np.sum(self.P_s), 1), f\"Sum of P(s) is not 1: {np.sum(self.P_s)}\"\n",
    "                #calculate the info to go \n",
    "                info_to_go = -np.log(self.P_s[next_state] + 1e-15)  \n",
    "                \n",
    "                #calculate policy log term\n",
    "                rows_of_actions_per_state = []\n",
    "                for s in range(self.state.num_states):\n",
    "                    row = self.state.Pi_a_s[s] * self.P_s[s]\n",
    "                    # print(f'row in Pi:{self.Pi_a_s[s]} ')\n",
    "                    # print(f'row {row}')\n",
    "                    rows_of_actions_per_state.append(row)\n",
    "                self.Pi_a = np.sum(np.asarray(rows_of_actions_per_state), axis=0)\n",
    "                \n",
    "                for a in range(self.state.num_actions):\n",
    "                    pi_analysis = (np.log(self.state.Pi_a_s[next_state, a] + 1e-15) - np.log(self.Pi_a[a]+1e-15))\n",
    "\n",
    "                #add calculated term into the list:\n",
    "                epoch_info_to_go.append(info_to_go)\n",
    "                epoch_pi_analysis.append(pi_analysis)\n",
    "\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                #recaluclate P_s_by_s because its based off Pi_a_s which is based on Q_table\n",
    "                self.find_connected_states()\n",
    "\n",
    "            self.state.info_to_go_term_training.append(epoch_info_to_go)\n",
    "            self.state.pi_analysis_term_training.append(epoch_pi_analysis)\n",
    "\n",
    "\n",
    "        \"\"\" Perform Blahut-Arimoto-like updates on the Probabilistic Q Learning System \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QTrainer(env, state, beta =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently testing, this uses a trained pi\n",
    "class BA_Qtable(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "        learning_rate = 0.9\n",
    "        discount_factor = 0.9\n",
    "        epochs = 50\n",
    "        beta = 5\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.state.num_actions)) # Q table has goal states in it as well\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.zeros(self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\" Perform training using BA updates on the Probabilistic Q Learning System \"\"\"\n",
    "\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 5 #max iterations for the Blahut-Arimoto update\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.steps = 1\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "\n",
    "\n",
    "\n",
    "                #calculate the policy from Q table\n",
    "                self.state.Pi_a_s = self.calculate_pi()\n",
    "\n",
    "                self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                print(f\"Epoch {epoch}: P(s)[s] {P_s[current_state]}\")\n",
    "                print(f\"Epoch {epoch}: P(s)[next_state] {P_s[next_state]}\")\n",
    "\n",
    "              \n",
    "                assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                #######loop of calculations#####\n",
    "                for _ in range(max_ba_iters):\n",
    "                    # self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                    # Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                    # P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                    # assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                       \n",
    "                    \n",
    "                    # pi_a_sums = []\n",
    "                    # for s in range(self.state.num_states):\n",
    "                    #     row = self.state.Pi_a_s[s] * P_s[s]\n",
    "                    #     pi_a_sums.append(row)\n",
    "                    # pi_a = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "                    pi_a = P_s@self.state.Pi_a_s\n",
    "                \n",
    "                    assert np.isclose(np.sum(pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(pi_a)}\"\n",
    "\n",
    "                    #calculate Z\n",
    "                    element_wise_a_by_q_table = pi_a * np.exp(self.state.beta * self.Q_table)\n",
    "                    \n",
    "                    zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                    \n",
    "                    #calculate for policy using partition fuction\n",
    "\n",
    "                    new_Pi_a_s = (pi_a * np.exp(self.state.beta * self.Q_table))/ zeta.reshape(-1,1) \n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "                    \n",
    "                diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "\n",
    "               \n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state.allowed_state_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 3)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.Pi_a_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently testing, this uses a trained pi\n",
    "class FreeEnergy(GridMixin):\n",
    "    def __init__(self, env, state: ModelState, epochs = 200, beta = 0.16):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    " \n",
    "        # self.max_steps = 100\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "      \n",
    "        self.Free_energy_table = np.full((self.state.num_states, self.state.num_actions), 0.0) # Free energy table\n",
    "\n",
    "\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.full((self.state.num_actions), 1/self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "    \"\"\" Perform training using BA updates on the Probabilistic Q Learning System \"\"\"\n",
    "\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 10 #max iterations for the Blahut-Arimoto update\n",
    "        tol = 1e-2  #tolerance for convergence\n",
    "     #tolerance for convergence\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.steps = 1\n",
    "            #self.env.reset()[0] #can reset the environment to start from the beginning\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx) #self.position_to_state_index()\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:  # and self.steps < self.max_steps:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                \n",
    "                assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                \n",
    "                #calculate the Free energy\n",
    "                F_0 = (-np.log(P_s[next_state]+ 1e-15)) - (self.beta * reward)\n",
    "                # print(f\" P_s[current_state]: {P_s[current_state]}\")\n",
    "                # print (f\"P_s[next_state]: {P_s[next_state]}\")\n",
    "                \n",
    "                G_0 = 0\n",
    "\n",
    "                for a in range(self.state.num_actions):\n",
    "                    G_0 += (np.log(self.state.Pi_a_s[next_state, a] + 1e-15) - np.log(self.Pi_a[a]+1e-15) + self.Free_energy_table[next_state, a]) * self.state.Pi_a_s[next_state, a]\n",
    "                        # print(f'G_0: {G_0}')\n",
    "                F_0 += G_0 \n",
    "\n",
    "                self.Free_energy_table[current_state, action] = F_0 #np.min([F_0, 200]) #    #np.min([F_0, 50])\n",
    "                #self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                # print(f\"F_0:{F_0}\")\n",
    "                    # self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                \n",
    "                \n",
    "\n",
    "                #######loop of calculations#####\n",
    "                for iteration in range(max_ba_iters):\n",
    "                  \n",
    "\n",
    "                    self.Pi_a = P_s@self.state.Pi_a_s \n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    assert np.isclose(np.sum(self.Pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(self.Pi_a)}\"\n",
    "\n",
    "                    \n",
    "                    # 1) log π(a)  — shape (A,)\n",
    "                    log_pi_a = np.log(self.Pi_a + 1e-15)\n",
    "\n",
    "                    # 2) log-joint   log π(a) − β F(s,a)  — broadcast to shape (S,A)\n",
    "                    log_joint = log_pi_a  -self.Free_energy_table\n",
    "\n",
    "                    # 3) row-shift: subtract max in each state to keep exp() ≤ 1\n",
    "                    log_joint -= log_joint.max(axis=1, keepdims=True)\n",
    "\n",
    "                    # 4) exponentiate and normalise each row\n",
    "                    new_Pi_a_s = np.exp(log_joint)\n",
    "                    new_Pi_a_s /= new_Pi_a_s.sum(axis=1, keepdims=True)   # Σ_a π(a|s)=1\n",
    "                                       \n",
    "                    #calculate Z, works only for small beta\n",
    "                    # element_wise_a_by_q_table = self.Pi_a * np.exp(-  self.Free_energy_table)\n",
    "                    \n",
    "                    # zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                   \n",
    "                    \n",
    "                    # #calculate for policy using partition fuction\n",
    "\n",
    "                    # new_Pi_a_s = (self.Pi_a * np.exp(-  self.Free_energy_table))/ zeta.reshape(-1,1) \n",
    "                    diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                    # print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                    \n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "                    iteration += 1\n",
    "                    if diff < tol:\n",
    "                        # print(f\"Convergence reached at epoch {epoch} \"f\"after {iteration} BA iteration(s); \"f\"Δ = {diff:.4e}\")\n",
    "                        break\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "                    \n",
    "\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    # print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                # print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode = None)\n",
    "#env.reset needed to bypass the step counter function which is otherwise needed but doesn't exist in SimpleEnv\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "free = FreeEnergy(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "free.train_BA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sandbox_run(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "\n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and self.step_count < 100:  # Prevent infinite loops\n",
    "  \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelState' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msanbox_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241m.\u001b[39mPi_a_s\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelState' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "sanbox_run.state.state.Pi_a_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sandbox_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sanbox_run \u001b[38;5;241m=\u001b[39m Sandbox_run(env, \u001b[43msandbox_state\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sandbox_state' is not defined"
     ]
    }
   ],
   "source": [
    "sanbox_run = Sandbox_run(env, sandbox_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sanbox_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env_human \u001b[38;5;241m=\u001b[39m SimpleEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m env_human\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m \u001b[43msanbox_run\u001b[49m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env_human\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sanbox_run' is not defined"
     ]
    }
   ],
   "source": [
    "env_human = SimpleEnv(render_mode='human')\n",
    "env_human.reset()\n",
    "sanbox_run.env = env_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanbox_run.run_policy_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free.Free_energy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRunner(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "    \n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and self.step_count < 1000:  # Prevent infinite loops\n",
    "  \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = QRunner(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional switch out env to human mode\n",
    "env_human = SimpleEnv(render_mode='human')\n",
    "runner.env = env_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run_policy_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This class can make new calculations, can't change any of the parameters of the class train or run\"\"\"\n",
    "\n",
    "class QAnalyzer(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "        \n",
    "    #get probability of state using time probability\n",
    "    def get_P_st(self, T):\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.find_connected_states() #depends on self.state.Pi_a_s\n",
    "\n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, T)\n",
    "        P_s = np.dot(initial_s.T, Ps_s_matrix)   #define P(s)\n",
    "\n",
    "        assert(np.isclose(np.sum(P_s), 1)), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "\n",
    "        return P_s\n",
    "    \n",
    "    def alt_P_r_s(self, state_probabilities):\n",
    "        \"\"\"Separate from the state index the actual position and direction of the agent.\"\"\"\n",
    "        positions_directions = {}\n",
    "        \n",
    "        \n",
    "        for state_idx, probability in enumerate(state_probabilities):\n",
    "            x, y, direction = self.state_index_to_position(state_idx)\n",
    "            \n",
    "            # Store in a dictionary under 'position' and 'direction'\n",
    "            positions_directions[state_idx] = {\n",
    "                \"position\": (x, y),\n",
    "                \"direction\": direction,\n",
    "                \"probability\": probability\n",
    "            }\n",
    "        return positions_directions\n",
    "\n",
    "    def get_pdl(self, T):\n",
    "        self.state.positions_directions_list_ps = []\n",
    "        for t in range(T):\n",
    "            tmp_probs = self.get_P_st(t) #defined here\n",
    "            pos_dict = self.alt_P_r_s(tmp_probs) #defined here\n",
    "    \n",
    "            self.state.positions_directions_list_ps.append(pos_dict)\n",
    "\n",
    "    def get_neglog_pdl(self, T):\n",
    "        self.state.positions_directions_list_neglogps = []\n",
    "        for t in range(T):\n",
    "            tmp_probs = self.get_P_st(t)\n",
    "            pos_dict = self.alt_P_r_s(-np.log((tmp_probs)+ 1e-15))\n",
    "            self.state.positions_directions_list_neglogps.append(pos_dict)\n",
    "    \n",
    "    \"\"\"Decision through time\"\"\"\n",
    "\n",
    "    def get_Pi_a(self, T):\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.find_connected_states()\n",
    "\n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, T)\n",
    "        P_s = np.dot(initial_s.T, Ps_s_matrix)   #define P(s)\n",
    "        \n",
    "        pi_a_sums = []\n",
    "        for s in range(self.state.num_states):\n",
    "            row = self.state.Pi_a_s[s] * P_s[s]\n",
    "            pi_a_sums.append(row)\n",
    "        pi_a_sums = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "        return pi_a_sums\n",
    "\n",
    "    def get_decision(self, T):\n",
    "        decisions = []\n",
    "        for t in range(T):\n",
    "            #tmp_probs is a list, each item is a vector of state probabilities for each time\n",
    "            #tmp_probs = self.get_P_st(t)\n",
    "            pi_a = self.get_Pi_a(t) #specific time dependent a array\n",
    "    \n",
    "            decision_terms = []\n",
    "            for state in range(self.state.num_states): #pick out a single state probability from all states\n",
    "                decision_term = 0\n",
    "                if state in self.state.allowed_state_idx:\n",
    "                # action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[state])\n",
    "                # next_state = np.argmax(self.state.P_s_given_s_a[state, action])\n",
    "                    for a in range(self.state.num_actions):\n",
    "                        \n",
    "                        decision_term +=((np.log(self.state.Pi_a_s[state , a] + 1e-15) - np.log(pi_a[a]+ 1e-15) ) * self.state.Pi_a_s[state, a])   #this is the decision term for that specific state, it is a sum over all actions\n",
    "                        #gives a decision term value for that specific state\n",
    "                decision_terms.append(decision_term)\n",
    "        \n",
    "            decisions.append(decision_terms)\n",
    "        return decisions\n",
    "    \n",
    "\n",
    "    def get_decision_terms(self, T):\n",
    "        \"\"\"Get decision terms for each time step and store them as position dictionaries.\"\"\"\n",
    "        decision_terms = self.get_decision(T)\n",
    "        self.state.positions_directions_list_decisions = []\n",
    "\n",
    "        for t_decisions in decision_terms:\n",
    "            pos_dict = self.alt_P_r_s(t_decisions)  # <- reuse existing function!\n",
    "            self.state.positions_directions_list_decisions.append(pos_dict)\n",
    "\n",
    "    \"\"\"Information to go delta term\"\"\"\n",
    "\n",
    "    def get_info_to_go_delta(self, initial_state, time, state, action):\n",
    "        #this function is state dependent\n",
    "        \n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, time)\n",
    "        P_s = np.dot(initial_state.T, Ps_s_matrix)   #define P(s)\n",
    "        \n",
    "        pi_a_sums = []\n",
    "        for s in range(self.state.num_states):\n",
    "            row = self.state.Pi_a_s[s] * P_s[s]\n",
    "            pi_a_sums.append(row)\n",
    "        pi_a_sums = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "        delta = ((- np.log(P_s[state] + 1e-10)) + ((np.log(self.state.Pi_a_s[state, action] +1e-10))- (np.log(pi_a_sums[action]) + 1e-10)))\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def walk(self):\n",
    "        \n",
    "        self.info_to_go_delta_term = 0  # Initialize information delta term\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 1  # Track steps to prevent infinite loops\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.info_state_calculation = []\n",
    "        while not done and self.step_count < 100:  # Prevent infinite loops\n",
    "            \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            self.info_to_go_delta_term = self.get_info_to_go_delta(initial_state = initial_s, time = self.step_count, state = current_state, action = action)  # Calculate information delta for the current state\n",
    "            self.info_state_calculation.append(self.info_to_go_delta_term)\n",
    "\n",
    "\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = QAnalyzer(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.imshow(np.linalg.matrix_power(analyze.state.P_s_by_s.T, 1), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_pdl(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_neglog_pdl(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_decision_terms(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(23.36283519544392),\n",
       " np.float64(2.2281135735996593),\n",
       " np.float64(3.4240366307621204),\n",
       " np.float64(2.469144228790274),\n",
       " np.float64(3.162571052573356),\n",
       " np.float64(2.4729996061341284),\n",
       " np.float64(3.337824095527396),\n",
       " np.float64(2.6667178266697746),\n",
       " np.float64(3.6375450375935436),\n",
       " np.float64(3.1316212948475974),\n",
       " np.float64(3.8328375384853848),\n",
       " np.float64(3.365582732331605),\n",
       " np.float64(4.709062803586622),\n",
       " np.float64(4.157136611627501),\n",
       " np.float64(3.5868187176801447),\n",
       " np.float64(3.4085826320905714),\n",
       " np.float64(4.661011597556888),\n",
       " np.float64(5.964803685737869),\n",
       " np.float64(6.395607336913935),\n",
       " np.float64(6.216012134541505),\n",
       " np.float64(6.232995954386063),\n",
       " np.float64(5.038758887875328),\n",
       " np.float64(7.503087283327312),\n",
       " np.float64(8.55753948786707),\n",
       " np.float64(5.851741437669209),\n",
       " np.float64(5.497714592984658),\n",
       " np.float64(4.6189644966376875),\n",
       " np.float64(5.784610987384084),\n",
       " np.float64(4.669588260348677),\n",
       " np.float64(3.996961596136783),\n",
       " np.float64(3.9938427979259536),\n",
       " np.float64(3.9336778964817136),\n",
       " np.float64(5.444937085418527),\n",
       " np.float64(3.9108750568330075),\n",
       " np.float64(3.907814275029814),\n",
       " np.float64(3.8951858717937733),\n",
       " np.float64(0.9612651709734401)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze.info_state_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_directions_list_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2305404279225594)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(2* np.log(7/3) - 14)/ 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.7323422468571682)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.log(2 * 0.15/ 0.7 * np.sqrt(2 * np.pi))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plots Heatmaps of the Process's Various Variables\n",
    "Pass in variables from the run class as objects of this class\n",
    "This class cannot make any changes to the variables, it can only plot them\"\"\"\n",
    "\n",
    "\n",
    "class QPlotter(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    def plot_all_heatmaps(self, pos_dir_list):\n",
    "        \"\"\"\n",
    "        Plot a heatmap for each step of the simulation using the data stored \n",
    "        in self.positions_directions_list. Each heatmap shows the probability (from P_s)\n",
    "        at the corresponding x, y positions, and prints the sum of all probabilities.\n",
    "        The color bar range is standardized across all plots using the global maximum value.\n",
    "        \"\"\"\n",
    "        # First, determine the grid size across all steps\n",
    "        all_positions = []\n",
    "        for pos_dict in pos_dir_list:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "        \n",
    "        # Determine grid dimensions\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        # Create a heatmap for each step using the standardized color range.\n",
    "        for step, pos_dict in enumerate(pos_dir_list):\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "\n",
    "            total_probability = grid.sum()\n",
    "            print(f\"Step {step}: Total probability = {total_probability:.4f}\")\n",
    "\n",
    "            # Plot the heatmap: set vmin to 0 and vmax to the computed global maximum.\n",
    "            plt.figure()\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step} \")\n",
    "            \n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"(P(s)\")\n",
    "            plt.show()\n",
    "\n",
    "    def plot_all_heatmaps_by_direction(self, pos_dir_list, value_key=\"probability\", annotate=True):\n",
    "        \"\"\"\n",
    "        Plot a heatmap for each direction across all time steps.\n",
    "        Optionally annotate each grid cell with its value.\n",
    "        \"\"\"\n",
    "        # Determine grid dimensions\n",
    "        all_positions = []\n",
    "        for pos_dict in pos_dir_list:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        # Direction-first plotting\n",
    "        for d in range(4):\n",
    "            for step, pos_dict in enumerate(pos_dir_list):\n",
    "                grid = np.zeros((max_y + 2, max_x + 2))\n",
    "\n",
    "                for entry in pos_dict.values():\n",
    "                    x, y = entry[\"position\"]\n",
    "                    direction = entry[\"direction\"]\n",
    "                    if direction != d:\n",
    "                        continue\n",
    "                    val = entry[value_key]\n",
    "                    grid[y, x] += val\n",
    "\n",
    "                total = grid.sum()\n",
    "                print(f\"Direction {d}, Step {step}: Total {value_key} = {total:.4f}\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                im = plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "                plt.title(f\"Decision Complexity | Dir {d}, Step {step}\")\n",
    "                plt.xlabel(\"X position\")\n",
    "                plt.ylabel(\"Y position\")\n",
    "                plt.colorbar(im, label=\"Weighted Decision Complexity\")\n",
    "\n",
    "                if annotate:\n",
    "                    for y in range(grid.shape[0]):\n",
    "                        for x in range(grid.shape[1]):\n",
    "                            val = grid[y, x]\n",
    "                            if val > 0:  # only annotate non-zero\n",
    "                                plt.text(x, y, f\"{val:.2f}\", ha='center', va='center', color='blue', fontsize=8)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "    def plot_all_heatmaps_by_direction(\n",
    "        self,\n",
    "        pos_dir_list,\n",
    "        value_key=\"probability\",\n",
    "        annotate=True,\n",
    "        *,\n",
    "        save=False,\n",
    "        out_dir=\"heatmaps\",\n",
    "        file_prefix=\"dir\",\n",
    "        fmt=\"pdf\",\n",
    "        dpi=150,\n",
    "):\n",
    "\n",
    "        # ------------------------------------------------------------------ #\n",
    "        # 1.  Determine grid size\n",
    "        # ------------------------------------------------------------------ #\n",
    "        max_x = max(entry[\"position\"][0] for pos in pos_dir_list for entry in pos.values())\n",
    "        max_y = max(entry[\"position\"][1] for pos in pos_dir_list for entry in pos.values())\n",
    "\n",
    "        # ------------------------------------------------------------------ #\n",
    "        # 2.  Prepare output directory if saving\n",
    "        # ------------------------------------------------------------------ #\n",
    "        if save:\n",
    "            out_dir = Path(out_dir)\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ------------------------------------------------------------------ #\n",
    "        # 3.  Loop over directions\n",
    "        # ------------------------------------------------------------------ #\n",
    "        for d in range(4):\n",
    "            # If we’re saving to PDF, open it once per direction\n",
    "            pdf = None\n",
    "            if save and fmt.lower() == \"pdf\":\n",
    "                pdf_path = out_dir / f\"{file_prefix}{d}.{fmt}\"\n",
    "                pdf = PdfPages(pdf_path)\n",
    "\n",
    "            for step, pos_dict in enumerate(pos_dir_list):\n",
    "                # Build grid\n",
    "                grid = np.zeros((max_y + 2, max_x + 2))\n",
    "                for entry in pos_dict.values():\n",
    "                    x, y = entry[\"position\"]\n",
    "                    direction = entry[\"direction\"]\n",
    "                    if direction == d:\n",
    "                        grid[y, x] += entry[value_key]\n",
    "\n",
    "                # Create figure\n",
    "                fig, ax = plt.subplots(figsize=(6, 5))\n",
    "                im = ax.imshow(grid, origin=\"upper\", cmap=\"hot\", interpolation=\"nearest\")\n",
    "                ax.set(\n",
    "                    title=f\"Decision Complexity | Dir {d} • Step {step}\",\n",
    "                    xlabel=\"X position\",\n",
    "                    ylabel=\"Y position\",\n",
    "                )\n",
    "                # fig.colorbar(im, ax=ax, label=\"Weighted \" + value_key.title())\n",
    "\n",
    "                if annotate:\n",
    "                    for y in range(grid.shape[0]):\n",
    "                        for x in range(grid.shape[1]):\n",
    "                            if grid[y, x] > 0:\n",
    "                                ax.text(x, y, f\"{grid[y, x]:.2f}\",\n",
    "                                        ha=\"center\", va=\"center\", color=\"blue\", fontsize=8)\n",
    "\n",
    "                # ── display ────────────────────────────────────────────────\n",
    "                if not save:\n",
    "                    plt.show()\n",
    "\n",
    "                # ── save ───────────────────────────────────────────────────\n",
    "                if save:\n",
    "                    if fmt.lower() == \"pdf\":\n",
    "                        pdf.savefig(fig)\n",
    "                    else:\n",
    "                        fname = f\"{file_prefix}{d}_step{step:03d}.{fmt}\"\n",
    "                        fig.savefig(out_dir / fname, dpi=dpi)\n",
    "\n",
    "                plt.close(fig)\n",
    "\n",
    "            # Close multipage PDF\n",
    "            if pdf is not None:\n",
    "                pdf.close()\n",
    "                print(f\"Saved {pdf_path}\")\n",
    "\n",
    "    def plot_save_all_heatmaps(self, folder_name):\n",
    "        \"\"\"\n",
    "        Generate and save a heatmap for each simulation step.\n",
    "        \n",
    "        Parameters:\n",
    "        - folder_name: The directory in which images are saved.\n",
    "        \n",
    "        Each heatmap is saved as a PNG file (e.g. \"heatmap_000.png\").\n",
    "        \"\"\"\n",
    "        # Ensure the folder exists\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "        \n",
    "        # Determine grid dimensions from all positions\n",
    "        all_positions = []\n",
    "        for pos_dict in self.state.positions_directions_list_ps:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "        \n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        fig_size = (6, 6)\n",
    "        \n",
    "        # Generate and save a heatmap for each step\n",
    "        for step, pos_dict in enumerate(self.state.positions_directions_list_ps):\n",
    "            # Initialize a grid (with zeros) of appropriate size.\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            \n",
    "            # Populate the grid by adding probabilities\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "            \n",
    "            # Calculate the total probability for this heatmap\n",
    "            total_probability = grid.sum()\n",
    "           \n",
    "            \n",
    "            # Create the figure for the heatmap\n",
    "            plt.figure(figsize = fig_size)\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step} \")\n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"-log(P(s)\")\n",
    "            \n",
    "            # Build the file path; file names are zero-padded for proper sorting.\n",
    "            file_path = os.path.join(folder_name, f\"heatmap_{step:03d}.png\")\n",
    "            plt.savefig(file_path)\n",
    "            #plt.close()  # Close the figure to free memory\n",
    "            \n",
    "    \"\"\"Plotter and saver, change the dictionary depending on the use case\"\"\"\n",
    "    def plot_or_save_all_heatmaps(self, save=False, folder_name=None):\n",
    "        all_positions = []\n",
    "        for pos_dict in self.state.positions_directions_list_ps:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        if save and folder_name and not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        for step, pos_dict in enumerate(self.state.positions_directions_list_ps):\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step}\")\n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"P(s)\")\n",
    "\n",
    "            if save:\n",
    "                file_path = os.path.join(folder_name, f\"heatmap_{step:03d}.png\")\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldplotter = QPlotter(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or save individual PNGs\n",
    "oldplotter.plot_all_heatmaps_by_direction(\n",
    "    pos_dir_list= state.positions_directions_list_decisions ,\n",
    "    save=True,\n",
    "    out_dir=\"heatmaps_png\",\n",
    "    file_prefix=\"dir\",\n",
    "    fmt=\"png\",           # dir0_step000.png, dir0_step001.png, ...\n",
    "    dpi=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldplotter.plot_all_heatmaps_by_direction(pos_dir_list = state.positions_directions_list_decisions, value_key=\"probability\", annotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldplotter.plot_all_heatmaps(pos_dir_list = state.positions_directions_list_decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlotter(GridMixin):\n",
    "    \"\"\"Read-only visualisations of position-probability dictionaries.\"\"\"\n",
    "\n",
    "    def __init__(self, env, state: ModelState,\n",
    "                 slide_bg=\"#f7f5ee\",  dpi=100):\n",
    "        self.env, self.state = env, state\n",
    "        self.slide_bg = slide_bg\n",
    "        self.dpi      = dpi\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  INTERNAL HELPERS\n",
    "    # ------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _grid_extent(pos_dir_list):\n",
    "        all_xy = [entry[\"position\"]\n",
    "                  for step in pos_dir_list\n",
    "                  for entry in step.values()]\n",
    "        max_x = max(p[0] for p in all_xy)\n",
    "        max_y = max(p[1] for p in all_xy)\n",
    "        return max_x + 1, max_y + 1            # +1 so index == max is shown\n",
    "\n",
    "    def _make_figure(self, widescreen=False, figsize_px=960):\n",
    "        \"\"\"\n",
    "        widescreen = False ➜ square dpi×dpi\n",
    "        widescreen =  True ➜ 16:9, face colour = slide bg\n",
    "        \"\"\"\n",
    "        if widescreen:\n",
    "            w, h = 16, 9\n",
    "        else:\n",
    "            w, h = 6, 6\n",
    "        fig = plt.figure(figsize=(w, h), dpi=self.dpi,\n",
    "                         facecolor=self.slide_bg if widescreen else None)\n",
    "        return fig\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  HEAT-MAPS  (your original routines, trimmed a bit)\n",
    "    # ------------------------------------------------------------------\n",
    "    def plot_save_all_heatmaps(self,pos_dir_list, folder=None, widescreen=False):\n",
    "        \"\"\"\n",
    "        When *folder* is None ➜ just plot; otherwise save PNGs there.\n",
    "        Set *widescreen=True* to export at 16:9 with the slide background.\n",
    "        \"\"\"\n",
    "        folder = Path(folder) if folder else None\n",
    "        if folder:\n",
    "            folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        max_x, max_y = self._grid_extent(pos_dir_list)\n",
    "\n",
    "        for step, pos_dict in enumerate(pos_dir_list):\n",
    "            grid = np.zeros((max_y, max_x))\n",
    "            for ent in pos_dict.values():\n",
    "                x, y = ent[\"position\"]\n",
    "                grid[y, x] += ent[\"probability\"]\n",
    "\n",
    "            fig = self._make_figure(widescreen)\n",
    "            ax  = fig.add_subplot(111)\n",
    "            im  = ax.imshow(grid, origin=\"upper\", cmap=\"hot\",\n",
    "                            interpolation=\"nearest\")\n",
    "            ax.set(title=f\"Information Gained by Environment: Step {step}\",\n",
    "                   xlabel=\"X position\", ylabel=\"Y position\")\n",
    "            fig.colorbar(im, ax=ax, label=\"-log P(s)\")\n",
    "\n",
    "            if folder:\n",
    "                fig.savefig(folder / f\"heatmap_{step:03d}.png\",\n",
    "                            bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  NEW ❶  —  SCATTER-PLOT MAKER\n",
    "    # ------------------------------------------------------------------\n",
    "    def plot_scatter(self, pos_dir_list, value_key=\"probability\",\n",
    "                     cmap=\"viridis\", widescreen=False, annotate=False):\n",
    "        \"\"\"\n",
    "        Scatter of (x, y) positions coloured by *value_key* (probability,\n",
    "        decision complexity, …).  *pos_dir_list* can be one step or many.\n",
    "        If many, points from later steps are plotted on top of earlier ones.\n",
    "        \"\"\"\n",
    "        max_x, max_y = self._grid_extent(pos_dir_list)\n",
    "        vmax = max(ent[value_key]\n",
    "                   for step in pos_dir_list\n",
    "                   for ent  in step.values())\n",
    "\n",
    "        fig = self._make_figure(widescreen)\n",
    "        ax  = fig.add_subplot(111)\n",
    "        norm = Normalize(vmin=0, vmax=vmax)\n",
    "\n",
    "        for step, pos_dict in enumerate(pos_dir_list):\n",
    "            xs, ys, cs = [], [], []\n",
    "            for ent in pos_dict.values():\n",
    "                xs.append(ent[\"position\"][0])\n",
    "                ys.append(ent[\"position\"][1])\n",
    "                cs.append(ent[value_key])\n",
    "            ax.scatter(xs, ys, c=cs, cmap=cmap, norm=norm,\n",
    "                       s=80, edgecolors=\"k\", linewidths=0.5,\n",
    "                       label=f\"step {step}\")\n",
    "\n",
    "            if annotate:\n",
    "                for x, y, c in zip(xs, ys, cs):\n",
    "                    ax.text(x, y, f\"{c:.2f}\", ha=\"center\", va=\"center\",\n",
    "                            fontsize=7, color=\"white\")\n",
    "\n",
    "        ax.set(xlim=(-.5, max_x-.5), ylim=(-.5, max_y-.5),\n",
    "               xlabel=\"X position\", ylabel=\"Y position\",\n",
    "               title=\"Position scatter coloured by \" + value_key)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                     ax=ax, label=value_key)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  NEW ❷  —  SCATTER-PLOT SAVER (BATCH)\n",
    "    # ------------------------------------------------------------------\n",
    "    def save_scatter_sequence(self, folder, value_key=\"probability\",\n",
    "                              cmap=\"viridis\", widescreen=False):\n",
    "        folder = Path(folder)\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        max_x, max_y = self._grid_extent(self.state.positions_directions_list_ps)\n",
    "        vmax = max(ent[value_key]\n",
    "                   for step in self.state.positions_directions_list_ps\n",
    "                   for ent  in step.values())\n",
    "\n",
    "        norm = Normalize(vmin=0, vmax=vmax)\n",
    "\n",
    "        for step, pos_dict in enumerate(self.state.positions_directions_list_ps):\n",
    "            fig = self._make_figure(widescreen)\n",
    "            ax  = fig.add_subplot(111)\n",
    "            xs, ys, cs = [], [], []\n",
    "            for ent in pos_dict.values():\n",
    "                xs.append(ent[\"position\"][0])\n",
    "                ys.append(ent[\"position\"][1])\n",
    "                cs.append(ent[value_key])\n",
    "            ax.scatter(xs, ys, c=cs, cmap=cmap, norm=norm,\n",
    "                       s=80, edgecolors=\"k\", linewidths=0.5)\n",
    "            ax.set(xlim=(-.5, max_x-.5), ylim=(-.5, max_y-.5),\n",
    "                   xlabel=\"X position\", ylabel=\"Y position\",\n",
    "                   title=f\"{value_key} | step {step}\")\n",
    "            ax.set_aspect(\"equal\")\n",
    "            fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                         ax=ax, label=value_key)\n",
    "\n",
    "            fig.savefig(folder / f\"scatter_{step:03d}.png\",\n",
    "                        bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = QPlotter(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_save_all_heatmaps(pos_dir_list= state.positions_directions_list_decisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_all_heatmaps(state.positions_directions_list_decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_all_heatmaps(state.positions_directions_list_neglogps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QPlotter.__init__() got an unexpected keyword argument 'slide_bg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[439], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43mQPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslide_bg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#f7f5ee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: QPlotter.__init__() got an unexpected keyword argument 'slide_bg'"
     ]
    }
   ],
   "source": [
    "plotter = QPlotter(env, state, slide_bg=\"#f7f5ee\", dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QVideo():\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "\n",
    "    def clear_folder(folder_name):\n",
    "        files = glob.glob(os.path.join(folder_name, \"*.png\"))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    def create_video_from_images(image_folder, output_file, fps=2):\n",
    "        \"\"\"\n",
    "        Create a video from a sequence of images saved in a folder.\n",
    "        \n",
    "        Parameters:\n",
    "        - image_folder: Folder where the heatmap images are stored.\n",
    "        - output_file: Path for the output video file (e.g., \"simulation.mp4\").\n",
    "        - fps: Frames per second for the output video.\n",
    "        \"\"\"\n",
    "        # Get a sorted list of image files from the folder\n",
    "        image_pattern = os.path.join(image_folder, \"heatmap_*.png\")\n",
    "        image_files = sorted(glob.glob(image_pattern))\n",
    "        \n",
    "        # Create a clip from the sequence of images\n",
    "        clip = ImageSequenceClip(image_files, fps=fps)\n",
    "        \n",
    "        # Write the video file\n",
    "        clip.write_videofile(output_file, codec='libx264', audio_codec=\"aac\" )\n",
    "    @staticmethod\n",
    "    def create_direction_videos(\n",
    "        image_folder: str | Path,\n",
    "        out_folder: str | Path = \"videos\",\n",
    "        file_prefix: str = \"dir\",   # matches  dir0_step000.png, …\n",
    "        fps: int = 2,\n",
    "        codec: str = \"libx264\",\n",
    "        audio_codec: str | None = \"aac\"\n",
    "                ):\n",
    "   \n",
    "        image_folder = Path(image_folder)\n",
    "        out_folder   = Path(out_folder)\n",
    "        out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for d in range(4):\n",
    "            pattern = image_folder / f\"{file_prefix}{d}_step*.png\"\n",
    "            frame_files = sorted(glob.glob(str(pattern)))\n",
    "\n",
    "            if not frame_files:\n",
    "                print(f\"  No frames found for direction {d} → skipping.\")\n",
    "                continue\n",
    "\n",
    "            clip = ImageSequenceClip(frame_files, fps=fps)\n",
    "            out_path = out_folder / f\"{file_prefix}{d}.mp4\"\n",
    "            clip.write_videofile(out_path.as_posix(),\n",
    "                                codec=codec,\n",
    "                                audio_codec=audio_codec)\n",
    "                                \n",
    "                                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = QVideo(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video heatmap_videos/dir0.mp4.\n",
      "MoviePy - Writing video heatmap_videos/dir0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready heatmap_videos/dir0.mp4\n",
      "MoviePy - Building video heatmap_videos/dir1.mp4.\n",
      "MoviePy - Writing video heatmap_videos/dir1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready heatmap_videos/dir1.mp4\n",
      "MoviePy - Building video heatmap_videos/dir2.mp4.\n",
      "MoviePy - Writing video heatmap_videos/dir2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready heatmap_videos/dir2.mp4\n",
      "MoviePy - Building video heatmap_videos/dir3.mp4.\n",
      "MoviePy - Writing video heatmap_videos/dir3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready heatmap_videos/dir3.mp4\n"
     ]
    }
   ],
   "source": [
    "video.create_direction_videos(\n",
    "    image_folder=\"heatmaps_png\",   # where dir0_step000.png, dir1_step000.png, … live\n",
    "    out_folder=\"heatmap_videos\",   # output folder for MP4s\n",
    "    file_prefix=\"dir\",             # matches your naming convention\n",
    "    fps=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This instantiates an env, creates a model state, and runs the training process, for multiple beta\n",
    "\"\"\"\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, env_class):\n",
    "        self.env_class = env_class\n",
    "\n",
    "    def run_mulitple_betas(self, beta_values, runs_per_beta = 10, epochs = 500, plot = False, video = False):\n",
    "        \"\"\"\n",
    "        Run the training function for mulitple betas, save the policies for each beta, and then\n",
    "        use the trained policies to run the test_policy function and collect the length of\n",
    "        the path taken, then plot.\n",
    "\n",
    "        args: \n",
    "        beta_values: list of beta values\n",
    "        runs_per_beta: number of runs per beta value\n",
    "        epochs: int\n",
    "        summary plot: bool for specific plots\n",
    "        video: bool\n",
    "\n",
    "        returns:\n",
    "            dict: mapping from beta -> average step count\n",
    "        \"\"\"\n",
    "        \n",
    "        betas = np.arange(0, 10, 0.1)\n",
    "        step_count_per_beta = {}\n",
    "\n",
    "        for beta in beta_values:\n",
    "            # Make a FRESH instance for each beta\n",
    "            env = self.env_class()\n",
    "            model = QModel(env)\n",
    "            state = ModelState(beta=beta)\n",
    "            trainer = QTrainer(env, state)\n",
    "\n",
    "\n",
    "            # Train from scratch inside that instance\n",
    "            model_for_beta.train(epochs=500)\n",
    "\n",
    "            # Evaluate it runs_per_beta times\n",
    "            step_counts = []\n",
    "            for _ in range(runs_per_beta):\n",
    "                model_for_beta.env.reset()\n",
    "                model_for_beta.run_policy_2()\n",
    "                step_counts.append(model_for_beta.step_count)\n",
    "\n",
    "            step_count_per_beta[beta] = np.mean(step_counts)\n",
    "\n",
    "        return step_count_per_beta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
