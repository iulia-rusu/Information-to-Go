{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __future__ import should always be first\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Gymnasium & Minigrid imports\n",
    "import gymnasium as gym  # Correct way to import Gymnasium\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from gymnasium.utils.play import play\n",
    "import pandas as pd\n",
    "# Visualization imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from moviepy import VideoFileClip, TextClip, CompositeVideoClip\n",
    "from moviepy import ImageSequenceClip\n",
    "\n",
    "from typing import Optional, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            size=10, \n",
    "            agent_start_pos=(1, 8), \n",
    "            agent_start_dir=0, \n",
    "            max_steps=256, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = (8, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        #create gird\n",
    "        self.grid = Grid(width, height)\n",
    "        #place barrier\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "        #place goal\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "        #place walls\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "        #place agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"find the shortest path\"\n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode = None)\n",
    "#env.reset needed to bypass the step counter function which is otherwise needed but doesn't exist in SimpleEnv\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model State Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "\n",
    "class ModelState:\n",
    "    #tables and arrays\n",
    "    Q_table: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    Pi_a_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    P_s_given_s_a: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    P_s_by_s: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    allowed_state_idx: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    #scalars\n",
    "    beta: Optional[float] = None\n",
    "    num_actions: Optional[int] = None\n",
    "    num_states: Optional[int] = None\n",
    "\n",
    "    #lists\n",
    "    info_to_go_term_training: List[float] = field(default_factory=list)\n",
    "    pi_analysis_term_training: List[float] = field(default_factory=list)\n",
    "\n",
    "    #list for P_s\n",
    "    positions_directions_list_ps: List[float] = field(default_factory=list)\n",
    "    positions_directions_list_neglogps: List[float] = field(default_factory=list)\n",
    "\n",
    "    #dictionaries\n",
    "    transition_counts: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate state instance, can also import from another file if needed\n",
    "state = ModelState()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SandBox State\n",
    "for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_state = copy.deepcopy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMixin:\n",
    "    def position_to_state_index(self, tuple_position = None): \n",
    "        grid_width = self.env.grid.width -2\n",
    "        if tuple_position is None:\n",
    "            direction = self.env.agent_dir\n",
    "            x, y = self.env.agent_pos\n",
    "\n",
    "        else:\n",
    "            if not isinstance(tuple_position, tuple) or len(tuple_position) != 3:\n",
    "                raise ValueError(f\"Invalid position format: {tuple_position}\")\n",
    "            x, y, direction = tuple_position\n",
    "        return np.int64(((y-1) * grid_width + (x-1)) * 4 + direction) \n",
    "    \n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width-2\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x+1, y+1, direction\n",
    "    \n",
    "    def find_state_indexes(self, env): #took out that in accepts env, bc redundant\n",
    "        \"\"\"Counts all states except walls and barriers\"\"\"\n",
    "        state_indexes_list = []\n",
    "        for x in range(1, env.grid.width-1):\n",
    "            for y in range(1, env.grid.height-1):\n",
    "                if env.grid.get(x, y) is None: #grabs all empty spaces\n",
    "                    for direction in range(4):\n",
    "                        state_index = self.position_to_state_index((x, y, direction))\n",
    "                        state_indexes_list.append(state_index)\n",
    "        return state_indexes_list  \n",
    "    \n",
    "    def next_state_index(self, current_state_idx, action):\n",
    "        # Convert current state index into (x, y, direction)\n",
    "        x, y, direction = self.state_index_to_position(current_state_idx )\n",
    "        \n",
    "        if action == 0:  # Turn left\n",
    "            direction = (direction - 1) % 4\n",
    "        elif action == 1:  # Turn right\n",
    "            direction = (direction + 1) % 4\n",
    "        elif action == 2:  # Move forward\n",
    "            if direction == 0:   # looking right increase x\n",
    "                x += 1\n",
    "            elif direction == 1: # looking down increase y\n",
    "                y += 1\n",
    "            elif direction == 2: # facing left: decrease x\n",
    "                x -= 1\n",
    "            elif direction == 3: # looking up: decrease y\n",
    "                y -= 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Convert the new (x, y, direction) back to a state index.\n",
    "        next_state = self.position_to_state_index((x, y, direction))\n",
    "        if next_state not in self.state.allowed_state_idx:\n",
    "            # If the next state is not allowed, return the current state\n",
    "            return current_state_idx\n",
    "        return next_state\n",
    "    \n",
    "    def find_all_next_states(self):\n",
    "        \"\"\"Find all possible next states for each state and action.\"\"\"\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            for action in range(self.state.num_actions):\n",
    "                next_state = self.next_state_index(state, action)\n",
    "                self.state.P_s_given_s_a[state, action, next_state] = 1\n",
    "            \n",
    "\n",
    "    def find_connected_states(self):\n",
    "        \"\"\"\n",
    "        Build and return a connectivity matrix P_s_by_s where the element at [state, next_state]\n",
    "        is given by the probability from self.Pi_a_s for the action that leads from state to next_state.\n",
    "        \"\"\"\n",
    "        # Reset the connectivity matrix at the beginning.\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states))\n",
    "        # Loop over all states.\n",
    "        for state in self.state.allowed_state_idx:\n",
    "            # Loop over all actions for the state.\n",
    "            for action in range(self.state.num_actions):\n",
    "                # Find the next state: assume a deterministic transition where exactly one entry is 1.\n",
    "                next_state = np.argmax(self.state.P_s_given_s_a[state, action, :])\n",
    "                # Set the connectivity matrix: you might choose to sum if multiple actions lead to the same state.\n",
    "                self.state.P_s_by_s[state, next_state] += self.state.Pi_a_s[state, action]\n",
    "        \n",
    "\n",
    "    def calculate_pi(self):\n",
    "        \"\"\"Calculate the policy from the Q-table.\"\"\"\n",
    "        denominator = np.sum((np.exp(self.state.beta * self.Q_table)), axis = 1, keepdims=True) # e^(beta * Q(s,a)) summed over all actions for each state\n",
    "        numerator = np.exp(self.state.beta * self.Q_table) # e^(beta * Q(s,a))\n",
    "        pi = numerator/denominator\n",
    "        \n",
    "        assert np.all(np.isclose(np.sum(pi, axis=1), np.ones(pi.shape[0]), atol=1e-5)), \"Policy does not sum to 1 for all states.\"\n",
    "        return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Various run functions, simple run, run with collecting stats, and run multiple betas\"\"\"\n",
    "\n",
    "class QTrainer(GridMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        state: ModelState,\n",
    "        learning_rate = 0.9,\n",
    "        discount_factor = 0.9,\n",
    "        epochs = 200,\n",
    "        beta = 5\n",
    "         \n",
    "    ):\n",
    "        self.env = env # MiniGrid environment called from the class Minigrid\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.state.num_actions)) # Q table has goal states in it as well\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.zeros(self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        #these lists and in state to be used for plotting\n",
    "        self.state.info_to_go_term_training = []\n",
    "        self.state.pi_analysis_term_training = []\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "\n",
    "        self.steps = 1\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "            # print(f\"Epoch {epoch}: Starting state: {current_state}\")\n",
    "            # print(f\"Epoch {epoch}: Starting position: {self.state_index_to_position(current_state)}\")\n",
    "\n",
    "            epoch_info_to_go = []\n",
    "            epoch_pi_analysis = []\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "                # print(f\"next_state: {next_state}\")\n",
    "                # print(f\"next  step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "                #reward (-1 for each step thats not the gaol)\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "\n",
    "\n",
    "\n",
    "                #calculate the policy from Q table\n",
    "                self.state.Pi_a_s = self.calculate_pi()\n",
    "\n",
    "                #calculate the P_s \n",
    "                self.find_connected_states() #call this to initialize the self.P(s'|s) matrix\n",
    "                self.Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                self.P_s = np.dot(initial_s .T, self.Ps_s_matrix)   #define P(s)\n",
    "                assert np.isclose(np.sum(self.P_s), 1), f\"Sum of P(s) is not 1: {np.sum(self.P_s)}\"\n",
    "                #calculate the info to go \n",
    "                info_to_go = -np.log(self.P_s[next_state] + 1e-15)  \n",
    "                \n",
    "                #calculate policy log term\n",
    "                rows_of_actions_per_state = []\n",
    "                for s in range(self.state.num_states):\n",
    "                    row = self.state.Pi_a_s[s] * self.P_s[s]\n",
    "                    # print(f'row in Pi:{self.Pi_a_s[s]} ')\n",
    "                    # print(f'row {row}')\n",
    "                    rows_of_actions_per_state.append(row)\n",
    "                self.Pi_a = np.sum(np.asarray(rows_of_actions_per_state), axis=0)\n",
    "                \n",
    "                for a in range(self.state.num_actions):\n",
    "                    pi_analysis = (np.log(self.state.Pi_a_s[next_state, a] + 1e-15) - np.log(self.Pi_a[a]+1e-15))\n",
    "\n",
    "                #add calculated term into the list:\n",
    "                epoch_info_to_go.append(info_to_go)\n",
    "                epoch_pi_analysis.append(pi_analysis)\n",
    "\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                #recaluclate P_s_by_s because its based off Pi_a_s which is based on Q_table\n",
    "                self.find_connected_states()\n",
    "\n",
    "            self.state.info_to_go_term_training.append(epoch_info_to_go)\n",
    "            self.state.pi_analysis_term_training.append(epoch_pi_analysis)\n",
    "\n",
    "\n",
    "        \"\"\" Perform Blahut-Arimoto-like updates on the Probabilistic Q Learning System \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QTrainer(env, state, beta =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.find_connected_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15182ad10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.imshow(trainer.state.P_s_by_s, cmap='hot')#, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently testing, this uses a trained pi\n",
    "class Sandbox(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "        learning_rate = 0.9\n",
    "        discount_factor = 0.9\n",
    "        epochs = 50\n",
    "        beta = 5\n",
    "        self.epochs = epochs\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.state.num_actions)) # Q table has goal states in it as well\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.zeros(self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\" Perform training using BA updates on the Probabilistic Q Learning System \"\"\"\n",
    "\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 5 #max iterations for the Blahut-Arimoto update\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.steps = 1\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "\n",
    "\n",
    "\n",
    "                #calculate the policy from Q table\n",
    "                self.state.Pi_a_s = self.calculate_pi()\n",
    "\n",
    "                self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                print(f\"Epoch {epoch}: P(s)[s] {P_s[current_state]}\")\n",
    "                print(f\"Epoch {epoch}: P(s)[next_state] {P_s[next_state]}\")\n",
    "\n",
    "              \n",
    "                assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                #######loop of calculations#####\n",
    "                for _ in range(max_ba_iters):\n",
    "                    # self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                    # Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                    # P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                    # assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                       \n",
    "                    \n",
    "                    # pi_a_sums = []\n",
    "                    # for s in range(self.state.num_states):\n",
    "                    #     row = self.state.Pi_a_s[s] * P_s[s]\n",
    "                    #     pi_a_sums.append(row)\n",
    "                    # pi_a = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "                    pi_a = P_s@self.state.Pi_a_s\n",
    "                \n",
    "                    assert np.isclose(np.sum(pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(pi_a)}\"\n",
    "\n",
    "                    #calculate Z\n",
    "                    element_wise_a_by_q_table = pi_a * np.exp(self.state.beta * self.Q_table)\n",
    "                    \n",
    "                    zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                    \n",
    "                    #calculate for policy using partition fuction\n",
    "\n",
    "                    new_Pi_a_s = (pi_a * np.exp(self.state.beta * self.Q_table))/ zeta.reshape(-1,1) \n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "                    \n",
    "                diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "\n",
    "               \n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently testing, this uses a trained pi\n",
    "class FreeEnergySandbox(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    " \n",
    " \n",
    "        beta = 5\n",
    "        self.epochs = 500\n",
    "        self.state = state\n",
    "        self.state.allowed_state_idx = self.find_state_indexes(env)\n",
    "        self.state.num_states = ((env.width -2) * (env.height -2) *4)\n",
    "        self.state.num_actions = env.action_space.n\n",
    "        #shapes defined for first time\n",
    "        self.state.P_s_given_s_a = np.zeros((self.state.num_states, self.state.num_actions, self.state.num_states)) # P(s'|s,a) matrix\n",
    "        self.state.P_s_by_s = np.zeros((self.state.num_states, self.state.num_states)) # P(s'|s) matrix\n",
    "        self.state.Pi_a_s = np.full((self.state.num_states, self.state.num_actions), 1/self.state.num_actions) # pi(s,a) matrix\n",
    "      \n",
    "        self.Free_energy_table = np.full((self.state.num_states, self.state.num_actions), 100) # Free energy table\n",
    "\n",
    "\n",
    "        \n",
    "        self.state.beta = beta\n",
    "\n",
    "        \n",
    "        self.Pi_a = np.full((self.state.num_actions), 1/self.state.num_actions)\n",
    "        self.P_s = np.zeros(self.state.num_states)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\" Perform training using BA updates on the Probabilistic Q Learning System \"\"\"\n",
    "\n",
    "\n",
    "    def train_BA(self):\n",
    "        max_ba_iters = 10 #max iterations for the Blahut-Arimoto update\n",
    "     #tolerance for convergence\n",
    "        self.find_all_next_states() #generates self.P(s'|s,a) matrix\n",
    "        \n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.steps = 1\n",
    "            current_state = np.random.choice(self.state.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "\n",
    "            initial_s = np.zeros(self.state.num_states)\n",
    "            initial_s[current_state] = 1\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #get the action using pi\n",
    "                action = np.random.choice(np.arange(self.state.num_actions), p = self.state.Pi_a_s[current_state])\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                self.find_connected_states() #P_s fixed by old Pi_a_s\n",
    "                Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, self.steps)\n",
    "                P_s = np.dot(initial_s.T, Ps_s_matrix) #define P(s)\n",
    "                \n",
    "                assert np.isclose(np.sum(P_s), 1), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "                \n",
    "                #calculate the Free energy\n",
    "                F_0 = (-np.log(P_s[next_state]+ 1e-15)) - (self.state.beta * reward)\n",
    "                # print(f\" P_s[current_state]: {P_s[current_state]}\")\n",
    "                # print (f\"P_s[next_state]: {P_s[next_state]}\")\n",
    "                \n",
    "                G_0 = 0\n",
    "\n",
    "                for a in range(self.state.num_actions):\n",
    "                    G_0 += (np.log(self.state.Pi_a_s[next_state, a] + 1e-15) - np.log(self.Pi_a[a]+1e-15) + self.Free_energy_table[next_state, a]) * self.state.Pi_a_s[next_state, a]\n",
    "                        # print(f'G_0: {G_0}')\n",
    "                F_0 += G_0 \n",
    "\n",
    "                self.Free_energy_table[current_state, action] = F_0 #np.min([F_0, 200]) #    #np.min([F_0, 50])\n",
    "                #self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                # print(f\"F_0:{F_0}\")\n",
    "                    # self.Free_energy_table[self.state.allowed_state_idx] = self.Free_energy_table[self.state.allowed_state_idx] - np.mean(self.Free_energy_table[self.state.allowed_state_idx])\n",
    "                \n",
    "                \n",
    "\n",
    "                #######loop of calculations#####\n",
    "                for _ in range(max_ba_iters):\n",
    "\n",
    "\n",
    "                    self.Pi_a = P_s@self.state.Pi_a_s \n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    assert np.isclose(np.sum(self.Pi_a), 1), f\"Sum of Pi_a is not 1: {np.sum(self.Pi_a)}\"\n",
    "\n",
    "                    \n",
    "                    # 1) log π(a)  — shape (A,)\n",
    "                    log_pi_a = np.log(self.Pi_a + 1e-15)\n",
    "\n",
    "                    # 2) log-joint   log π(a) − β F(s,a)  — broadcast to shape (S,A)\n",
    "                    log_joint = log_pi_a - self.state.beta * self.Free_energy_table\n",
    "\n",
    "                    # 3) row-shift: subtract max in each state to keep exp() ≤ 1\n",
    "                    log_joint -= log_joint.max(axis=1, keepdims=True)\n",
    "\n",
    "                    # 4) exponentiate and normalise each row\n",
    "                    new_Pi_a_s = np.exp(log_joint)\n",
    "                    new_Pi_a_s /= new_Pi_a_s.sum(axis=1, keepdims=True)   # Σ_a π(a|s)=1\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #calculate Z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # element_wise_a_by_q_table = self.Pi_a * np.exp(-  self.Free_energy_table)\n",
    "                    \n",
    "                    # zeta = np.sum((element_wise_a_by_q_table), axis = 1)\n",
    "                   \n",
    "                    \n",
    "                    # #calculate for policy using partition fuction\n",
    "\n",
    "                    # new_Pi_a_s = (self.Pi_a * np.exp(-  self.Free_energy_table))/ zeta.reshape(-1,1) \n",
    "                    diff = np.linalg.norm(new_Pi_a_s - self.state.Pi_a_s, ord='fro')\n",
    "                    # print(f\"Epoch {epoch}: Frobenius norm difference: {diff:.4f}\") # with percent\n",
    "                    self.state.Pi_a_s = new_Pi_a_s\n",
    "\n",
    "                      # Frobenius norm for matrix difference\n",
    "                    \n",
    "\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    # print(f\"Goal reached at state {next_state} in epoch {epoch} after {self.steps} steps.\")\n",
    "                    break\n",
    "                # assert(False)\n",
    "                current_state = next_state\n",
    "                # print(f\"Epoch {epoch}: Step {self.steps}, Current state: {current_state}\")\n",
    "                self.steps += 1\n",
    "\n",
    "               \n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sandbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start the class with the sandboxed state\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sandbox \u001b[38;5;241m=\u001b[39m \u001b[43mSandbox\u001b[49m(env, sandbox_state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sandbox' is not defined"
     ]
    }
   ],
   "source": [
    "# start the class with the sandboxed state\n",
    "sandbox = Sandbox(env, sandbox_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode = None)\n",
    "#env.reset needed to bypass the step counter function which is otherwise needed but doesn't exist in SimpleEnv\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional switch out env to human mode\n",
    "env_human = SimpleEnv(render_mode='human')\n",
    "env_human.reset()\n",
    "sandbox.env = env_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox.train_BA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "free =FreeEnergySandbox(env, sandbox_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "free.train_BA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandbox.step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sandbox.info_state_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sandbox_run(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        state: ModelState\n",
    "\n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and self.step_count < 100:  # Prevent infinite loops\n",
    "  \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelState' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msanbox_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241m.\u001b[39mPi_a_s\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelState' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "sanbox_run.state.state.Pi_a_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanbox_run = Sandbox_run(env, sandbox_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.29499819e-115, 6.26072268e-183, 1.00000000e+000],\n",
       "       [9.35762297e-014, 1.00000000e+000, 9.96473301e-062],\n",
       "       [1.00000000e+000, 2.86251858e-020, 1.13797987e-035],\n",
       "       [2.48891883e-198, 1.00000000e+000, 8.69485652e-164],\n",
       "       [2.48891883e-213, 2.49772757e-137, 1.00000000e+000],\n",
       "       [7.64060659e-144, 1.80485139e-050, 1.00000000e+000],\n",
       "       [6.30511676e-016, 1.00000000e+000, 4.23337159e-079],\n",
       "       [2.85242279e-096, 9.99999807e-001, 1.92874948e-007],\n",
       "       [1.05365183e-306, 5.46287446e-285, 1.00000000e+000],\n",
       "       [8.19401262e-040, 1.00000000e+000, 1.38389653e-072],\n",
       "       [3.48110684e-072, 3.04823495e-098, 1.00000000e+000],\n",
       "       [0.00000000e+000, 1.00000000e+000, 7.09945017e-279],\n",
       "       [1.67110597e-291, 4.49212861e-248, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.35762297e-014, 5.46287446e-255],\n",
       "       [1.00000000e+000, 3.70695639e-120, 2.19487851e-057],\n",
       "       [1.05365183e-291, 1.00000000e+000, 3.23455268e-207],\n",
       "       [5.83788690e-272, 2.84236370e-187, 1.00000000e+000],\n",
       "       [1.80485139e-035, 1.00000000e+000, 5.85854824e-166],\n",
       "       [4.24835426e-033, 4.23337159e-109, 1.00000000e+000],\n",
       "       [1.47889751e-089, 5.90009054e-044, 1.00000000e+000],\n",
       "       [3.93353725e-274, 4.80050182e-235, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.37901594e-163, 0.00000000e+000],\n",
       "       [8.19401262e-040, 1.00000000e+000, 7.17509597e-051],\n",
       "       [1.68891188e-063, 4.24835426e-033, 1.00000000e+000],\n",
       "       [2.85242334e-111, 1.79848622e-126, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.96473301e-077, 2.04664112e-146],\n",
       "       [1.00000000e+000, 5.87928270e-105, 2.33727929e-120],\n",
       "       [1.38879439e-011, 1.00000000e+000, 3.97544974e-016],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.20327817e-265, 1.00000000e+000, 8.16511481e-101],\n",
       "       [1.00000000e+000, 8.16511481e-116, 3.46883002e-118],\n",
       "       [3.26901736e-009, 2.33727928e-150, 9.99999997e-001],\n",
       "       [3.97544974e-031, 1.00000000e+000, 3.97544974e-016],\n",
       "       [5.87928270e-120, 2.19487851e-087, 1.00000000e+000],\n",
       "       [2.65977679e-200, 3.96142952e-122, 1.00000000e+000],\n",
       "       [1.00000000e+000, 5.16642063e-055, 2.19487851e-057],\n",
       "       [1.92874985e-037, 1.57484639e-152, 1.00000000e+000],\n",
       "       [6.69050538e-170, 8.13631891e-207, 1.00000000e+000],\n",
       "       [4.23337159e-109, 1.21609930e-052, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.32462145e-090, 4.52398179e-066],\n",
       "       [0.00000000e+000, 5.83788690e-272, 1.00000000e+000],\n",
       "       [1.47889751e-089, 1.57484639e-152, 1.00000000e+000],\n",
       "       [2.50656748e-046, 1.00000000e+000, 2.83233954e-233],\n",
       "       [1.00000000e+000, 1.13797987e-050, 1.91516960e-159],\n",
       "       [1.06487866e-078, 1.48413159e-013, 1.00000000e+000],\n",
       "       [9.99999999e-001, 1.38879438e-011, 1.29958142e-009],\n",
       "       [2.86251858e-035, 5.16642063e-070, 1.00000000e+000],\n",
       "       [2.20264658e-011, 1.47889751e-089, 1.00000000e+000],\n",
       "       [7.14979157e-157, 2.50656748e-061, 1.00000000e+000],\n",
       "       [2.05388455e-100, 2.19487851e-087, 1.00000000e+000],\n",
       "       [2.50656748e-061, 2.65977679e-200, 1.00000000e+000],\n",
       "       [1.47889751e-074, 1.00000000e+000, 1.06487866e-048],\n",
       "       [9.89457172e-229, 1.00000000e+000, 5.87928270e-090],\n",
       "       [1.37901594e-178, 1.91516960e-189, 1.00000000e+000],\n",
       "       [3.23455268e-222, 1.00000000e+000, 4.52398179e-066],\n",
       "       [1.00000000e+000, 2.86251858e-020, 4.18873988e-307],\n",
       "       [8.75651076e-027, 1.00000000e+000, 3.97544974e-016],\n",
       "       [9.99971376e-001, 1.38385691e-087, 2.86243664e-005],\n",
       "       [1.47889751e-074, 1.00000000e+000, 2.05388455e-070],\n",
       "       [4.21844176e-170, 1.00000000e+000, 2.05388455e-070],\n",
       "       [2.05388455e-100, 5.85854824e-196, 1.00000000e+000],\n",
       "       [1.21181048e-128, 8.19401262e-055, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.92959040e-153, 9.92959040e-138],\n",
       "       [1.29958143e-039, 8.75651076e-042, 1.00000000e+000],\n",
       "       [1.48413159e-013, 1.68891188e-063, 1.00000000e+000],\n",
       "       [8.75651076e-042, 8.19401262e-055, 1.00000000e+000],\n",
       "       [4.23325041e-094, 9.99971376e-001, 2.86243664e-005],\n",
       "       [1.00000000e+000, 7.17509597e-066, 6.28288051e-077],\n",
       "       [5.90009054e-029, 1.00000000e+000, 7.17509597e-051],\n",
       "       [1.29958143e-024, 1.00000000e+000, 3.72007598e-029],\n",
       "       [1.21181048e-113, 1.00000000e+000, 7.17509597e-051],\n",
       "       [1.68891188e-048, 1.00000000e+000, 3.25748853e-055],\n",
       "       [5.52108228e-057, 1.68891188e-063, 1.00000000e+000],\n",
       "       [5.16642063e-070, 1.92874985e-037, 1.00000000e+000],\n",
       "       [1.38879439e-026, 1.29958143e-039, 1.00000000e+000],\n",
       "       [1.00000000e+000, 8.75651076e-027, 6.71418429e-064],\n",
       "       [1.92194773e-098, 1.00000000e+000, 8.75651076e-012],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [5.00000000e-001, 5.00000000e-001, 3.11932150e-229],\n",
       "       [1.00000000e+000, 1.06487866e-063, 1.46848465e-211],\n",
       "       [4.53999298e-020, 1.47368188e-165, 1.00000000e+000],\n",
       "       [9.96473301e-077, 1.00000000e+000, 2.66919022e-094],\n",
       "       [1.92194773e-113, 5.50161108e-133, 1.00000000e+000],\n",
       "       [6.13304409e-001, 7.45839062e-038, 3.86695591e-001],\n",
       "       [8.69485652e-179, 1.00000000e+000, 2.50656748e-031],\n",
       "       [3.97544974e-046, 5.16642063e-070, 1.00000000e+000],\n",
       "       [2.49772757e-122, 1.00000000e+000, 3.03748474e-144],\n",
       "       [1.00000000e+000, 2.19487851e-072, 4.52398179e-066],\n",
       "       [1.06112315e-139, 1.00000000e+000, 2.19487851e-057],\n",
       "       [3.48110684e-072, 1.80485139e-050, 1.00000000e+000],\n",
       "       [1.00000000e+000, 2.18713783e-148, 0.00000000e+000],\n",
       "       [1.00000000e+000, 2.18713783e-148, 1.68295560e-109],\n",
       "       [2.33727929e-135, 1.00000000e+000, 8.69485652e-164],\n",
       "       [3.68085585e-287, 2.66919022e-124, 1.00000000e+000],\n",
       "       [3.05902227e-007, 9.99999694e-001, 8.75650808e-012],\n",
       "       [2.67863696e-033, 1.00000000e+000, 1.13797987e-035],\n",
       "       [0.00000000e+000, 1.46330584e-317, 1.00000000e+000],\n",
       "       [2.66919022e-124, 9.35762297e-029, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.78582315e-263, 5.87928270e-090],\n",
       "       [1.00000000e+000, 1.38389653e-087, 1.13797987e-035],\n",
       "       [5.90009054e-044, 2.67863696e-048, 1.00000000e+000],\n",
       "       [1.48413159e-013, 4.53999298e-020, 1.00000000e+000],\n",
       "       [1.00000000e+000, 2.86251858e-020, 3.70695639e-105],\n",
       "       [1.38389653e-102, 5.87928270e-120, 1.00000000e+000],\n",
       "       [1.21181048e-128, 2.66919022e-124, 1.00000000e+000],\n",
       "       [1.00000000e+000, 2.19487851e-072, 2.05388455e-070],\n",
       "       [7.14979155e-157, 3.26901736e-009, 9.99999997e-001],\n",
       "       [3.25748853e-070, 1.00000000e+000, 2.19487851e-057],\n",
       "       [1.00000000e+000, 3.69388307e-196, 1.38389653e-072],\n",
       "       [1.00000000e+000, 8.72562919e-103, 1.79848622e-096],\n",
       "       [1.00000000e+000, 3.24600035e-146, 4.23337159e-079],\n",
       "       [6.73794373e-018, 4.85164960e-007, 9.99999515e-001],\n",
       "       [3.03748474e-174, 2.50656748e-061, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.05738089e-215, 5.90009054e-014],\n",
       "       [3.97544974e-046, 2.67863696e-048, 1.00000000e+000],\n",
       "       [8.19401262e-055, 2.67863696e-048, 1.00000000e+000],\n",
       "       [4.81749166e-159, 9.89457172e-244, 1.00000000e+000],\n",
       "       [4.23337158e-094, 9.99999999e-001, 1.29958142e-009],\n",
       "       [1.00000000e+000, 2.04664112e-161, 7.17509597e-051],\n",
       "       [1.68891188e-048, 1.00000000e+000, 3.25748853e-055],\n",
       "       [2.85242334e-096, 9.99999999e-001, 1.29958142e-009],\n",
       "       [2.04664112e-176, 1.06487866e-078, 1.00000000e+000],\n",
       "       [1.00000000e+000, 2.67863696e-033, 2.33727929e-120],\n",
       "       [3.48110684e-072, 2.49772757e-137, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.38389653e-087, 2.50656748e-031],\n",
       "       [0.00000000e+000, 0.00000000e+000, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.96473301e-077, 9.92959040e-138],\n",
       "       [1.00000000e+000, 3.04823495e-083, 3.25748853e-055],\n",
       "       [1.00000000e+000, 1.68891188e-048, 7.17509597e-051],\n",
       "       [1.00000000e+000, 3.48110684e-057, 1.92194773e-083],\n",
       "       [3.70695639e-120, 1.00000000e+000, 3.24600035e-131],\n",
       "       [1.29958142e-024, 9.99999999e-001, 1.29958142e-009],\n",
       "       [4.81749166e-159, 1.79214350e-202, 1.00000000e+000],\n",
       "       [1.00000000e+000, 3.72007598e-044, 3.48110684e-042],\n",
       "       [1.00000000e+000, 4.52398179e-081, 3.94745875e-168],\n",
       "       [1.47889751e-074, 1.00000000e+000, 1.29499819e-085],\n",
       "       [1.13396656e-141, 3.05902321e-022, 1.00000000e+000],\n",
       "       [1.00000000e+000, 3.23455268e-222, 1.67702032e-185],\n",
       "       [3.05902321e-022, 8.16511481e-131, 1.00000000e+000],\n",
       "       [2.05388455e-100, 1.38879439e-026, 1.00000000e+000],\n",
       "       [4.52398179e-096, 5.14820022e-146, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.92194773e-098, 8.75651076e-012],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e-015, 1.00000000e-015, 1.00000000e+000],\n",
       "       [4.50802707e-157, 1.00000000e+000, 2.19487851e-057],\n",
       "       [1.00000000e+000, 7.66764807e-053, 3.72007598e-029],\n",
       "       [1.57484639e-152, 1.58042006e-076, 1.00000000e+000],\n",
       "       [4.24835426e-033, 2.19487851e-087, 1.00000000e+000],\n",
       "       [1.38389653e-102, 4.83454164e-083, 1.00000000e+000],\n",
       "       [4.52398179e-081, 1.00000000e+000, 3.48110684e-042],\n",
       "       [1.00000000e+000, 1.29958143e-024, 3.97544974e-016],\n",
       "       [3.05902227e-007, 9.99999694e-001, 1.06487833e-048],\n",
       "       [7.64060659e-129, 1.00000000e+000, 1.29043112e-161],\n",
       "       [1.00000000e+000, 4.81749166e-144, 2.50656748e-031],\n",
       "       [1.21609930e-037, 1.00000000e+000, 9.32462145e-075],\n",
       "       [1.00000000e+000, 3.48110684e-057, 2.50656748e-031],\n",
       "       [5.14820022e-131, 1.00000000e+000, 5.50161108e-103],\n",
       "       [9.99954602e-001, 4.53978687e-005, 6.28259528e-077],\n",
       "       [1.00000000e+000, 7.17509597e-066, 1.12996740e-187],\n",
       "       [2.06115362e-024, 2.66919022e-124, 1.00000000e+000],\n",
       "       [4.24835426e-033, 2.86251858e-035, 1.00000000e+000],\n",
       "       [1.00000000e+000, 4.24835426e-018, 2.66919022e-094],\n",
       "       [1.00000000e+000, 1.57484639e-137, 3.48110684e-042],\n",
       "       [3.01600450e-098, 1.05734813e-002, 9.89426519e-001],\n",
       "       [1.00000000e+000, 9.96473301e-077, 3.96142952e-092],\n",
       "       [1.21181048e-128, 3.03748474e-174, 1.00000000e+000],\n",
       "       [3.26901736e-009, 3.97544972e-046, 9.99999997e-001],\n",
       "       [3.25748853e-070, 1.00000000e+000, 2.19487851e-057],\n",
       "       [6.71418429e-079, 1.00000000e+000, 9.96473301e-062],\n",
       "       [7.66764807e-068, 3.70695639e-135, 1.00000000e+000],\n",
       "       [1.06487866e-078, 3.48110684e-072, 1.00000000e+000],\n",
       "       [1.38389653e-087, 1.00000000e+000, 1.13396656e-111],\n",
       "       [1.13396656e-126, 1.00000000e+000, 1.06487866e-048],\n",
       "       [3.97544974e-046, 4.81749166e-159, 1.00000000e+000],\n",
       "       [3.05902227e-007, 9.99999694e-001, 2.05388393e-070],\n",
       "       [1.00000000e+000, 3.72007598e-044, 6.71418429e-064],\n",
       "       [4.83454164e-083, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e+000, 3.96142952e-107, 6.28288051e-077],\n",
       "       [1.00000000e+000, 4.23337159e-094, 1.79848622e-096],\n",
       "       [4.23337159e-109, 4.52398179e-096, 1.00000000e+000],\n",
       "       [3.97544974e-046, 1.56929239e-228, 1.00000000e+000],\n",
       "       [9.99999693e-001, 3.05902227e-007, 1.29958103e-009],\n",
       "       [1.58042006e-076, 1.13396656e-141, 1.00000000e+000],\n",
       "       [4.81749166e-159, 2.05388455e-100, 1.00000000e+000],\n",
       "       [1.00000000e+000, 3.97544974e-031, 1.79848622e-096],\n",
       "       [2.67863696e-033, 1.00000000e+000, 7.14979157e-127],\n",
       "       [1.68295560e-139, 4.53999298e-020, 1.00000000e+000],\n",
       "       [5.14820022e-131, 1.00000000e+000, 6.28288051e-077],\n",
       "       [9.92959040e-168, 4.53999298e-020, 1.00000000e+000],\n",
       "       [1.00000000e+000, 9.35762297e-014, 4.83454164e-053],\n",
       "       [1.00000000e+000, 6.26072268e-168, 9.32462145e-075],\n",
       "       [4.24835426e-018, 1.00000000e+000, 3.25748853e-055],\n",
       "       [3.70695639e-135, 1.92874985e-037, 1.00000000e+000],\n",
       "       [9.99999998e-001, 2.06115362e-009, 5.52108227e-027],\n",
       "       [2.67863696e-048, 2.19487851e-087, 1.00000000e+000],\n",
       "       [4.11540605e-003, 6.10780412e-001, 3.85104182e-001],\n",
       "       [2.05243415e-009, 9.95769616e-001, 4.23038208e-003],\n",
       "       [3.96142952e-122, 5.52108228e-057, 1.00000000e+000],\n",
       "       [4.85164960e-007, 2.50656626e-061, 9.99999515e-001],\n",
       "       [2.50656748e-046, 1.00000000e+000, 3.46883002e-118],\n",
       "       [6.62047965e-322, 3.69388307e-211, 1.00000000e+000],\n",
       "       [2.67863696e-048, 1.00000000e-015, 1.00000000e+000],\n",
       "       [1.00000000e+000, 5.16642063e-055, 3.25748853e-055],\n",
       "       [6.30511676e-016, 1.00000000e+000, 3.23455268e-207],\n",
       "       [1.58042006e-076, 2.06115362e-024, 1.00000000e+000],\n",
       "       [1.00000000e+000, 6.71418429e-079, 7.66764807e-038],\n",
       "       [1.00000000e+000, 1.06487866e-063, 1.06487866e-048],\n",
       "       [4.81749166e-144, 1.00000000e+000, 2.05388455e-070],\n",
       "       [1.00000000e+000, 4.20356459e-246, 3.96142952e-092],\n",
       "       [1.00000000e+000, 1.80485139e-035, 1.68295560e-109],\n",
       "       [1.00000000e+000, 1.29958143e-024, 3.70695639e-105],\n",
       "       [2.04664112e-176, 6.71418429e-094, 1.00000000e+000],\n",
       "       [9.32462145e-105, 2.05388455e-100, 1.00000000e+000],\n",
       "       [1.92874985e-037, 5.52108228e-057, 1.00000000e+000],\n",
       "       [4.53999298e-020, 1.92874985e-037, 1.00000000e+000],\n",
       "       [2.49772757e-122, 1.00000000e+000, 6.28288051e-077],\n",
       "       [1.06487866e-078, 1.13797987e-065, 1.00000000e+000],\n",
       "       [1.00000000e+000, 2.65977679e-185, 1.38389653e-072],\n",
       "       [1.58042006e-076, 5.52108228e-057, 1.00000000e+000],\n",
       "       [6.69050538e-170, 4.83454164e-083, 1.00000000e+000],\n",
       "       [1.00000000e+000, 4.23337159e-094, 8.16511481e-101],\n",
       "       [5.50161108e-133, 6.30511676e-031, 1.00000000e+000],\n",
       "       [1.00000000e+000, 6.28288051e-092, 4.52398179e-066],\n",
       "       [1.57484639e-152, 1.29499819e-115, 1.00000000e+000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanbox_run.state.Pi_a_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_human = SimpleEnv(render_mode='human')\n",
    "env_human.reset()\n",
    "sanbox_run.env = env_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanbox_run.run_policy_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRunner(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "    \n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and self.step_count < 100:  # Prevent infinite loops\n",
    "  \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = QRunner(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional switch out env to human mode\n",
    "env_human = SimpleEnv(render_mode='human')\n",
    "runner.env = env_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run_policy_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This class can make new calculations, can't change any of the parameters of the class train or run\"\"\"\n",
    "\n",
    "class QAnalyzer(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "        \n",
    "    #get probability of state using time probability\n",
    "    def get_P_st(self, T):\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.find_connected_states() #depends on self.state.Pi_a_s\n",
    "\n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, T)\n",
    "        P_s = np.dot(initial_s.T, Ps_s_matrix)   #define P(s)\n",
    "\n",
    "        assert(np.isclose(np.sum(P_s), 1)), f\"Sum of P(s) is not 1: {np.sum(P_s)}\"\n",
    "\n",
    "        return P_s\n",
    "    \n",
    "    def alt_P_r_s(self, state_probabilities):\n",
    "        \"\"\"Separate from the state index the actual position and direction of the agent.\"\"\"\n",
    "        positions_directions = {}\n",
    "        \n",
    "        \n",
    "        for state_idx, probability in enumerate(state_probabilities):\n",
    "            x, y, direction = self.state_index_to_position(state_idx)\n",
    "            \n",
    "            # Store in a dictionary under 'position' and 'direction'\n",
    "            positions_directions[state_idx] = {\n",
    "                \"position\": (x, y),\n",
    "                \"direction\": direction,\n",
    "                \"probability\": probability\n",
    "            }\n",
    "        return positions_directions\n",
    "\n",
    "    def get_pdl(self, T):\n",
    "        self.state.positions_directions_list_ps = []\n",
    "        for t in range(T):\n",
    "            tmp_probs = self.get_P_st(t) #defined here\n",
    "            pos_dict = self.alt_P_r_s(tmp_probs) #defined here\n",
    "    \n",
    "            self.state.positions_directions_list_ps.append(pos_dict)\n",
    "\n",
    "    def get_neglog_pdl(self, T):\n",
    "        self.state.positions_directions_list_neglogps = []\n",
    "        for t in range(T):\n",
    "            tmp_probs = self.get_P_st(t)\n",
    "            pos_dict = self.alt_P_r_s(-np.log((tmp_probs)+ 1e-15))\n",
    "            self.state.positions_directions_list_neglogps.append(pos_dict)\n",
    "    \n",
    "    \"\"\"Decision through time\"\"\"\n",
    "\n",
    "    def get_Pi_a(self, T):\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.find_connected_states()\n",
    "\n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, T)\n",
    "        P_s = np.dot(initial_s.T, Ps_s_matrix)   #define P(s)\n",
    "        \n",
    "        pi_a_sums = []\n",
    "        for s in range(self.state.num_states):\n",
    "            row = self.state.Pi_a_s[s] * P_s[s]\n",
    "            pi_a_sums.append(row)\n",
    "        pi_a_sums = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "        return pi_a_sums\n",
    "\n",
    "    def get_decision(self, T):\n",
    "        decisions = []\n",
    "        for t in range(T):\n",
    "            #tmp_probs is a list, each item is a vector of state probabilities for each time\n",
    "            #tmp_probs = self.get_P_st(t)\n",
    "            pi_a = self.get_Pi_a(t) #specific time dependent a array\n",
    "    \n",
    "            decision_terms = []\n",
    "            for state in range(self.state.num_states): #pick out a single state probability from all states\n",
    "                decision_term = 0\n",
    "                if state in self.state.allowed_state_idx:\n",
    "                # action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[state])\n",
    "                # next_state = np.argmax(self.state.P_s_given_s_a[state, action])\n",
    "                    for a in range(self.state.num_actions):\n",
    "                        \n",
    "                        decision_term +=((np.log(self.state.Pi_a_s[state , a] + 1e-15) - np.log(pi_a[a]+ 1e-15) ) * self.state.Pi_a_s[state, a])   #this is the decision term for that specific state, it is a sum over all actions\n",
    "                        #gives a decision term value for that specific state\n",
    "                decision_terms.append(decision_term)\n",
    "        \n",
    "            decisions.append(decision_terms)\n",
    "        return decisions\n",
    "    \n",
    "\n",
    "    def get_decision_terms(self, T):\n",
    "        \"\"\"Get decision terms for each time step and store them as position dictionaries.\"\"\"\n",
    "        decision_terms = self.get_decision(T)\n",
    "        self.state.positions_directions_list_decisions = []\n",
    "\n",
    "        for t_decisions in decision_terms:\n",
    "            pos_dict = self.alt_P_r_s(t_decisions)  # <- reuse existing function!\n",
    "            self.state.positions_directions_list_decisions.append(pos_dict)\n",
    "\n",
    "    \"\"\"Information to go delta term\"\"\"\n",
    "\n",
    "    def get_info_to_go_delta(self, initial_state, time, state, action):\n",
    "        #this function is state dependent\n",
    "        \n",
    "        Ps_s_matrix = np.linalg.matrix_power(self.state.P_s_by_s, time)\n",
    "        P_s = np.dot(initial_state.T, Ps_s_matrix)   #define P(s)\n",
    "        \n",
    "        pi_a_sums = []\n",
    "        for s in range(self.state.num_states):\n",
    "            row = self.state.Pi_a_s[s] * P_s[s]\n",
    "            pi_a_sums.append(row)\n",
    "        pi_a_sums = np.sum(np.asarray(pi_a_sums), axis=0)\n",
    "        delta = ((- np.log(P_s[state] + 1e-10)) + ((np.log(self.state.Pi_a_s[state, action] +1e-10))- (np.log(pi_a_sums[action]) + 1e-10)))\n",
    "        return delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def walk(self):\n",
    "        \n",
    "        self.info_to_go_delta_term = 0  # Initialize information delta term\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        self.step_count = 1  # Track steps to prevent infinite loops\n",
    "        initial_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        initial_s = np.zeros(self.state.num_states)\n",
    "        initial_s[initial_state] = 1\n",
    "        self.info_state_calculation = []\n",
    "        while not done and self.step_count < 100:  # Prevent infinite loops\n",
    "            \n",
    "            action = np.random.choice(np.arange(self.state.num_actions), p=self.state.Pi_a_s[current_state])  # Choose best action\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "            self.env.render()  # Visualize movement\n",
    "            # Calculate info to go term for the current step\n",
    "            self.info_to_go_delta_term = self.get_info_to_go_delta(initial_state = initial_s, time = self.step_count, state = current_state, action = action)  # Calculate information delta for the current state\n",
    "            self.info_state_calculation.append(self.info_to_go_delta_term)\n",
    "\n",
    "\n",
    "            current_state = next_state  # Update current state\n",
    "            self.step_count += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state.allowed_state_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.P_s_given_s_a[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(state.P_s_given_s_a[0, 1] == 1)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = QAnalyzer(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10878342, 0.13252508, 0.75869149])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze.get_Pi_a(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.imshow(np.linalg.matrix_power(analyze.state.P_s_by_s.T, 1), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([224]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(analyze.tmp_probs == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_pdl(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_neglog_pdl(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.get_decision_terms(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(23.36283519544392),\n",
       " np.float64(2.2281135735996593),\n",
       " np.float64(3.4240366307621204),\n",
       " np.float64(2.469144228790274),\n",
       " np.float64(3.162571052573356),\n",
       " np.float64(2.4729996061341284),\n",
       " np.float64(3.337824095527396),\n",
       " np.float64(2.6667178266697746),\n",
       " np.float64(3.6375450375935436),\n",
       " np.float64(3.1316212948475974),\n",
       " np.float64(3.8328375384853848),\n",
       " np.float64(3.365582732331605),\n",
       " np.float64(4.709062803586622),\n",
       " np.float64(4.157136611627501),\n",
       " np.float64(3.5868187176801447),\n",
       " np.float64(3.4085826320905714),\n",
       " np.float64(4.661011597556888),\n",
       " np.float64(5.964803685737869),\n",
       " np.float64(6.395607336913935),\n",
       " np.float64(6.216012134541505),\n",
       " np.float64(6.232995954386063),\n",
       " np.float64(5.038758887875328),\n",
       " np.float64(7.503087283327312),\n",
       " np.float64(8.55753948786707),\n",
       " np.float64(5.851741437669209),\n",
       " np.float64(5.497714592984658),\n",
       " np.float64(4.6189644966376875),\n",
       " np.float64(5.784610987384084),\n",
       " np.float64(4.669588260348677),\n",
       " np.float64(3.996961596136783),\n",
       " np.float64(3.9938427979259536),\n",
       " np.float64(3.9336778964817136),\n",
       " np.float64(5.444937085418527),\n",
       " np.float64(3.9108750568330075),\n",
       " np.float64(3.907814275029814),\n",
       " np.float64(3.8951858717937733),\n",
       " np.float64(0.9612651709734401)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze.info_state_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plots Heatmaps of the Process's Various Variables\n",
    "Pass in variables from the run class as objects of this class\n",
    "This class cannot make any changes to the variables, it can only plot them\"\"\"\n",
    "\n",
    "\n",
    "class QPlotter(GridMixin):\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    def plot_all_heatmaps(self, pos_dir_list):\n",
    "        \"\"\"\n",
    "        Plot a heatmap for each step of the simulation using the data stored \n",
    "        in self.positions_directions_list. Each heatmap shows the probability (from P_s)\n",
    "        at the corresponding x, y positions, and prints the sum of all probabilities.\n",
    "        The color bar range is standardized across all plots using the global maximum value.\n",
    "        \"\"\"\n",
    "        # First, determine the grid size across all steps\n",
    "        all_positions = []\n",
    "        for pos_dict in pos_dir_list:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "        \n",
    "        # Determine grid dimensions\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        # Create a heatmap for each step using the standardized color range.\n",
    "        for step, pos_dict in enumerate(pos_dir_list):\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "\n",
    "            total_probability = grid.sum()\n",
    "            print(f\"Step {step}: Total probability = {total_probability:.4f}\")\n",
    "\n",
    "            # Plot the heatmap: set vmin to 0 and vmax to the computed global maximum.\n",
    "            plt.figure()\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step} \")\n",
    "            \n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"(P(s)\")\n",
    "            plt.show()\n",
    "\n",
    "    def plot_all_heatmaps_by_direction(self, pos_dir_list, value_key=\"probability\", annotate=True):\n",
    "        \"\"\"\n",
    "        Plot a heatmap for each direction across all time steps.\n",
    "        Optionally annotate each grid cell with its value.\n",
    "        \"\"\"\n",
    "        # Determine grid dimensions\n",
    "        all_positions = []\n",
    "        for pos_dict in pos_dir_list:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        # Direction-first plotting\n",
    "        for d in range(4):\n",
    "            for step, pos_dict in enumerate(pos_dir_list):\n",
    "                grid = np.zeros((max_y + 2, max_x + 2))\n",
    "\n",
    "                for entry in pos_dict.values():\n",
    "                    x, y = entry[\"position\"]\n",
    "                    direction = entry[\"direction\"]\n",
    "                    if direction != d:\n",
    "                        continue\n",
    "                    val = entry[value_key]\n",
    "                    grid[y, x] += val\n",
    "\n",
    "                total = grid.sum()\n",
    "                print(f\"Direction {d}, Step {step}: Total {value_key} = {total:.4f}\")\n",
    "\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                im = plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "                plt.title(f\"Decision Complexity | Dir {d}, Step {step}\")\n",
    "                plt.xlabel(\"X position\")\n",
    "                plt.ylabel(\"Y position\")\n",
    "                plt.colorbar(im, label=\"Weighted Decision Complexity\")\n",
    "\n",
    "                if annotate:\n",
    "                    for y in range(grid.shape[0]):\n",
    "                        for x in range(grid.shape[1]):\n",
    "                            val = grid[y, x]\n",
    "                            if val > 0:  # only annotate non-zero\n",
    "                                plt.text(x, y, f\"{val:.2f}\", ha='center', va='center', color='blue', fontsize=8)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_save_all_heatmaps(self, folder_name):\n",
    "        \"\"\"\n",
    "        Generate and save a heatmap for each simulation step.\n",
    "        \n",
    "        Parameters:\n",
    "        - folder_name: The directory in which images are saved.\n",
    "        \n",
    "        Each heatmap is saved as a PNG file (e.g. \"heatmap_000.png\").\n",
    "        \"\"\"\n",
    "        # Ensure the folder exists\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "        \n",
    "        # Determine grid dimensions from all positions\n",
    "        all_positions = []\n",
    "        for pos_dict in self.state.positions_directions_list_ps:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "        \n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        fig_size = (6, 6)\n",
    "        \n",
    "        # Generate and save a heatmap for each step\n",
    "        for step, pos_dict in enumerate(self.state.positions_directions_list_ps):\n",
    "            # Initialize a grid (with zeros) of appropriate size.\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            \n",
    "            # Populate the grid by adding probabilities\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "            \n",
    "            # Calculate the total probability for this heatmap\n",
    "            total_probability = grid.sum()\n",
    "           \n",
    "            \n",
    "            # Create the figure for the heatmap\n",
    "            plt.figure(figsize = fig_size)\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step} \")\n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"-log(P(s)\")\n",
    "            \n",
    "            # Build the file path; file names are zero-padded for proper sorting.\n",
    "            file_path = os.path.join(folder_name, f\"heatmap_{step:03d}.png\")\n",
    "            plt.savefig(file_path)\n",
    "            #plt.close()  # Close the figure to free memory\n",
    "            \n",
    "    \"\"\"Plotter and saver, change the dictionary depending on the use case\"\"\"\n",
    "    def plot_or_save_all_heatmaps(self, save=False, folder_name=None):\n",
    "        all_positions = []\n",
    "        for pos_dict in self.state.positions_directions_list_ps:\n",
    "            for entry in pos_dict.values():\n",
    "                all_positions.append(entry[\"position\"])\n",
    "\n",
    "        max_x = max(pos[0] for pos in all_positions)\n",
    "        max_y = max(pos[1] for pos in all_positions)\n",
    "\n",
    "        if save and folder_name and not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        for step, pos_dict in enumerate(self.state.positions_directions_list_ps):\n",
    "            grid = np.zeros((max_y + 2, max_x + 2))\n",
    "            for entry in pos_dict.values():\n",
    "                x, y = entry[\"position\"]\n",
    "                probability = entry[\"probability\"]\n",
    "                grid[y, x] += probability\n",
    "\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(grid, origin='upper', cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Information Gained by Environment: Step {step}\")\n",
    "            plt.xlabel(\"X position\")\n",
    "            plt.ylabel(\"Y position\")\n",
    "            plt.colorbar(label=\"P(s)\")\n",
    "\n",
    "            if save:\n",
    "                file_path = os.path.join(folder_name, f\"heatmap_{step:03d}.png\")\n",
    "                plt.savefig(file_path)\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = QPlotter(env, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_all_heatmaps_by_direction(state.positions_directions_list_decisions, value_key=\"probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_all_heatmaps(state.positions_directions_list_decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_all_heatmaps(state.positions_directions_list_neglogps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QVideo():\n",
    "    def __init__(self, env, state: ModelState):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "\n",
    "    def clear_folder(folder_name):\n",
    "        files = glob.glob(os.path.join(folder_name, \"*.png\"))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    def create_video_from_images(image_folder, output_file, fps=2):\n",
    "        \"\"\"\n",
    "        Create a video from a sequence of images saved in a folder.\n",
    "        \n",
    "        Parameters:\n",
    "        - image_folder: Folder where the heatmap images are stored.\n",
    "        - output_file: Path for the output video file (e.g., \"simulation.mp4\").\n",
    "        - fps: Frames per second for the output video.\n",
    "        \"\"\"\n",
    "        # Get a sorted list of image files from the folder\n",
    "        image_pattern = os.path.join(image_folder, \"heatmap_*.png\")\n",
    "        image_files = sorted(glob.glob(image_pattern))\n",
    "        \n",
    "        # Create a clip from the sequence of images\n",
    "        clip = ImageSequenceClip(image_files, fps=fps)\n",
    "        \n",
    "        # Write the video file\n",
    "        clip.write_videofile(output_file, codec='libx264', audio_codec=\"aac\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This instantiates an env, creates a model state, and runs the training process, for multiple beta\n",
    "\"\"\"\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, env_class):\n",
    "        self.env_class = env_class\n",
    "\n",
    "    def run_mulitple_betas(self, beta_values, runs_per_beta = 10, epochs = 500, plot = False, video = False):\n",
    "        \"\"\"\n",
    "        Run the training function for mulitple betas, save the policies for each beta, and then\n",
    "        use the trained policies to run the test_policy function and collect the length of\n",
    "        the path taken, then plot.\n",
    "\n",
    "        args: \n",
    "        beta_values: list of beta values\n",
    "        runs_per_beta: number of runs per beta value\n",
    "        epochs: int\n",
    "        summary plot: bool for specific plots\n",
    "        video: bool\n",
    "\n",
    "        returns:\n",
    "            dict: mapping from beta -> average step count\n",
    "        \"\"\"\n",
    "        \n",
    "        betas = np.arange(0, 10, 0.1)\n",
    "        step_count_per_beta = {}\n",
    "\n",
    "        for beta in beta_values:\n",
    "            # Make a FRESH instance for each beta\n",
    "            env = self.env_class()\n",
    "            model = QModel(env)\n",
    "            state = ModelState(beta=beta)\n",
    "            trainer = QTrainer(env, state)\n",
    "\n",
    "\n",
    "            # Train from scratch inside that instance\n",
    "            model_for_beta.train(epochs=500)\n",
    "\n",
    "            # Evaluate it runs_per_beta times\n",
    "            step_counts = []\n",
    "            for _ in range(runs_per_beta):\n",
    "                model_for_beta.env.reset()\n",
    "                model_for_beta.run_policy_2()\n",
    "                step_counts.append(model_for_beta.step_count)\n",
    "\n",
    "            step_count_per_beta[beta] = np.mean(step_counts)\n",
    "\n",
    "        return step_count_per_beta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
