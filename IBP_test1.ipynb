{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __future__ import should always be first\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Gymnasium & Minigrid imports\n",
    "import gymnasium as gym  # Correct way to import Gymnasium\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from gymnasium.utils.play import play\n",
    "import pandas as pd\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning head direction turned off, but code is glitchy\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(self, size=10, agent_start_pos=(1, 8), agent_start_dir=0, max_steps=256, **kwargs):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.reached_goal = False\n",
    "        self.step_count = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Only allow Forward, Left+Forward, Right+Forward\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 3 actions\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            \n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.reached_goal = False\n",
    "        self.agent_dir = 0\n",
    "        return obs\n",
    "    \n",
    "\n",
    "    def get_view_exts(self, agent_view_size=None):\n",
    "        \"\"\"Override default view extensions to bypass direction checks.\"\"\"\n",
    "        agent_view_size = agent_view_size or self.agent_view_size\n",
    "        topX = self.agent_pos[0] - agent_view_size // 2\n",
    "        topY = self.agent_pos[1] - agent_view_size // 2\n",
    "        botX = topX + agent_view_size\n",
    "        botY = topY + agent_view_size\n",
    "        return topX, topY, botX, botY\n",
    "    \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def dir_vec(self):\n",
    "        \"\"\"Override MiniGrid's default direction vector.\"\"\"\n",
    "        return np.array([0, 1])  # Always move upwards by default\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(\n",
    "            1 for x in range(self.grid.width)\n",
    "            for y in range(self.grid.height)\n",
    "            if not self.grid.get(x, y)\n",
    "        )\n",
    "        return free_cells  \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Modify step function to ensure correct movement (no diagonal jumps, no wall clipping).\"\"\"\n",
    "        self.agent_dir = 0  # ✅ Keep agent direction fixed\n",
    "\n",
    "        # ✅ Define movement vectors: (dx, dy)\n",
    "        move_vectors = {\n",
    "            0: (0, -1),   # Move Forward (UP)\n",
    "            1: (-1, 0),   # Move Left\n",
    "            2: (1, -1)     # Move Right\n",
    "        }\n",
    "\n",
    "        # Get movement vector for action\n",
    "        move_vector = move_vectors.get(action, (0, 0))  # Default: no movement if invalid action\n",
    "        \n",
    "        # Compute the new position\n",
    "        new_x = self.agent_pos[0] + move_vector[0]\n",
    "        new_y = self.agent_pos[1] + move_vector[1]\n",
    "\n",
    "        # ✅ Ensure movement respects grid boundaries & walls\n",
    "        if (0 <= new_x < self.grid.width) and (0 <= new_y < self.grid.height):\n",
    "            cell_contents = self.grid.get(new_x, new_y)  # Check what's in the new position\n",
    "\n",
    "            if cell_contents is None or isinstance(cell_contents, Goal):  \n",
    "                # ✅ Only update if the move is valid\n",
    "                self.agent_pos = (new_x, new_y)  \n",
    "   \n",
    "\n",
    "        # ✅ Call MiniGrid's original step function\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        # ✅ Ensure agent never moves out of bounds\n",
    "        x, y = self.agent_pos\n",
    "        x = max(0, min(x, self.grid.width - 1))\n",
    "        y = max(0, min(y, self.grid.height - 1))\n",
    "        self.agent_pos = (x, y)\n",
    "\n",
    "        # ✅ Check if goal is reached\n",
    "        if np.array_equal(self.agent_pos, (8, 1)):  \n",
    "            self.reached_goal = True\n",
    "\n",
    "        reward = 0 if self.reached_goal else -1  # Assign reward\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Action with Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            size=10, \n",
    "            agent_start_pos=(1, 8), \n",
    "            agent_start_dir=0, \n",
    "            max_steps=256, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = (8, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        #create gird\n",
    "        self.grid = Grid(width, height)\n",
    "        #place barrier\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "        #place goal\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "        #place walls\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "        #place agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"find the shortest path\"\n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Environment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=\"human\")  # Create environment\n",
    "# free_energy_solver = FreeEnergyMin(env, beta=0.5)\n",
    "obs = env.reset()[0]  # Reset environment and get initial state\n",
    "env.render()  # Display initial state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.count_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, truncated, info = env.step(0)  #turn left\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, truncated, info = env.step(1) #turn right\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, truncated, info = env.step(2) #move forward\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_energy_solver.position_to_state_index(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_energy_solver.state_index_to_position(328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test env\n",
    "env = SimpleEnv(render_mode=\"human\") \n",
    "# env.reset()\n",
    "# free_energy_solver = FreeEnergyMin(env, beta=0.5)\n",
    "# free_energy_solver.estimate_transitions() #for when you want to do a random walk, no learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Policy Based On Information-to-Go Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n * env.count_states() #number of actions * number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.count_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_energy_solver.num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_energy_solver.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.agent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_energy_solver.num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreeEnergyMin:\n",
    "    \"\"\"Free Energy Minimization to find optimal policy in MiniGrid Environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env, \n",
    "            beta=0.5\n",
    "    ):\n",
    "        \n",
    "        self.env = env  # MiniGrid environment\n",
    "        self.num_states = np.int64(env.count_states())  # Number of states\n",
    "        print(f\"Number of states: {self.num_states}\")\n",
    "        self.num_actions = env.action_space.n  # 3 actions (Forward, Left+Forward, Right+Forward)\n",
    "        print(f\"Number of actions: {self.num_actions}\")\n",
    "        self.beta = beta  # Temperature parameter\n",
    "\n",
    "        # Initialize policy (uniform distribution)\n",
    "        self.Pi_a_s = np.full((self.num_states, self.num_actions), 1 / self.num_actions) \n",
    "        print(f\"policy shape: {self.Pi_a_s.shape}\")\n",
    "        self.Pi_a = np.full(self.num_actions, 1 / self.num_actions)\n",
    "        print(f\"marginal action distribution shape: {self.Pi_a.shape}\")\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def position_to_state_index(self, state):\n",
    "        \"\"\"Converts (x, y, direction) into a unique state index.\"\"\"\n",
    "        grid_width = self.env.grid.width\n",
    "        grid_height = self.env.grid.height\n",
    "        x, y = self.env.agent_pos  \n",
    "        direction = self.env.agent_dir  \n",
    "\n",
    "        return np.int64((y * grid_width + x) * 4 + direction)\n",
    "\n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a 1D state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x, y, direction\n",
    "    \n",
    "    def estimate_transitions(self, episodes=10):\n",
    "        \"\"\"Simulate environment for debugging with random agent movements.\"\"\"\n",
    "        for _ in range(1, episodes + 1):\n",
    "            state, _ = self.env.reset()  \n",
    "            \n",
    "\n",
    "            for step in range(self.env.max_steps):\n",
    "                action = self.env.action_space.sample() \n",
    "                print(f\"Step {step}: Action {action} taken.\")\n",
    "                next_state, _, done, _, _ = self.env.step(action)  \n",
    "                \n",
    "                #Convert states to indices for debugging\n",
    "                s_idx = self.position_to_state_index(state)\n",
    "                s_next_idx = self.position_to_state_index(next_state)\n",
    "                \n",
    "                state = next_state  #Update current state\n",
    "                \n",
    "                if done:\n",
    "                    \n",
    "                    break  # Stop if episode ends\n",
    "\n",
    "        print(\"Finished testing environment.\")\n",
    "\n",
    "    def compute_free_energy(self, num_iterations=500):\n",
    "        \"\"\"Iteratively update Free Energy and optimize the policy.\"\"\"\n",
    "        states = self.num_states\n",
    "        actions = self.num_actions\n",
    "\n",
    "        print(f'Number of states: {states}')\n",
    "        print(f'Number of actions: {actions}')\n",
    "\n",
    "        # Initialize transition probabilities (deterministic)\n",
    "        P_s_given_sa = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "\n",
    "        # Assume uniform state distribution initially\n",
    "        P_s = np.full(self.num_states, 1 / self.num_states)\n",
    "\n",
    "        # Initialize Free Energy\n",
    "        self.F = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "        free_energy = []\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            prev_F = np.sum(self.F)\n",
    "\n",
    "            for state_idx in range(states):  # Loop over all states\n",
    "                x, y, direction = self.state_index_to_position(state_idx)\n",
    "\n",
    "                # Ensure the agent is placed in a valid position\n",
    "                if self.env.grid.get(x, y) is not None and not isinstance(self.env.grid.get(x, y), Goal):\n",
    "                    print(f\"Invalid state {x}, {y} (occupied). Trying another.\")\n",
    "                    continue\n",
    "                    \n",
    "\n",
    "                self.env.place_agent((x, y))\n",
    "                self.env.agent_dir = direction\n",
    "                \n",
    "\n",
    "                for a in range(actions):  # Loop over actions\n",
    "                    print(f\"Processing state {state_idx}, action {a}\")\n",
    "\n",
    "                    # Execute action\n",
    "                    action = np.argmax(self.Pi_a_s[state_idx])  # Select action based on policy\n",
    "                    next_state, _, done, _, _ = self.env.step(action)\n",
    "                    s_next = self.position_to_state_index(next_state) #convert position to next state\n",
    "\n",
    "                    # If terminal, break\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                    # Sample next action from policy\n",
    "                    next_action = np.argmax(self.Pi_a_s[s_next])\n",
    "                    next_state_1, _, _, _, _ = self.env.step(next_action)\n",
    "                    s_next_next = self.position_to_state_index(next_state_1)\n",
    "\n",
    "                    # **Deterministic Transition Probability**\n",
    "                    P_s_given_sa[state_idx, action, s_next] = 1\n",
    "\n",
    "                    # Compute expectation term (log ratio of policy)\n",
    "                    J = np.sum(np.log(np.maximum(self.Pi_a_s[s_next, :] / np.maximum(self.Pi_a, 1e-10), 1e-10)))\n",
    "\n",
    "                    # Define reward function\n",
    "                    reward = 0 if self.env.reached_goal else -1\n",
    "\n",
    "                    # Update Free Energy functional\n",
    "                    self.F[state_idx, a] = (\n",
    "                        np.sum(P_s_given_sa[state_idx, a, :] * np.log(np.maximum(P_s_given_sa[state_idx, a, :] / P_s, 1e-10)))\n",
    "                        - self.beta * reward + J\n",
    "                    )\n",
    "\n",
    "            # Compute partition function Zπ(s, β)\n",
    "            Z = np.sum(self.Pi_a[None, :] * np.exp(-self.F / (self.beta + 1e-10)), axis=1, keepdims=True) + 1e-5\n",
    "\n",
    "            # Update policy π(a|s)\n",
    "            self.Pi_a_s = (self.Pi_a[None, :] / Z) * np.exp(-self.F)\n",
    "\n",
    "            # Update marginal π(a)\n",
    "            self.Pi_a = np.sum(self.Pi_a_s * P_s[:, None], axis=0) + 1e-10\n",
    "            self.Pi_a /= np.sum(self.Pi_a)\n",
    "\n",
    "            # Compute max policy change\n",
    "            max_policy_change = np.max(np.abs(self.Pi_a_s - self.Pi_a[None, :]))\n",
    "\n",
    "            # Print status\n",
    "            print(f\"Iteration {iteration}: Free Energy Sum: {np.sum(self.F)}, Change: {prev_F - np.sum(self.F)}\")\n",
    "            print(f\"Iteration {iteration}: Max Policy Change = {max_policy_change}\")\n",
    "\n",
    "            free_energy.append(np.sum(self.F))\n",
    "            \n",
    "            # Convergence check\n",
    "            if max_policy_change < 1e-5:\n",
    "                print(f\"Converged at iteration {iteration}.\")\n",
    "                break\n",
    "            \n",
    "            \n",
    "        \n",
    "        policy = self.Pi_a_s\n",
    "\n",
    "        return policy, free_energy \n",
    "\n",
    "\n",
    "    def run_policy(self, policy):\n",
    "        \"\"\"Run the environment using the learned policy. Convert state in policy to its position using \"\"\"\n",
    "        \n",
    "        start_pos = self.env.reset()[0] #reset the environment and get initial state\n",
    "        state_idx = self.position_to_state_index(start_pos) #convert starting position to index\n",
    "        done = False\n",
    "        while not done:\n",
    "            x, y, direction = self.state_index_to_position(state_idx) \n",
    "            self.env.place_agent((x, y))\n",
    "            self.env.agent_dir = direction\n",
    "            action = np.argmax(policy[state_idx])  # Choose best action from policy\n",
    "            position, _, done, _, _ = self.env.step(action) #take the action\n",
    "            state_idx = self.position_to_state_index(position) #use this next position , decode the state\n",
    "            self.env.render()\n",
    "            \n",
    "\n",
    "\n",
    "    def plot_free_energy(self, free_energy):\n",
    "        \"\"\"Plot the free energy over iterations.\"\"\"\n",
    "        \n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(free_energy, marker='o', linestyle='-')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Sum of Free Energy\")\n",
    "        plt.title(\"Free Energy Minimization Over Iterations\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume that -bR + F(a,s,b) is basically like Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.count_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(env.width-2) * (env.height-2) * 4 - (8 *4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        learning_rate = 0.9,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 0.5,\n",
    "        epochs = 1000\n",
    "         \n",
    "    ):\n",
    "        self.env = env # MiniGrid environment called from the class Minigrid\n",
    "        self.num_states = np.int64(env.count_states())\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.num_actions)) # Q table has goal states in it as well\n",
    "        # print(f\"Q table shape: {self.Q_table.shape}\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        self.epsilon =  epsilon # exploration probability\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "        self.allowed_state_idx = self.find_state_indexes(env)\n",
    "        # print(f\"allowed state indexes: {self.allowed_state_idx}\")\n",
    "\n",
    "     # Store state indexes at initialization\n",
    "        # self.state_indexes_list = self.find_state_indexes(env)\n",
    "        # print(f'state indexes list: {self.state_indexes_list}')\n",
    "     # making a dictionary to convert state index to index\n",
    "        # self.state_to_index = {state: i for i, state in enumerate(self.state_indexes_list)}\n",
    "        # print(f'state to index: {self.state_to_index}')\n",
    "        \n",
    "    # def position_to_state_index(self, state):\n",
    "    #     \"\"\"Converts (x, y, direction) into a unique state index.\"\"\"\n",
    "    #     grid_width = self.env.grid.width\n",
    "        \n",
    "    #     x, y, direction  = state  \n",
    "        \n",
    "\n",
    "    #     return np.int64((y * grid_width + x) * 4 + direction)\n",
    "    \n",
    "    # def position_to_state_index(self, obs):\n",
    "    #     \"\"\"Converts an observation (either (x, y, direction) or an image-based dict) into a state index.\"\"\"\n",
    "    #     grid_width = self.env.grid.width -2\n",
    "        \n",
    "    #     # Case 1: Observation is a dictionary (image-based)\n",
    "    #     if isinstance(obs, dict) and 'image' in obs:\n",
    "    #         x, y = self.env.agent_pos  # Extract (x, y) directly from the environment\n",
    "    #         direction = self.env.agent_dir  # Extract direction\n",
    "    #     # Case 2: Observation is already in (x, y, direction) format\n",
    "    #     elif isinstance(obs, tuple) and len(obs) == 3:\n",
    "    #         x, y, direction = obs\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Invalid observation format: {obs}\")\n",
    "\n",
    "        # Convert (x, y, direction) to a unique state index\n",
    "        # return np.int64(((y-1) * grid_width + (x-1)) * 4 + direction)\n",
    "    \n",
    "    def position_to_state_index(self, tuple_position = None): \n",
    "        grid_width = self.env.grid.width -2\n",
    "        if tuple_position is None:\n",
    "            direction = self.env.agent_dir\n",
    "            x, y = self.env.agent_pos\n",
    "\n",
    "        else:\n",
    "            if not isinstance(tuple_position, tuple) or len(tuple_position) != 3:\n",
    "                raise ValueError(f\"Invalid position format: {tuple_position}\")\n",
    "            x, y, direction = tuple_position\n",
    "        return np.int64(((y-1) * grid_width + (x-1)) * 4 + direction) \n",
    "    \n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width-2\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x+1, y+1, direction\n",
    "    \n",
    "    def find_state_indexes(self, env):\n",
    "        \"\"\"Counts all states except walls and barriers\"\"\"\n",
    "        state_indexes_list = []\n",
    "        for x in range(1, env.grid.width-1):\n",
    "            for y in range(1, env.grid.height-1):\n",
    "                if env.grid.get(x, y) is None: #grabs all empty spaces\n",
    "                    for direction in range(4):\n",
    "                        state_index = self.position_to_state_index((x, y, direction))\n",
    "                        state_indexes_list.append(state_index)\n",
    "        return state_indexes_list        \n",
    "    \n",
    "    \n",
    "    def train(self, epochs):\n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            current_state = np.random.choice(self.allowed_state_idx)\n",
    "            x,y,dir = self.state_index_to_position(current_state)\n",
    "            self.env.agent_pos = (x,y)\n",
    "            self.env.agent_dir = dir\n",
    "            # print(f\"Epoch {epoch}: Starting state: {current_state}\")\n",
    "            # print(f\"Epoch {epoch}: Starting position: {self.state_index_to_position(current_state)}\")\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                # #replace current index with its table index\n",
    "                # table_index = self.state_to_index[current_state]\n",
    "                # Epsilon-greedy action selection\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                    # print(f\"random action: {action}, exploring\")\n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[current_state])\n",
    "                    # print(f\"greedy action: {action}, exploiting\")\n",
    "\n",
    "\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "                # print(f\"next_state: {next_state}\")\n",
    "                # print(f\"state:{self.state_index_to_position(next_state)}\")\n",
    "\n",
    "                # if next_state in goal_states:\n",
    "                #     # print(f'reached goal state: {next_state}, not in q table, skipping')\n",
    "                #     # self.Q_table[table_index, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_table_index]) - self.Q_table[table_index, action])\n",
    "                    # break\n",
    "\n",
    "                # if next_state not in self.state_to_index:\n",
    "                #     continue\n",
    "                # # Convert sparse index to dense index\n",
    "                # next_table_index = self.state_to_index[next_state]\n",
    "                #reward (-1 for each step thats not the gaol)\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "                # if next_state in goal_states:\n",
    "                #     reward = 0  # High reward for reaching the goal\n",
    "                # elif action in [0, 1]:  # Turning left or right\n",
    "                #     reward = -1.5  # Higher penalty for turning\n",
    "                # else:  # Moving forward\n",
    "                #     reward = -1  # Regular step penalty\n",
    "\n",
    "                #Q valye update rule\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "                #update state\n",
    "                if next_state in goal_states:\n",
    "                    break\n",
    "                \n",
    "                current_state = next_state\n",
    "        \n",
    "        \n",
    "    def run_policy(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q table. Convert state in policy to its position using \"\"\"\n",
    "        \n",
    "        self.env.reset()[0] #reset the environment and get initial state\n",
    "        current_state = self.position_to_state_index() #convert starting position to index\n",
    "        done = False\n",
    "        while not done:\n",
    " \n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action from policy\n",
    "            position, _, done, _, _ = self.env.step(action) #take the action\n",
    "            next_state = self.position_to_state_index() #use this next position , decode the state\n",
    "            self.env.render()\n",
    "\n",
    "            #update current state\n",
    "            current_state = next_state\n",
    "\n",
    "        \n",
    "    def run_policy_1(self):\n",
    "        \"\"\"Run the trained policy from the Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        step_count = 0  # To track movement\n",
    "\n",
    "        while not done:\n",
    "            # Debugging: Print current state and Q-values\n",
    "            print(f\"Step {step_count}: State {current_state}\")\n",
    "            print(f\"Q-values: {self.Q_table[current_state]}\")\n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action\n",
    "            print(f\"Chosen action: {action}\")\n",
    "\n",
    "            self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert to state index\n",
    "            \n",
    "            # Debugging: Check if the agent is looping\n",
    "            if next_state == current_state:\n",
    "                print(f\"⚠️ Warning: Agent is stuck! Current state {current_state} is the same as next state {next_state}.\")\n",
    "                break  # Stop infinite loops\n",
    "\n",
    "            self.env.render()  # Visualiz\n",
    "\n",
    "\n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and step_count < 100:  # Prevent infinite loops\n",
    "            # Debugging: Print current state and Q-values\n",
    "            print(f\"Step {step_count}: State {current_state}\")\n",
    "            print(f\"Q-values: {self.Q_table[current_state]}\")\n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action\n",
    "            print(f\"Chosen action: {action}\")\n",
    "\n",
    "            # Before taking the step, print agent's current position\n",
    "            print(f\"Before step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "\n",
    "            # After step, print agent's new position\n",
    "            print(f\"After step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "\n",
    "            # Detect if the agent is looping in the same state\n",
    "            if next_state == current_state:\n",
    "                print(f\"⚠️ Warning: Agent is stuck! Current state {current_state} is the same as next state {next_state}.\")\n",
    "                break  # Prevent infinite loop\n",
    "\n",
    "            self.env.render()  # Visualize movement\n",
    "            current_state = next_state  # Update current state\n",
    "            step_count += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Q-table into a DataFrame for better readability\n",
    "q_table_df = pd.DataFrame(q_training.Q_table, columns=[f\"Action {i}\" for i in range(3)])\n",
    "\n",
    "# Add a column for the corresponding grid position\n",
    "positions = [q_training.state_index_to_position(state_idx) for state_idx in range(q_training.Q_table.shape[0])]\n",
    "q_table_df[\"Grid Position\"] = positions  # Append positions to Q-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action 0</th>\n",
       "      <th>Action 1</th>\n",
       "      <th>Action 2</th>\n",
       "      <th>Grid Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.388735</td>\n",
       "      <td>-1.275383</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>(1, 1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.246242</td>\n",
       "      <td>9.989028</td>\n",
       "      <td>-1.115199</td>\n",
       "      <td>(1, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.999989</td>\n",
       "      <td>-1.671758</td>\n",
       "      <td>-1.640043</td>\n",
       "      <td>(1, 1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.998837</td>\n",
       "      <td>8.869096</td>\n",
       "      <td>8.839226</td>\n",
       "      <td>(1, 1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.876451</td>\n",
       "      <td>9.998894</td>\n",
       "      <td>-0.330318</td>\n",
       "      <td>(2, 1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-1.773552</td>\n",
       "      <td>9.988545</td>\n",
       "      <td>-1.719612</td>\n",
       "      <td>(7, 8, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>9.884074</td>\n",
       "      <td>-1.726055</td>\n",
       "      <td>-1.687354</td>\n",
       "      <td>(8, 8, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>8.830386</td>\n",
       "      <td>9.893532</td>\n",
       "      <td>-1.725284</td>\n",
       "      <td>(8, 8, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-1.352983</td>\n",
       "      <td>-1.294881</td>\n",
       "      <td>9.998960</td>\n",
       "      <td>(8, 8, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-1.583119</td>\n",
       "      <td>8.857118</td>\n",
       "      <td>8.957594</td>\n",
       "      <td>(8, 8, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Action 0  Action 1  Action 2 Grid Position\n",
       "0   -1.388735 -1.275383  0.439800     (1, 1, 0)\n",
       "1   -1.246242  9.989028 -1.115199     (1, 1, 1)\n",
       "2    9.999989 -1.671758 -1.640043     (1, 1, 2)\n",
       "3    9.998837  8.869096  8.839226     (1, 1, 3)\n",
       "4    8.876451  9.998894 -0.330318     (2, 1, 0)\n",
       "..        ...       ...       ...           ...\n",
       "251 -1.773552  9.988545 -1.719612     (7, 8, 3)\n",
       "252  9.884074 -1.726055 -1.687354     (8, 8, 0)\n",
       "253  8.830386  9.893532 -1.725284     (8, 8, 1)\n",
       "254 -1.352983 -1.294881  9.998960     (8, 8, 2)\n",
       "255 -1.583119  8.857118  8.957594     (8, 8, 3)\n",
       "\n",
       "[256 rows x 4 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38873517, -1.27538339,  0.43980017],\n",
       "       [-1.24624164,  9.98902788, -1.11519908],\n",
       "       [ 9.9999887 , -1.67175761, -1.64004299],\n",
       "       [ 9.99883732,  8.86909607,  8.8392259 ],\n",
       "       [ 8.8764506 ,  9.9988938 , -0.33031763],\n",
       "       [-0.72506238, -1.23625643,  8.94438356],\n",
       "       [ 9.88947658,  8.90589755,  1.21337273],\n",
       "       [-1.18474097, -0.60248588, -1.49751044],\n",
       "       [-1.55393312,  9.999892  ,  1.75276699],\n",
       "       [ 9.9998844 , -1.09050745, -0.59140829],\n",
       "       [ 8.88574059, -1.57012407, -1.42730551],\n",
       "       [-1.3997368 ,  8.95485493, -1.63484482],\n",
       "       [ 3.48226218,  1.59804248,  0.89335515],\n",
       "       [-1.2725398 ,  4.54084357, -1.38243347],\n",
       "       [-1.59602028,  8.84259749, -0.45233968],\n",
       "       [ 9.94224357,  2.95448595,  9.35341573],\n",
       "       [-1.45211548, -1.14664006,  1.32642347],\n",
       "       [ 0.22875399, -1.23571731, -0.53484817],\n",
       "       [ 0.49978413, -1.53867411, -1.67445008],\n",
       "       [-1.35996679, -0.58186544, -1.55131605],\n",
       "       [-0.19065314,  0.59543225,  4.05454   ],\n",
       "       [ 9.93697176,  0.83625107, -0.58169532],\n",
       "       [ 0.30955104,  1.9479417 , -0.62002499],\n",
       "       [ 0.62726083,  3.77919097, -0.90687136],\n",
       "       [ 1.00000321,  1.        , 10.        ],\n",
       "       [ 4.        , -0.45922772, -0.1125157 ],\n",
       "       [ 1.00891   ,  1.00000003,  0.99195903],\n",
       "       [-0.49985729,  4.54      ,  1.00294019],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.70502066,  9.9895381 , -0.69254489],\n",
       "       [ 9.90723521, -0.1743607 , -0.89828681],\n",
       "       [ 8.94023677, -1.12432949, -1.04034328],\n",
       "       [ 9.89138018,  9.9896358 , -1.51212954],\n",
       "       [-0.8485313 ,  9.98997734, -0.35136796],\n",
       "       [-0.61322707,  8.92236284,  8.86655112],\n",
       "       [ 9.98945701, -0.9871096 , -1.27386424],\n",
       "       [ 8.87503712,  9.99990013, -1.53284602],\n",
       "       [ 9.17985912, -0.21006972, -0.42948626],\n",
       "       [ 9.40131924,  8.85708189, -1.2335168 ],\n",
       "       [ 3.09420757,  9.94043899, -0.96271698],\n",
       "       [-1.2633267 ,  1.04072733,  9.8884217 ],\n",
       "       [ 9.98955128, -1.52870944, -0.53672407],\n",
       "       [ 9.8936133 , -1.34613359, -1.37908652],\n",
       "       [-0.54588622, -1.30645707,  9.13326528],\n",
       "       [-1.30811095,  9.04583478, -1.48985322],\n",
       "       [ 0.72583248, -0.16472282,  9.99894104],\n",
       "       [ 8.95649567,  9.99908587, -0.45970773],\n",
       "       [-1.27961343,  0.9544747 , -0.36118032],\n",
       "       [ 9.13304718, -0.60406875,  3.44766094],\n",
       "       [ 0.56868066,  0.66356782, -0.09967717],\n",
       "       [-0.21397848, -0.11631873, -0.39684889],\n",
       "       [ 0.74072251, -0.30966736,  9.91005776],\n",
       "       [-0.20514512,  9.07681643,  8.95407891],\n",
       "       [-0.4956502 , -1.01732446, -1.01000209],\n",
       "       [-0.50384534, -0.97201854, -1.00473331],\n",
       "       [-0.88915452, -1.07901038, -0.11301977],\n",
       "       [-0.62029882, -0.88853885,  3.29642662],\n",
       "       [ 4.000594  ,  3.08788872,  0.98544433],\n",
       "       [ 1.08447496,  3.16942926, -0.17548045],\n",
       "       [-0.47920035, -0.60905484,  0.05356306],\n",
       "       [-0.22373093,  1.02110804, 10.        ],\n",
       "       [ 9.89205024,  8.89168111, -1.49124192],\n",
       "       [ 8.86835158,  9.99888892, -1.47877919],\n",
       "       [-1.2585121 ,  8.89202905, -1.11689169],\n",
       "       [-1.49180073, -0.31865633,  0.82017213],\n",
       "       [-1.43467661,  8.84228877,  9.9990376 ],\n",
       "       [ 9.99999989,  8.86730775, -1.38858976],\n",
       "       [-1.53760269,  9.99988536, -0.66182649],\n",
       "       [ 9.98897073,  8.8445613 ,  1.63285582],\n",
       "       [-1.24791857,  8.84478197,  8.86435784],\n",
       "       [ 8.86749003,  9.98859628, -1.45451192],\n",
       "       [ 8.84381676,  9.99988801, -1.27083331],\n",
       "       [-0.85375451,  9.99998847,  3.13048154],\n",
       "       [ 9.88452198, -1.65498637, -1.61398615],\n",
       "       [ 9.99884165, -1.5137041 , -1.63039661],\n",
       "       [ 9.98860828, -1.45380636, -0.5921555 ],\n",
       "       [ 9.99988856, -1.68429169,  9.88421699],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 9.90622314, -1.37824791, -1.42928211],\n",
       "       [-1.32425774,  9.99886954, -1.73204468],\n",
       "       [-1.49713433, -0.89923401,  9.98923727],\n",
       "       [ 9.9895897 , -0.71412602, -0.52752643],\n",
       "       [ 9.99890957,  8.85668022, -1.47662734],\n",
       "       [-1.39844823, -1.36507835,  9.8872799 ],\n",
       "       [ 8.89835506, -1.17141675,  3.44191301],\n",
       "       [ 9.89253658, -1.32294252, -1.37563909],\n",
       "       [-1.57515341,  9.88961101, -1.41769626],\n",
       "       [ 9.99885377,  9.8851096 ,  8.84575491],\n",
       "       [-1.42651958,  9.99886877, -1.2863237 ],\n",
       "       [ 9.99999888,  8.86538651, -1.27335294],\n",
       "       [ 9.99886083,  8.84210473, -1.68684668],\n",
       "       [-1.51692764,  8.84851527, -1.61921618],\n",
       "       [ 9.98867685, -1.59247537, -1.63344751],\n",
       "       [-1.38417699, -1.58234603,  1.63983441],\n",
       "       [ 8.83998571,  9.98852436, -1.80502226],\n",
       "       [-1.31631325,  8.84895916,  9.98838148],\n",
       "       [-1.54557043, -1.67467352,  9.14588704],\n",
       "       [-1.73469955,  9.8845692 , -1.67299228],\n",
       "       [ 9.99998831,  8.81863413,  8.81648762],\n",
       "       [ 8.82581643, -1.81983607,  9.88297321],\n",
       "       [-1.82609556,  8.81662033,  0.43298099],\n",
       "       [ 8.8248648 , -1.80842722,  8.81654907],\n",
       "       [ 9.99882444,  8.82077404, -1.74687351],\n",
       "       [ 9.88336924, -1.74717671, -0.36939767],\n",
       "       [-1.75238312,  8.84720844, -1.82146433],\n",
       "       [-1.71743747, -1.59696013,  4.09934366],\n",
       "       [ 8.81328643,  9.88226786, -1.75715972],\n",
       "       [ 8.82055943,  9.99999988, -1.8431902 ],\n",
       "       [-1.81110829, -1.80714569, -0.14231994],\n",
       "       [ 9.98814368, -1.8639506 , -1.8570711 ],\n",
       "       [ 9.8864632 , -1.57279398,  8.87951694],\n",
       "       [ 9.99883423,  9.88232602, -1.76919427],\n",
       "       [-1.76313577,  8.84622358,  0.55247785],\n",
       "       [-1.74521525, -1.45413965,  8.88077873],\n",
       "       [-1.10225264,  9.99988812, -0.59778665],\n",
       "       [ 8.86707231,  8.85535705,  8.838995  ],\n",
       "       [ 8.84182204,  9.98849531,  0.43488198],\n",
       "       [ 9.88546112, -1.31920136,  0.44028531],\n",
       "       [-1.0720934 ,  9.99988528, -0.32740438],\n",
       "       [-1.31398584,  8.87650163, -0.56673606],\n",
       "       [-1.46817335,  9.8896669 , -1.29793894],\n",
       "       [-1.17777786,  8.92413713, -1.15356367],\n",
       "       [-1.69148988,  9.99884569, -1.56015842],\n",
       "       [ 9.98840604, -1.5176291 , -1.61916528],\n",
       "       [ 9.88533246,  8.8742915 , -1.25517096],\n",
       "       [ 9.98838009, -1.64581623, -0.3844537 ],\n",
       "       [-1.60518257, -1.75129499,  9.88395629],\n",
       "       [ 9.88224022,  8.84362377, -1.70280705],\n",
       "       [ 8.85887758,  8.84304111,  9.88717916],\n",
       "       [-1.58577782,  8.82050215, -1.43096441],\n",
       "       [-1.71616686,  9.8830621 ,  8.82758608],\n",
       "       [-1.78693041, -1.41338928, -0.41996438],\n",
       "       [ 8.8305423 ,  9.99988536, -1.49495747],\n",
       "       [ 9.99883859,  8.82354573,  8.82994925],\n",
       "       [ 8.8241114 ,  9.99999885, -0.62154674],\n",
       "       [-1.47934536, -1.73698415, -0.38870522],\n",
       "       [ 8.83503501,  9.98847936, -1.72067572],\n",
       "       [-1.64599433, -1.66593735,  4.21789854],\n",
       "       [-1.84570288,  9.88141391, -1.83700169],\n",
       "       [ 8.82139954, -1.86925567, -1.8108363 ],\n",
       "       [ 9.98825725, -1.847924  ,  8.82512091],\n",
       "       [ 9.9998818 , -1.84538575,  0.43775292],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 8.82167743, -1.74018997, -1.81485804],\n",
       "       [-1.75159743,  8.83870932, -0.61972139],\n",
       "       [ 9.88205125, -1.79779917, -1.86701685],\n",
       "       [-1.87360039, -1.80399413,  8.8398433 ],\n",
       "       [ 8.82797584,  8.82090819, -1.77988203],\n",
       "       [ 9.88187297, -1.8146199 , -1.82939736],\n",
       "       [-1.85895047, -1.83510394, -1.82348481],\n",
       "       [-1.82251102, -1.8516765 ,  0.71136483],\n",
       "       [ 9.99988302,  9.88232661,  8.82477577],\n",
       "       [ 8.82531254,  9.99988444, -1.79551944],\n",
       "       [-1.57616893,  9.88446459, -0.63079207],\n",
       "       [-1.74652402,  9.99999884, -1.63974664],\n",
       "       [ 9.9988209 , -1.7425696 , -0.47315421],\n",
       "       [ 9.99882795, -1.6850893 , -1.70125317],\n",
       "       [ 9.88204799, -1.74151839, -0.56891591],\n",
       "       [-1.68105438,  8.82306504, -1.90153584],\n",
       "       [-1.68145565,  9.98849824,  0.35569814],\n",
       "       [-1.71798127,  9.9988455 , -1.62167455],\n",
       "       [-1.70494594, -1.7610256 ,  8.8448334 ],\n",
       "       [ 8.84132059, -1.69973301, -1.76341886],\n",
       "       [-1.66320977,  8.83935484, -1.79279385],\n",
       "       [ 8.82401774, -1.74110953,  8.83983899],\n",
       "       [-1.7889954 , -1.71868842,  0.63134863],\n",
       "       [-1.8028249 , -1.74449365, -0.08713393],\n",
       "       [ 8.80990463,  9.98816133, -0.60520933],\n",
       "       [ 8.81857529,  9.99998823, -1.80694678],\n",
       "       [ 8.81308468, -1.83062601,  8.8170995 ],\n",
       "       [ 9.88107646,  8.80959067, -1.90127144],\n",
       "       [-1.93571882,  9.99881214, -1.80944471],\n",
       "       [ 8.81151021,  8.81538562,  0.55001978],\n",
       "       [-1.88821937, -1.91588731,  0.43475594],\n",
       "       [-1.92218875,  9.99881056, -1.89484309],\n",
       "       [ 9.99988114, -1.78126033, -0.40761709],\n",
       "       [-1.86476672, -1.70398294,  9.9988586 ],\n",
       "       [-1.73369279,  9.98822992, -1.86444196],\n",
       "       [ 9.98814582, -1.8649553 , -1.79964765],\n",
       "       [-1.69306046,  8.84697694,  9.98822386],\n",
       "       [ 9.88334046, -1.71171149, -1.80194248],\n",
       "       [-1.80224087,  8.83178278, -1.85468056],\n",
       "       [ 9.98827134, -1.75075063, -1.8064947 ],\n",
       "       [-1.73677999, -1.72207602, -1.58411379],\n",
       "       [-1.72878111,  9.98834535, -1.79418932],\n",
       "       [ 9.88583366,  9.88411986,  8.84520172],\n",
       "       [ 8.82573946, -1.69119638, -1.78328807],\n",
       "       [ 9.98853772, -1.54327147,  8.84454925],\n",
       "       [-1.48128291, -1.48067953, -1.40663111],\n",
       "       [-1.0040248 ,  9.88496868, -0.5167233 ],\n",
       "       [ 9.99889875,  9.88808755, -1.82672966],\n",
       "       [ 8.88256923,  8.83083918, -1.51965   ],\n",
       "       [-1.52609506,  9.88597581, -0.60264629],\n",
       "       [ 8.84738893,  9.88860911, -1.5375678 ],\n",
       "       [-1.32359278, -1.28709075,  0.53892242],\n",
       "       [-1.65202926,  8.82022007, -1.79622778],\n",
       "       [ 8.82093212,  8.82896382, -1.77474502],\n",
       "       [ 9.98838342, -1.76541642, -1.54282335],\n",
       "       [-1.73385876,  9.88460236, -1.80891962],\n",
       "       [-1.67106631,  9.98819879,  0.36098402],\n",
       "       [ 8.82006678,  9.99883   , -1.7550514 ],\n",
       "       [ 8.82695413,  9.98831527,  8.93869427],\n",
       "       [ 8.8356272 , -1.63189074, -0.5260771 ],\n",
       "       [-1.73376866,  9.99881786, -0.78907915],\n",
       "       [ 8.82917825, -1.73707001,  8.82888861],\n",
       "       [-1.48902709, -1.6517243 , -1.67260454],\n",
       "       [-1.53732407,  9.99882253, -0.85013138],\n",
       "       [ 8.88546625, -1.39468374, -1.78235396],\n",
       "       [ 8.92825147,  8.93795013, -0.60478392],\n",
       "       [-1.48914017, -1.29971943,  0.44040211],\n",
       "       [-1.13606766,  9.99893258, -1.88475214],\n",
       "       [ 8.82395166, -1.78952681, -0.39166543],\n",
       "       [ 9.88197361,  9.88319949,  8.81795407],\n",
       "       [ 9.8845345 , -1.68080782,  8.94971931],\n",
       "       [ 9.99884629, -1.80809312, -1.74106788],\n",
       "       [-1.71107067,  8.82000156, -1.70651933],\n",
       "       [ 9.99998819,  9.88177697,  8.81840057],\n",
       "       [-1.84853795, -1.68262709,  8.86245987],\n",
       "       [-1.77355175,  9.98854544, -1.71961226],\n",
       "       [ 9.88407406, -1.7260552 , -1.68735372],\n",
       "       [ 8.83038638,  9.8935321 , -1.72528394],\n",
       "       [-1.35298261, -1.29488102,  9.99895973],\n",
       "       [-1.58311887,  8.85711777,  8.9575937 ]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_training.Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_training.state_index_to_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode=None) \n",
    "env.reset()\n",
    "q_training = Qlearning(env)\n",
    "q_training.train(800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Q_table_epochs_600000_e_95_discount_9_learnrate_9' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "Q_table_epochs_600000_e_95_discount_9_learnrate_9= q_training.Q_table\n",
    "%store Q_table_epochs_600000_e_95_discount_9_learnrate_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(225)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_training.position_to_state_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minigrid.core.world_object.Wall at 0x1474e7c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.grid.get(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.agent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.agent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: State 224\n",
      "Q-values: [-8.90572652 -8.8981852  -8.64914828]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (1, 8), Dir: 0\n",
      "After step: Agent Pos: (np.int64(2), np.int64(8)), Dir: 0\n",
      "Step 1: State 228\n",
      "Q-values: [-8.78344162 -8.78423345 -8.49905365]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(2), np.int64(8)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(3), np.int64(8)), Dir: 0\n",
      "Step 2: State 232\n",
      "Q-values: [-8.64907598 -8.64889383 -8.33228183]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(3), np.int64(8)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(4), np.int64(8)), Dir: 0\n",
      "Step 3: State 236\n",
      "Q-values: [-8.49903871 -8.49905365 -8.14697981]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(8)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(5), np.int64(8)), Dir: 0\n",
      "Step 4: State 240\n",
      "Q-values: [-7.94108868 -8.33228183 -8.33228183]\n",
      "Chosen action: 0\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(8)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(5), np.int64(8)), Dir: 3\n",
      "Step 5: State 243\n",
      "Q-values: [-8.14697981 -8.14697981 -7.71232075]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(8)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(5), np.int64(7)), Dir: 3\n",
      "Step 6: State 211\n",
      "Q-values: [-7.94108868 -7.94108868 -7.45813417]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(7)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(5), np.int64(6)), Dir: 3\n",
      "Step 7: State 179\n",
      "Q-values: [-7.71232075 -7.71232075 -7.17570464]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(6)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(5), np.int64(5)), Dir: 3\n",
      "Step 8: State 147\n",
      "Q-values: [-6.86189404 -7.45813417 -6.86189404]\n",
      "Chosen action: 0\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(5)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(5), np.int64(5)), Dir: 2\n",
      "Step 9: State 146\n",
      "Q-values: [-7.17570464 -7.17570464 -6.5132156 ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(5)), Dir: 2\n",
      "After step: Agent Pos: (np.int64(4), np.int64(5)), Dir: 2\n",
      "Step 10: State 142\n",
      "Q-values: [-6.86189404 -6.12579511 -6.86189404]\n",
      "Chosen action: 1\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(5)), Dir: 2\n",
      "After step: Agent Pos: (np.int64(4), np.int64(5)), Dir: 3\n",
      "Step 11: State 143\n",
      "Q-values: [-6.5132156 -6.5132156 -5.6953279]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(5)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(4), np.int64(4)), Dir: 3\n",
      "Step 12: State 111\n",
      "Q-values: [-6.12579511 -6.12579511 -5.217031  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(4)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(4), np.int64(3)), Dir: 3\n",
      "Step 13: State 79\n",
      "Q-values: [-5.6953279 -5.6953279 -4.68559  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(3)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(4), np.int64(2)), Dir: 3\n",
      "Step 14: State 47\n",
      "Q-values: [-5.217031 -4.68559  -4.0951  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(2)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(4), np.int64(1)), Dir: 3\n",
      "Step 15: State 15\n",
      "Q-values: [-4.68559 -3.439   -4.0951 ]\n",
      "Chosen action: 1\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(1)), Dir: 3\n",
      "After step: Agent Pos: (np.int64(4), np.int64(1)), Dir: 0\n",
      "Step 16: State 12\n",
      "Q-values: [-4.0951 -4.0951 -2.71  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(4), np.int64(1)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(5), np.int64(1)), Dir: 0\n",
      "Step 17: State 16\n",
      "Q-values: [-3.439 -3.439 -1.9  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(5), np.int64(1)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(6), np.int64(1)), Dir: 0\n",
      "Step 18: State 20\n",
      "Q-values: [-2.71 -2.71 -1.  ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(6), np.int64(1)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(7), np.int64(1)), Dir: 0\n",
      "Step 19: State 24\n",
      "Q-values: [-1.9 -1.9  0. ]\n",
      "Chosen action: 2\n",
      "Before step: Agent Pos: (np.int64(7), np.int64(1)), Dir: 0\n",
      "After step: Agent Pos: (np.int64(8), np.int64(1)), Dir: 0\n"
     ]
    }
   ],
   "source": [
    "env_human = SimpleEnv(render_mode=\"human\") #make same env but in human mode \n",
    "\n",
    "q_training.env= env_human #switch out the env in q training with human env\n",
    "q_training.run_policy_2() #run the policy in the human env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_human.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_training.state_indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, state in enumerate(q_training.state_indexes_list):\n",
    "    x, y, _ = q_training.state_index_to_position(state) \n",
    "    print(f\"State index {i}: Position ({x}, {y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Q_table' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "Q_table = q_training.Q_table\n",
    "%store Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = q_training.Q_table\n",
    "def state_index_to_position(state_idx):\n",
    "    \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "    grid_width = 8\n",
    "        \n",
    "    direction = state_idx % 4\n",
    "    linear_idx = state_idx // 4\n",
    "\n",
    "    y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "    return x+1, y+1, direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Learning with Policy Updates each Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning_policy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        learning_rate = 0.9,\n",
    "        discount_factor = 0.95,\n",
    "        epsilon = 0.95,\n",
    "        epochs = 1000\n",
    "         \n",
    "    ):\n",
    "        self.env = env # MiniGrid environment called from the class Minigrid\n",
    "        self.num_states = np.int64(env.count_states())\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q_table = np.zeros((((env.width -2) * (env.height -2) *4), self.num_actions)) # Q table has goal states in it as well\n",
    "        # print(f\"Q table shape: {self.Q_table.shape}\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        self.epsilon =  epsilon  # exploration probability\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "        self.allowed_state_idx = self.find_state_indexes(env)\n",
    "        # print(f\"allowed state indexes: {self.allowed_state_idx}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def position_to_state_index(self, tuple_position = None): \n",
    "        grid_width = self.env.grid.width -2\n",
    "        if tuple_position is None:\n",
    "            direction = self.env.agent_dir\n",
    "            x, y = self.env.agent_pos\n",
    "\n",
    "        else:\n",
    "            if not isinstance(tuple_position, tuple) or len(tuple_position) != 3:\n",
    "                raise ValueError(f\"Invalid position format: {tuple_position}\")\n",
    "            x, y, direction = tuple_position\n",
    "        return np.int64(((y-1) * grid_width + (x-1)) * 4 + direction) \n",
    "    \n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width-2\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x+1, y+1, direction\n",
    "    \n",
    "    def find_state_indexes(self, env):\n",
    "        \"\"\"Counts all states except walls and barriers\"\"\"\n",
    "        state_indexes_list = []\n",
    "        for x in range(1, env.grid.width-1):\n",
    "            for y in range(1, env.grid.height-1):\n",
    "                if env.grid.get(x, y) is None: #grabs all empty spaces\n",
    "                    for direction in range(4):\n",
    "                        state_index = self.position_to_state_index((x, y, direction))\n",
    "                        state_indexes_list.append(state_index)\n",
    "        return state_indexes_list        \n",
    "    \n",
    "    \n",
    "    def train(self, epochs):\n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            current_state = np.random.choice(self.allowed_state_idx)\n",
    "            # print(f\"Epoch {epoch}: Starting state: {current_state}\")\n",
    "            while current_state not in goal_states:\n",
    "                # #replace current index with its table index\n",
    "                # table_index = self.state_to_index[current_state]\n",
    "                # Epsilon-greedy action selection\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                    # print(f\"random action: {action}, exploring\")\n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[current_state])\n",
    "                    # print(f\"greedy action: {action}, exploiting\")\n",
    "\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index()\n",
    "                # print(f\"next_state: {next_state}\")\n",
    "\n",
    "                #reward (-1 for each step thats not the gaol)\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                #Q valye update rule\n",
    "                self.Q_table[current_state, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_state]) - self.Q_table[current_state, action])\n",
    "\n",
    "                #update state\n",
    "                current_state = next_state\n",
    "        \n",
    "        \n",
    "    def run_policy(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q table. Convert state in policy to its position using \"\"\"\n",
    "        \n",
    "        self.env.reset()[0] #reset the environment and get initial state\n",
    "        current_state = self.position_to_state_index() #convert starting position to index\n",
    "        done = False\n",
    "        while not done:\n",
    " \n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action from policy\n",
    "            position, _, done, _, _ = self.env.step(action) #take the action\n",
    "            next_state = self.position_to_state_index() #use this next position , decode the state\n",
    "            self.env.render()\n",
    "\n",
    "            #update current state\n",
    "            current_state = next_state\n",
    "\n",
    "        \n",
    "    def run_policy_1(self):\n",
    "        \"\"\"Run the trained policy from the Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        step_count = 0  # To track movement\n",
    "\n",
    "        while not done:\n",
    "            # Debugging: Print current state and Q-values\n",
    "            print(f\"Step {step_count}: State {current_state}\")\n",
    "            print(f\"Q-values: {self.Q_table[current_state]}\")\n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action\n",
    "            print(f\"Chosen action: {action}\")\n",
    "\n",
    "            self.env.step(action)  # Take action\n",
    "            next_state = self.position_to_state_index()  # Convert to state index\n",
    "            \n",
    "            # Debugging: Check if the agent is looping\n",
    "            if next_state == current_state:\n",
    "                print(f\"⚠️ Warning: Agent is stuck! Current state {current_state} is the same as next state {next_state}.\")\n",
    "                break  # Stop infinite loops\n",
    "\n",
    "            self.env.render()  # Visualiz\n",
    "\n",
    "\n",
    "    def run_policy_2(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q-table.\"\"\"\n",
    "\n",
    "        self.env.reset()[0]  # Reset environment\n",
    "        current_state = self.position_to_state_index()  # Convert starting position to index\n",
    "        done = False\n",
    "        step_count = 0  # Track steps to prevent infinite loops\n",
    "\n",
    "        while not done and step_count < 100:  # Prevent infinite loops\n",
    "            # Debugging: Print current state and Q-values\n",
    "            print(f\"Step {step_count}: State {current_state}\")\n",
    "            print(f\"Q-values: {self.Q_table[current_state]}\")\n",
    "            \n",
    "            action = np.argmax(self.Q_table[current_state])  # Choose best action\n",
    "            print(f\"Chosen action: {action}\")\n",
    "\n",
    "            # Before taking the step, print agent's current position\n",
    "            print(f\"Before step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "\n",
    "            next_obs, _, done, _, _ = self.env.step(action)  # Take action\n",
    "\n",
    "            # After step, print agent's new position\n",
    "            print(f\"After step: Agent Pos: {self.env.agent_pos}, Dir: {self.env.agent_dir}\")\n",
    "\n",
    "            next_state = self.position_to_state_index()  # Convert new state\n",
    "\n",
    "            # Detect if the agent is looping in the same state\n",
    "            if next_state == current_state:\n",
    "                print(f\"⚠️ Warning: Agent is stuck! Current state {current_state} is the same as next state {next_state}.\")\n",
    "                break  # Prevent infinite loop\n",
    "\n",
    "            self.env.render()  # Visualize movement\n",
    "            current_state = next_state  # Update current state\n",
    "            step_count += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        learning_rate = 0.8,\n",
    "        discount_factor = 0.9,\n",
    "        epsilon = 0.2,\n",
    "        epochs = 1000\n",
    "         \n",
    "    ):\n",
    "        self.env = env # MiniGrid environment called from the class Minigrid\n",
    "        self.num_states = np.int64(env.count_states())\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q_table = np.zeros((self.num_states, self.num_actions)) # Q table\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor # discount factor\n",
    "        self.epsilon =  epsilon  # exploration probability\n",
    "        self.epochs = epochs\n",
    "        self.policy = np.full((self.num_states, self.num_actions) , 1/self.num_actions)\n",
    "\n",
    "     # Store state indexes at initialization\n",
    "        self.state_indexes_list = self.find_state_indexes(env)\n",
    "        print(f'state indexes list: {self.state_indexes_list}')\n",
    "     # making a dictionary to convert state index to index\n",
    "        self.state_to_index = {state: i for i, state in enumerate(self.state_indexes_list)}\n",
    "        print(f'state to index: {self.state_to_index}')\n",
    "    \n",
    "    def position_to_state_index(self, obs):\n",
    "        \"\"\"Converts an observation (either (x, y, direction) or an image-based dict) into a state index.\"\"\"\n",
    "        grid_width = self.env.grid.width\n",
    "        \n",
    "        # Case 1: Observation is a dictionary (image-based)\n",
    "        if isinstance(obs, dict) and 'image' in obs:\n",
    "            x, y = self.env.agent_pos  # Extract (x, y) directly from the environment\n",
    "            direction = self.env.agent_dir  # Extract direction\n",
    "        # Case 2: Observation is already in (x, y, direction) format\n",
    "        elif isinstance(obs, tuple) and len(obs) == 3:\n",
    "            x, y, direction = obs\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid observation format: {obs}\")\n",
    "\n",
    "        # Convert (x, y, direction) to a unique state index\n",
    "        return np.int64((y * grid_width + x) * 4 + direction)\n",
    "    \n",
    "    def state_index_to_position(self, state_idx):\n",
    "        \"\"\"Converts a scalar state index back into (x, y, direction).\"\"\"\n",
    "        grid_width = self.env.grid.width\n",
    "        \n",
    "        direction = state_idx % 4\n",
    "        linear_idx = state_idx // 4\n",
    "\n",
    "        y, x = divmod(linear_idx, grid_width)  # Convert to (x, y)\n",
    "        \n",
    "        return x, y, direction\n",
    "    \n",
    "    def find_state_indexes(self, env):\n",
    "        state_indexes_list = []\n",
    "        for x in range(env.grid.width):\n",
    "            for y in range(env.grid.height):\n",
    "                if env.grid.get(x, y) is None: #grabs all empty spaces\n",
    "                    for direction in range(4):\n",
    "                        state_index = self.position_to_state_index((x, y, direction))\n",
    "                        state_indexes_list.append(state_index)\n",
    "        return state_indexes_list        \n",
    "    \n",
    "    \n",
    "    def train(self, epochs):\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        goal_states = [self.position_to_state_index((8, 1, d)) for d in range(4)] #goal state index\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            current_state = np.random.choice(self.state_indexes_list)\n",
    "\n",
    "            while current_state not in goal_states:\n",
    "                #replace current index with its table index\n",
    "                table_index = self.state_to_index[current_state]\n",
    "                # select an action at random from the policy!\n",
    "                action = self.env.action_space.sample()\n",
    "                print(f\"random action: {action}\")\n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[table_index])\n",
    "                    # print(f\"greedy action: {action}, exploiting\")\n",
    "\n",
    "                #transition to the next state\n",
    "                next_obs, _, done, _, _ = self.env.step(action)\n",
    "                # print(f\"next_observation: {next_obs}\")\n",
    "                next_state = self.position_to_state_index(next_obs)\n",
    "                print(f\"next_state: {next_state}\")\n",
    "\n",
    "                if next_state in goal_states:\n",
    "                    print(f'reached goal state: {next_state}, not in q table, skipping')\n",
    "                    continue\n",
    "\n",
    "                if next_state not in self.state_to_index:\n",
    "                    continue\n",
    "                # Convert sparse index to dense index\n",
    "                next_table_index = self.state_to_index[next_state]\n",
    "\n",
    "                #reward (-1 for each step thats not the gaol)\n",
    "                reward = 0 if next_state in goal_states else -1\n",
    "\n",
    "                #Q valye update rule\n",
    "                self.Q_table[table_index, action] += self.learning_rate * (reward + self.discount_factor * np.max(self.Q_table[next_table_index]) - self.Q_table[table_index, action])\n",
    "\n",
    "                #update state\n",
    "                current_state = next_state\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    def run_policy(self):\n",
    "        \"\"\"Run the environment using the learned policy from a Q table. Convert state in policy to its position using \"\"\"\n",
    "        \n",
    "        start_pos = self.env.reset()[0] #reset the environment and get initial state\n",
    "        state_idx = self.position_to_state_index(start_pos) #convert starting position to index\n",
    "        done = False\n",
    "        while not done:\n",
    "            x, y, direction = self.state_index_to_position(state_idx) \n",
    "            self.env.place_agent((x, y))\n",
    "            self.env.agent_dir = direction\n",
    "            action = np.argmax(self.Q_table[state_idx])  # Choose best action from policy\n",
    "            position, _, done, _, _ = self.env.step(action) #take the action\n",
    "            state_idx = self.position_to_state_index(position) #use this next position , decode the state\n",
    "            self.env.render()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = SimpleEnv(render_mode=\"human\")\n",
    "free_energy_solver = FreeEnergyMin(env, beta=0.5)\n",
    "# free_energy_solver.estimate_transitions() #for when you want to do a random walk, no learning\n",
    "free_energy_solver.compute_free_energy()\n",
    "free_energy_solver.run_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space:\", env.action_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
