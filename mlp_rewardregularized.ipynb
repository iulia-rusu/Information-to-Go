{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __future__ import should always be first\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Gymnasium & Minigrid imports\n",
    "import gymnasium as gym  # Correct way to import Gymnasium\n",
    "from gymnasium.spaces import Dict, Discrete, Box\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.constants import DIR_TO_VEC\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.actions import Actions\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from gymnasium.utils.play import play\n",
    "import pandas as pd\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            size=10, \n",
    "            agent_start_pos=(1, 8), \n",
    "            agent_start_dir=0, \n",
    "            max_steps=1000, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = (8, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        #create gird\n",
    "        self.grid = Grid(width, height)\n",
    "        #place barrier\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "        #place goal\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "        #place walls\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "        #place agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"find the shortest path\"\n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(render_mode= None)\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs shape after step: (7, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "obs, _, _, _, _ = env.step(action)\n",
    "print(\"Obs shape after step:\", obs[\"image\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs shape after step: (7, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "obs1, _, _, _, _ = env.step(action)\n",
    "print(\"Obs shape after step:\", obs[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False,  True,  True],\n",
       "        [False,  True,  True]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[\"image\"] == obs1[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('direction': Discrete(4), 'image': Box(0, 255, (7, 7, 3), uint8), 'mission': MissionSpace(<function SimpleEnv._gen_mission at 0x16c611440>, None))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGridFlatImg(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Keep only the 7x7 RGB image from a MiniGrid Dict observation.\n",
    "    Output: 147-dim float32 vector in [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # initialise the parent ObservationWrapper so it can do its bookkeeping\n",
    "        super().__init__(env)\n",
    "\n",
    "        img_size = np.prod(env.observation_space[\"image\"].shape)   # 7*7*3 = 147\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(img_size,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        img_flat = obs[\"image\"].astype(np.float32).flatten() / 255.0\n",
    "        return img_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGridReward(gym.Wrapper):\n",
    "    def __init__(self, env, goal_states):\n",
    "        super().__init__(env)\n",
    "        self.goal_states = set(goal_states)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # access agent position after the transition\n",
    "        x, y = self.env.unwrapped.agent_pos\n",
    "        next_state = (x, y)\n",
    "\n",
    "        rew = 0 if next_state in self.goal_states else -1\n",
    "        done = terminated or truncated or (rew == 0)\n",
    "\n",
    "        return obs, rew, done, truncated, info\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapped = MiniGridFlatImg(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapped.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "obs, _, _, _, _ = env_wrapped.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "obs1, _, _, _, _ = env_wrapped.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (147,), float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_wrapped.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_wrapped.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapped_rew= MiniGridReward(env_wrapped, goal_states = [(8, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapped_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env_wrapped_rew.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(obs), \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(type(obs), obs.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation = nn.Tanh, output_activation = nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        act = activation if i < len(sizes) -2 else output_activation #everything but last layer has activation, outherwise output\n",
    "        layers += [nn.Linear(sizes [i], sizes[i +1], act())]\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name = env_wrapped_rew, hidden_sizes = [32], lr = 1e-2, epochs = 50, batch_size = 50, a = 1, render = False):\n",
    "\n",
    "    env = env_name\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0] \n",
    "    # print(\"obs_dim:\", obs_dim)\n",
    "    n_acts = env.action_space.n\n",
    "    # print(\"n_acts:\", n_acts)\n",
    "\n",
    "    # pi(a) setting prior based on the observed distribution of actions being mostly moving forward\n",
    "    pi_a = np.array([1/6, 1/6, 2/3], dtype=np.float32)          # ← prior\n",
    "    # pi_a = np.array([1/3, 1/3, 1/3], dtype= np.float32)\n",
    "    pi_a = torch.tensor(pi_a)  \n",
    "\n",
    "    #generate polucy network\n",
    "    logits_net = mlp(sizes = [obs_dim] + hidden_sizes + [n_acts])\n",
    "\n",
    "    #takes policy network and returns action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits = logits)\n",
    "\n",
    "    #samples actions from the action distrubution from the policy network\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "    \n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "      # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs, info = env.reset()      # first obs comes from starting distribution\n",
    "        # print(\"obs shape:\", obs.shape, \"obs type:\", type(obs))\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "            # print(\"obs shape:\", obs.shape, \"obs type:\", type(obs))\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _, _ = env.step(act)\n",
    "            \n",
    "            # obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "\n",
    "            #new reward term, gets decision term added to it, that is observation dependent \n",
    "            #pi_a has non-uniform \n",
    "            pi_a_s_t = get_policy(torch.as_tensor(obs, dtype=torch.float32)).log_prob(torch.as_tensor(act, dtype=torch.float32))\n",
    "            log_pi_a_vec  = pi_a.log()\n",
    "\n",
    "            decision = pi_a_s_t - log_pi_a_vec[act]\n",
    "            rew = rew - a * decision\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret.item())\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len     #why is this the way the setup is, this is where i want to add rewards\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, info  = env.reset()\n",
    "                done = False\n",
    "                ep_rews = []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "        \n",
    "    return logits_net\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_wrapped_rew.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: -935.887 \t return: -768.665 \t ep_len: 767.714\n",
      "epoch:   1 \t loss: -1019.749 \t return: -810.063 \t ep_len: 808.429\n",
      "epoch:   2 \t loss: -882.739 \t return: -651.123 \t ep_len: 650.222\n",
      "epoch:   3 \t loss: -942.276 \t return: -746.460 \t ep_len: 745.000\n",
      "epoch:   4 \t loss: -988.060 \t return: -872.462 \t ep_len: 870.500\n",
      "epoch:   5 \t loss: -888.391 \t return: -756.515 \t ep_len: 755.429\n",
      "epoch:   6 \t loss: -985.975 \t return: -851.324 \t ep_len: 849.429\n",
      "epoch:   7 \t loss: -985.162 \t return: -888.390 \t ep_len: 886.167\n",
      "epoch:   8 \t loss: -996.082 \t return: -850.418 \t ep_len: 848.333\n",
      "epoch:   9 \t loss: -1036.115 \t return: -887.501 \t ep_len: 885.333\n",
      "epoch:  10 \t loss: -1043.842 \t return: -944.270 \t ep_len: 942.000\n",
      "epoch:  11 \t loss: -1012.003 \t return: -870.506 \t ep_len: 868.833\n",
      "epoch:  12 \t loss: -941.549 \t return: -773.809 \t ep_len: 772.429\n",
      "epoch:  13 \t loss: -954.441 \t return: -796.200 \t ep_len: 794.714\n",
      "epoch:  14 \t loss: -899.859 \t return: -741.926 \t ep_len: 741.000\n",
      "epoch:  15 \t loss: -907.058 \t return: -724.165 \t ep_len: 723.125\n",
      "epoch:  16 \t loss: -1021.301 \t return: -860.526 \t ep_len: 858.833\n",
      "epoch:  17 \t loss: -1048.429 \t return: -943.487 \t ep_len: 941.333\n",
      "epoch:  18 \t loss: -920.910 \t return: -703.632 \t ep_len: 702.375\n",
      "epoch:  19 \t loss: -1045.257 \t return: -926.354 \t ep_len: 924.333\n",
      "epoch:  20 \t loss: -1002.594 \t return: -892.733 \t ep_len: 890.833\n",
      "epoch:  21 \t loss: -1070.825 \t return: -976.621 \t ep_len: 974.500\n",
      "epoch:  22 \t loss: -998.156 \t return: -847.720 \t ep_len: 846.333\n",
      "epoch:  23 \t loss: -974.509 \t return: -744.960 \t ep_len: 743.714\n",
      "epoch:  24 \t loss: -1081.863 \t return: -989.307 \t ep_len: 987.333\n",
      "epoch:  25 \t loss: -1027.445 \t return: -925.718 \t ep_len: 924.167\n",
      "epoch:  26 \t loss: -929.507 \t return: -700.122 \t ep_len: 699.000\n",
      "epoch:  27 \t loss: -934.711 \t return: -796.130 \t ep_len: 795.000\n",
      "epoch:  28 \t loss: -990.525 \t return: -888.460 \t ep_len: 887.000\n",
      "epoch:  29 \t loss: -1019.433 \t return: -858.925 \t ep_len: 857.167\n",
      "epoch:  30 \t loss: -1050.588 \t return: -931.572 \t ep_len: 929.333\n",
      "epoch:  31 \t loss: -996.561 \t return: -837.104 \t ep_len: 835.286\n",
      "epoch:  32 \t loss: -978.667 \t return: -837.912 \t ep_len: 836.167\n",
      "epoch:  33 \t loss: -981.819 \t return: -838.693 \t ep_len: 837.167\n",
      "epoch:  34 \t loss: -1021.034 \t return: -822.914 \t ep_len: 821.143\n",
      "epoch:  35 \t loss: -962.955 \t return: -842.831 \t ep_len: 841.167\n",
      "epoch:  36 \t loss: -925.176 \t return: -724.719 \t ep_len: 723.429\n",
      "epoch:  37 \t loss: -953.217 \t return: -787.322 \t ep_len: 785.857\n",
      "epoch:  38 \t loss: -1010.726 \t return: -881.001 \t ep_len: 879.000\n",
      "epoch:  39 \t loss: -1032.241 \t return: -919.738 \t ep_len: 918.000\n",
      "epoch:  40 \t loss: -1050.553 \t return: -902.041 \t ep_len: 899.833\n",
      "epoch:  41 \t loss: -1066.852 \t return: -965.194 \t ep_len: 962.833\n",
      "epoch:  42 \t loss: -1097.828 \t return: -1002.609 \t ep_len: 1000.000\n",
      "epoch:  43 \t loss: -975.302 \t return: -866.973 \t ep_len: 865.333\n",
      "epoch:  44 \t loss: -1045.959 \t return: -895.767 \t ep_len: 893.500\n",
      "epoch:  45 \t loss: -1056.021 \t return: -957.488 \t ep_len: 955.167\n",
      "epoch:  46 \t loss: -1082.620 \t return: -987.827 \t ep_len: 985.500\n",
      "epoch:  47 \t loss: -1063.690 \t return: -961.970 \t ep_len: 959.667\n",
      "epoch:  48 \t loss: -1053.597 \t return: -889.249 \t ep_len: 887.167\n",
      "epoch:  49 \t loss: -1047.487 \t return: -910.477 \t ep_len: 908.333\n",
      "epoch:  50 \t loss: -1000.868 \t return: -870.785 \t ep_len: 869.000\n",
      "epoch:  51 \t loss: -1074.630 \t return: -977.530 \t ep_len: 975.167\n",
      "epoch:  52 \t loss: -994.832 \t return: -852.177 \t ep_len: 850.500\n",
      "epoch:  53 \t loss: -1008.370 \t return: -849.011 \t ep_len: 847.333\n",
      "epoch:  54 \t loss: -968.596 \t return: -856.741 \t ep_len: 855.333\n",
      "epoch:  55 \t loss: -828.334 \t return: -660.916 \t ep_len: 660.222\n",
      "epoch:  56 \t loss: -881.820 \t return: -719.067 \t ep_len: 718.125\n",
      "epoch:  57 \t loss: -958.126 \t return: -806.158 \t ep_len: 804.857\n",
      "epoch:  58 \t loss: -975.091 \t return: -811.843 \t ep_len: 810.571\n",
      "epoch:  59 \t loss: -1063.018 \t return: -961.361 \t ep_len: 959.833\n",
      "epoch:  60 \t loss: -913.545 \t return: -712.867 \t ep_len: 712.000\n",
      "epoch:  61 \t loss: -1049.256 \t return: -907.528 \t ep_len: 905.833\n",
      "epoch:  62 \t loss: -867.831 \t return: -658.837 \t ep_len: 658.375\n",
      "epoch:  63 \t loss: -969.640 \t return: -753.071 \t ep_len: 752.000\n",
      "epoch:  64 \t loss: -1065.981 \t return: -966.922 \t ep_len: 965.333\n",
      "epoch:  65 \t loss: -1065.617 \t return: -866.198 \t ep_len: 864.667\n",
      "epoch:  66 \t loss: -1050.925 \t return: -938.042 \t ep_len: 936.500\n",
      "epoch:  67 \t loss: -871.612 \t return: -698.599 \t ep_len: 698.000\n",
      "epoch:  68 \t loss: -871.746 \t return: -656.041 \t ep_len: 655.500\n",
      "epoch:  69 \t loss: -935.573 \t return: -790.859 \t ep_len: 790.286\n",
      "epoch:  70 \t loss: -930.599 \t return: -720.660 \t ep_len: 720.125\n",
      "epoch:  71 \t loss: -945.261 \t return: -628.262 \t ep_len: 627.625\n",
      "epoch:  72 \t loss: -1016.513 \t return: -801.111 \t ep_len: 799.857\n",
      "epoch:  73 \t loss: -1052.361 \t return: -941.074 \t ep_len: 939.500\n",
      "epoch:  74 \t loss: -999.537 \t return: -852.948 \t ep_len: 851.857\n",
      "epoch:  75 \t loss: -1097.201 \t return: -1001.828 \t ep_len: 1000.000\n",
      "epoch:  76 \t loss: -1024.933 \t return: -848.034 \t ep_len: 846.857\n",
      "epoch:  77 \t loss: -1049.682 \t return: -907.939 \t ep_len: 906.333\n",
      "epoch:  78 \t loss: -1014.565 \t return: -910.713 \t ep_len: 909.333\n",
      "epoch:  79 \t loss: -931.906 \t return: -764.988 \t ep_len: 764.143\n",
      "epoch:  80 \t loss: -934.237 \t return: -725.972 \t ep_len: 725.143\n",
      "epoch:  81 \t loss: -904.601 \t return: -769.128 \t ep_len: 768.429\n",
      "epoch:  82 \t loss: -994.128 \t return: -849.762 \t ep_len: 848.333\n",
      "epoch:  83 \t loss: -943.864 \t return: -739.572 \t ep_len: 738.857\n",
      "epoch:  84 \t loss: -1011.533 \t return: -779.311 \t ep_len: 778.286\n",
      "epoch:  85 \t loss: -964.999 \t return: -805.053 \t ep_len: 804.000\n",
      "epoch:  86 \t loss: -828.747 \t return: -637.841 \t ep_len: 637.500\n",
      "epoch:  87 \t loss: -989.339 \t return: -821.206 \t ep_len: 820.429\n",
      "epoch:  88 \t loss: -949.900 \t return: -734.619 \t ep_len: 734.000\n",
      "epoch:  89 \t loss: -1009.648 \t return: -855.251 \t ep_len: 854.143\n",
      "epoch:  90 \t loss: -1005.008 \t return: -749.916 \t ep_len: 749.125\n",
      "epoch:  91 \t loss: -1015.426 \t return: -905.292 \t ep_len: 904.333\n",
      "epoch:  92 \t loss: -975.792 \t return: -853.129 \t ep_len: 852.333\n",
      "epoch:  93 \t loss: -946.737 \t return: -767.225 \t ep_len: 766.857\n",
      "epoch:  94 \t loss: -956.153 \t return: -845.470 \t ep_len: 844.857\n",
      "epoch:  95 \t loss: -1029.264 \t return: -939.572 \t ep_len: 938.667\n",
      "epoch:  96 \t loss: -937.047 \t return: -797.770 \t ep_len: 797.571\n",
      "epoch:  97 \t loss: -714.897 \t return: -556.203 \t ep_len: 556.600\n",
      "epoch:  98 \t loss: -873.438 \t return: -774.986 \t ep_len: 775.000\n",
      "epoch:  99 \t loss: -877.014 \t return: -648.464 \t ep_len: 648.625\n",
      "epoch: 100 \t loss: -878.189 \t return: -735.829 \t ep_len: 735.857\n",
      "epoch: 101 \t loss: -811.462 \t return: -527.969 \t ep_len: 528.300\n",
      "epoch: 102 \t loss: -847.874 \t return: -748.438 \t ep_len: 748.571\n",
      "epoch: 103 \t loss: -797.584 \t return: -597.420 \t ep_len: 597.778\n",
      "epoch: 104 \t loss: -914.717 \t return: -691.098 \t ep_len: 691.250\n",
      "epoch: 105 \t loss: -705.381 \t return: -627.812 \t ep_len: 628.500\n",
      "epoch: 106 \t loss: -722.843 \t return: -580.104 \t ep_len: 580.900\n",
      "epoch: 107 \t loss: -716.472 \t return: -574.110 \t ep_len: 574.889\n",
      "epoch: 108 \t loss: -619.840 \t return: -511.052 \t ep_len: 511.909\n",
      "epoch: 109 \t loss: -462.045 \t return: -361.385 \t ep_len: 362.357\n",
      "epoch: 110 \t loss: -597.751 \t return: -479.124 \t ep_len: 479.917\n",
      "epoch: 111 \t loss: -507.528 \t return: -498.906 \t ep_len: 499.727\n",
      "epoch: 112 \t loss: -530.106 \t return: -600.589 \t ep_len: 601.222\n",
      "epoch: 113 \t loss: -513.792 \t return: -446.969 \t ep_len: 447.615\n",
      "epoch: 114 \t loss: -441.098 \t return: -419.581 \t ep_len: 420.250\n",
      "epoch: 115 \t loss: -359.599 \t return: -426.165 \t ep_len: 426.833\n",
      "epoch: 116 \t loss: -434.995 \t return: -559.421 \t ep_len: 559.778\n",
      "epoch: 117 \t loss: -272.753 \t return: -335.511 \t ep_len: 336.235\n",
      "epoch: 118 \t loss: -293.281 \t return: -358.019 \t ep_len: 358.667\n",
      "epoch: 119 \t loss: -339.973 \t return: -479.292 \t ep_len: 479.636\n",
      "epoch: 120 \t loss: -326.684 \t return: -532.932 \t ep_len: 533.300\n",
      "epoch: 121 \t loss: -400.826 \t return: -699.881 \t ep_len: 699.750\n",
      "epoch: 122 \t loss: -277.598 \t return: -485.933 \t ep_len: 486.000\n",
      "epoch: 123 \t loss: -291.918 \t return: -726.053 \t ep_len: 725.429\n",
      "epoch: 124 \t loss: -254.610 \t return: -730.694 \t ep_len: 729.714\n",
      "epoch: 125 \t loss: -223.257 \t return: -561.795 \t ep_len: 561.500\n",
      "epoch: 126 \t loss: -255.783 \t return: -767.967 \t ep_len: 766.714\n",
      "epoch: 127 \t loss: -205.969 \t return: -789.070 \t ep_len: 787.714\n",
      "epoch: 128 \t loss: -211.023 \t return: -963.599 \t ep_len: 961.333\n",
      "epoch: 129 \t loss: -169.634 \t return: -590.077 \t ep_len: 589.556\n",
      "epoch: 130 \t loss: -159.653 \t return: -497.750 \t ep_len: 497.455\n",
      "epoch: 131 \t loss: -168.087 \t return: -532.821 \t ep_len: 532.200\n",
      "epoch: 132 \t loss: -207.955 \t return: -837.958 \t ep_len: 836.333\n",
      "epoch: 133 \t loss: -190.399 \t return: -862.390 \t ep_len: 860.500\n",
      "epoch: 134 \t loss: -181.908 \t return: -717.437 \t ep_len: 716.000\n",
      "epoch: 135 \t loss: -165.452 \t return: -600.698 \t ep_len: 600.000\n",
      "epoch: 136 \t loss: -152.666 \t return: -498.466 \t ep_len: 498.182\n",
      "epoch: 137 \t loss: -203.392 \t return: -718.723 \t ep_len: 717.500\n",
      "epoch: 138 \t loss: -220.104 \t return: -880.574 \t ep_len: 879.000\n",
      "epoch: 139 \t loss: -191.387 \t return: -628.913 \t ep_len: 628.375\n",
      "epoch: 140 \t loss: -220.248 \t return: -630.233 \t ep_len: 629.625\n",
      "epoch: 141 \t loss: -198.247 \t return: -565.701 \t ep_len: 565.111\n",
      "epoch: 142 \t loss: -232.967 \t return: -811.837 \t ep_len: 810.571\n",
      "epoch: 143 \t loss: -240.601 \t return: -738.566 \t ep_len: 737.714\n",
      "epoch: 144 \t loss: -210.317 \t return: -652.707 \t ep_len: 651.875\n",
      "epoch: 145 \t loss: -224.983 \t return: -638.207 \t ep_len: 637.625\n",
      "epoch: 146 \t loss: -188.423 \t return: -587.123 \t ep_len: 586.444\n",
      "epoch: 147 \t loss: -177.936 \t return: -582.466 \t ep_len: 581.889\n",
      "epoch: 148 \t loss: -214.864 \t return: -826.000 \t ep_len: 824.286\n",
      "epoch: 149 \t loss: -225.830 \t return: -643.237 \t ep_len: 642.556\n",
      "epoch: 150 \t loss: -197.234 \t return: -787.464 \t ep_len: 786.286\n",
      "epoch: 151 \t loss: -219.187 \t return: -645.147 \t ep_len: 644.375\n",
      "epoch: 152 \t loss: -210.231 \t return: -559.088 \t ep_len: 558.600\n",
      "epoch: 153 \t loss: -191.224 \t return: -514.451 \t ep_len: 514.200\n",
      "epoch: 154 \t loss: -196.847 \t return: -505.876 \t ep_len: 505.600\n",
      "epoch: 155 \t loss: -210.387 \t return: -718.358 \t ep_len: 717.429\n",
      "epoch: 156 \t loss: -204.455 \t return: -688.520 \t ep_len: 687.750\n",
      "epoch: 157 \t loss: -235.936 \t return: -818.082 \t ep_len: 816.857\n",
      "epoch: 158 \t loss: -197.459 \t return: -786.869 \t ep_len: 785.571\n",
      "epoch: 159 \t loss: -237.794 \t return: -765.081 \t ep_len: 763.857\n",
      "epoch: 160 \t loss: -193.601 \t return: -580.435 \t ep_len: 580.111\n",
      "epoch: 161 \t loss: -209.224 \t return: -558.016 \t ep_len: 557.900\n",
      "epoch: 162 \t loss: -254.687 \t return: -569.408 \t ep_len: 569.444\n",
      "epoch: 163 \t loss: -324.685 \t return: -624.654 \t ep_len: 624.667\n",
      "epoch: 164 \t loss: -283.372 \t return: -488.428 \t ep_len: 488.667\n",
      "epoch: 165 \t loss: -227.825 \t return: -418.199 \t ep_len: 418.500\n",
      "epoch: 166 \t loss: -271.683 \t return: -469.301 \t ep_len: 469.636\n",
      "epoch: 167 \t loss: -286.012 \t return: -432.478 \t ep_len: 432.833\n",
      "epoch: 168 \t loss: -297.474 \t return: -425.352 \t ep_len: 425.692\n",
      "epoch: 169 \t loss: -392.664 \t return: -576.192 \t ep_len: 576.444\n",
      "epoch: 170 \t loss: -443.131 \t return: -571.744 \t ep_len: 572.111\n",
      "epoch: 171 \t loss: -359.131 \t return: -360.022 \t ep_len: 360.786\n",
      "epoch: 172 \t loss: -363.714 \t return: -362.762 \t ep_len: 363.600\n",
      "epoch: 173 \t loss: -496.882 \t return: -562.447 \t ep_len: 563.222\n",
      "epoch: 174 \t loss: -547.398 \t return: -580.384 \t ep_len: 581.111\n",
      "epoch: 175 \t loss: -609.796 \t return: -678.061 \t ep_len: 678.875\n",
      "epoch: 176 \t loss: -574.128 \t return: -532.744 \t ep_len: 533.500\n",
      "epoch: 177 \t loss: -538.797 \t return: -427.477 \t ep_len: 428.308\n",
      "epoch: 178 \t loss: -657.236 \t return: -501.899 \t ep_len: 502.600\n",
      "epoch: 179 \t loss: -625.958 \t return: -555.326 \t ep_len: 556.222\n",
      "epoch: 180 \t loss: -700.762 \t return: -667.350 \t ep_len: 668.000\n",
      "epoch: 181 \t loss: -668.973 \t return: -561.178 \t ep_len: 561.900\n",
      "epoch: 182 \t loss: -583.494 \t return: -479.919 \t ep_len: 480.909\n",
      "epoch: 183 \t loss: -652.257 \t return: -540.898 \t ep_len: 541.600\n",
      "epoch: 184 \t loss: -667.323 \t return: -559.572 \t ep_len: 560.444\n",
      "epoch: 185 \t loss: -638.602 \t return: -551.114 \t ep_len: 551.900\n",
      "epoch: 186 \t loss: -613.070 \t return: -496.561 \t ep_len: 497.364\n",
      "epoch: 187 \t loss: -631.243 \t return: -516.856 \t ep_len: 517.636\n",
      "epoch: 188 \t loss: -733.829 \t return: -751.524 \t ep_len: 752.286\n",
      "epoch: 189 \t loss: -496.429 \t return: -382.464 \t ep_len: 383.333\n",
      "epoch: 190 \t loss: -884.662 \t return: -843.450 \t ep_len: 843.571\n",
      "epoch: 191 \t loss: -633.639 \t return: -501.151 \t ep_len: 501.900\n",
      "epoch: 192 \t loss: -732.434 \t return: -654.112 \t ep_len: 654.667\n",
      "epoch: 193 \t loss: -725.883 \t return: -574.884 \t ep_len: 575.556\n",
      "epoch: 194 \t loss: -854.432 \t return: -774.723 \t ep_len: 775.000\n",
      "epoch: 195 \t loss: -595.669 \t return: -390.519 \t ep_len: 391.286\n",
      "epoch: 196 \t loss: -927.724 \t return: -874.093 \t ep_len: 874.000\n",
      "epoch: 197 \t loss: -766.445 \t return: -584.038 \t ep_len: 584.400\n",
      "epoch: 198 \t loss: -785.082 \t return: -641.534 \t ep_len: 642.000\n",
      "epoch: 199 \t loss: -902.123 \t return: -733.476 \t ep_len: 733.571\n",
      "epoch: 200 \t loss: -846.735 \t return: -735.405 \t ep_len: 735.500\n",
      "epoch: 201 \t loss: -654.412 \t return: -509.315 \t ep_len: 509.800\n",
      "epoch: 202 \t loss: -855.523 \t return: -653.125 \t ep_len: 653.222\n",
      "epoch: 203 \t loss: -801.634 \t return: -633.132 \t ep_len: 633.250\n",
      "epoch: 204 \t loss: -906.410 \t return: -730.438 \t ep_len: 730.143\n",
      "epoch: 205 \t loss: -944.282 \t return: -862.147 \t ep_len: 861.833\n",
      "epoch: 206 \t loss: -885.833 \t return: -630.173 \t ep_len: 629.875\n",
      "epoch: 207 \t loss: -975.482 \t return: -834.683 \t ep_len: 833.667\n",
      "epoch: 208 \t loss: -1010.185 \t return: -900.926 \t ep_len: 899.667\n",
      "epoch: 209 \t loss: -1084.961 \t return: -996.596 \t ep_len: 994.833\n",
      "epoch: 210 \t loss: -739.251 \t return: -564.123 \t ep_len: 563.667\n",
      "epoch: 211 \t loss: -924.856 \t return: -790.765 \t ep_len: 789.857\n",
      "epoch: 212 \t loss: -985.678 \t return: -842.421 \t ep_len: 841.167\n",
      "epoch: 213 \t loss: -947.172 \t return: -774.910 \t ep_len: 773.714\n",
      "epoch: 214 \t loss: -879.762 \t return: -771.242 \t ep_len: 770.571\n",
      "epoch: 215 \t loss: -973.322 \t return: -861.956 \t ep_len: 860.833\n",
      "epoch: 216 \t loss: -870.001 \t return: -663.601 \t ep_len: 662.875\n",
      "epoch: 217 \t loss: -961.027 \t return: -814.661 \t ep_len: 813.571\n",
      "epoch: 218 \t loss: -1003.996 \t return: -853.126 \t ep_len: 851.714\n",
      "epoch: 219 \t loss: -1009.137 \t return: -848.258 \t ep_len: 846.833\n",
      "epoch: 220 \t loss: -1057.968 \t return: -960.381 \t ep_len: 958.667\n",
      "epoch: 221 \t loss: -1026.667 \t return: -867.437 \t ep_len: 866.000\n",
      "epoch: 222 \t loss: -1049.840 \t return: -940.583 \t ep_len: 938.667\n",
      "epoch: 223 \t loss: -1084.632 \t return: -994.741 \t ep_len: 992.667\n",
      "epoch: 224 \t loss: -960.024 \t return: -853.553 \t ep_len: 852.000\n",
      "epoch: 225 \t loss: -920.212 \t return: -690.335 \t ep_len: 689.250\n",
      "epoch: 226 \t loss: -863.101 \t return: -727.729 \t ep_len: 726.571\n",
      "epoch: 227 \t loss: -1016.787 \t return: -899.727 \t ep_len: 897.500\n",
      "epoch: 228 \t loss: -1092.527 \t return: -1002.836 \t ep_len: 1000.000\n",
      "epoch: 229 \t loss: -977.018 \t return: -833.921 \t ep_len: 832.000\n",
      "epoch: 230 \t loss: -989.078 \t return: -842.368 \t ep_len: 840.333\n",
      "epoch: 231 \t loss: -1044.659 \t return: -901.685 \t ep_len: 899.167\n",
      "epoch: 232 \t loss: -975.823 \t return: -858.352 \t ep_len: 856.286\n",
      "epoch: 233 \t loss: -1087.426 \t return: -1003.120 \t ep_len: 1000.000\n",
      "epoch: 234 \t loss: -998.377 \t return: -824.917 \t ep_len: 822.857\n",
      "epoch: 235 \t loss: -948.841 \t return: -774.706 \t ep_len: 773.000\n",
      "epoch: 236 \t loss: -1052.398 \t return: -877.999 \t ep_len: 875.667\n",
      "epoch: 237 \t loss: -1031.842 \t return: -852.767 \t ep_len: 850.714\n",
      "epoch: 238 \t loss: -932.591 \t return: -751.298 \t ep_len: 749.875\n",
      "epoch: 239 \t loss: -862.185 \t return: -676.329 \t ep_len: 675.500\n",
      "epoch: 240 \t loss: -1071.312 \t return: -980.040 \t ep_len: 977.667\n",
      "epoch: 241 \t loss: -819.756 \t return: -621.329 \t ep_len: 620.778\n",
      "epoch: 242 \t loss: -944.906 \t return: -728.776 \t ep_len: 727.571\n",
      "epoch: 243 \t loss: -994.240 \t return: -849.535 \t ep_len: 848.333\n",
      "epoch: 244 \t loss: -1058.008 \t return: -875.364 \t ep_len: 873.833\n",
      "epoch: 245 \t loss: -1025.537 \t return: -918.685 \t ep_len: 917.167\n",
      "epoch: 246 \t loss: -963.478 \t return: -824.197 \t ep_len: 823.143\n",
      "epoch: 247 \t loss: -844.706 \t return: -643.068 \t ep_len: 642.500\n",
      "epoch: 248 \t loss: -1043.280 \t return: -891.880 \t ep_len: 890.667\n",
      "epoch: 249 \t loss: -976.234 \t return: -843.484 \t ep_len: 842.571\n",
      "epoch: 250 \t loss: -962.416 \t return: -802.842 \t ep_len: 801.857\n",
      "epoch: 251 \t loss: -917.142 \t return: -728.197 \t ep_len: 727.714\n",
      "epoch: 252 \t loss: -925.150 \t return: -693.751 \t ep_len: 693.250\n",
      "epoch: 253 \t loss: -882.312 \t return: -662.185 \t ep_len: 661.778\n",
      "epoch: 254 \t loss: -855.613 \t return: -684.017 \t ep_len: 683.750\n",
      "epoch: 255 \t loss: -883.703 \t return: -635.970 \t ep_len: 635.625\n",
      "epoch: 256 \t loss: -1001.468 \t return: -852.985 \t ep_len: 852.000\n",
      "epoch: 257 \t loss: -886.507 \t return: -663.854 \t ep_len: 663.375\n",
      "epoch: 258 \t loss: -932.245 \t return: -779.010 \t ep_len: 778.143\n",
      "epoch: 259 \t loss: -1027.635 \t return: -850.454 \t ep_len: 849.333\n",
      "epoch: 260 \t loss: -900.096 \t return: -745.620 \t ep_len: 744.750\n",
      "epoch: 261 \t loss: -927.650 \t return: -711.111 \t ep_len: 710.125\n",
      "epoch: 262 \t loss: -799.693 \t return: -625.909 \t ep_len: 625.444\n",
      "epoch: 263 \t loss: -981.636 \t return: -773.890 \t ep_len: 772.714\n",
      "epoch: 264 \t loss: -983.289 \t return: -857.965 \t ep_len: 856.571\n",
      "epoch: 265 \t loss: -1019.884 \t return: -799.682 \t ep_len: 798.143\n",
      "epoch: 266 \t loss: -959.581 \t return: -871.147 \t ep_len: 870.167\n",
      "epoch: 267 \t loss: -1060.578 \t return: -954.968 \t ep_len: 953.000\n",
      "epoch: 268 \t loss: -971.132 \t return: -861.408 \t ep_len: 859.833\n",
      "epoch: 269 \t loss: -1007.950 \t return: -841.345 \t ep_len: 839.714\n",
      "epoch: 270 \t loss: -1074.974 \t return: -984.682 \t ep_len: 982.167\n",
      "epoch: 271 \t loss: -959.068 \t return: -732.977 \t ep_len: 731.571\n",
      "epoch: 272 \t loss: -910.520 \t return: -764.825 \t ep_len: 763.571\n",
      "epoch: 273 \t loss: -1045.107 \t return: -894.149 \t ep_len: 892.000\n",
      "epoch: 274 \t loss: -1038.556 \t return: -921.681 \t ep_len: 919.333\n",
      "epoch: 275 \t loss: -969.448 \t return: -836.046 \t ep_len: 834.167\n",
      "epoch: 276 \t loss: -1086.299 \t return: -998.129 \t ep_len: 995.500\n",
      "epoch: 277 \t loss: -934.988 \t return: -789.083 \t ep_len: 787.571\n",
      "epoch: 278 \t loss: -1045.584 \t return: -915.577 \t ep_len: 913.167\n",
      "epoch: 279 \t loss: -1014.849 \t return: -805.096 \t ep_len: 803.143\n",
      "epoch: 280 \t loss: -1090.959 \t return: -1002.933 \t ep_len: 1000.000\n",
      "epoch: 281 \t loss: -998.740 \t return: -848.447 \t ep_len: 846.143\n",
      "epoch: 282 \t loss: -941.235 \t return: -772.243 \t ep_len: 770.143\n",
      "epoch: 283 \t loss: -971.070 \t return: -839.449 \t ep_len: 836.833\n",
      "epoch: 284 \t loss: -923.190 \t return: -747.531 \t ep_len: 745.143\n",
      "epoch: 285 \t loss: -944.788 \t return: -858.538 \t ep_len: 855.667\n",
      "epoch: 286 \t loss: -1061.877 \t return: -1004.103 \t ep_len: 1000.000\n",
      "epoch: 287 \t loss: -979.237 \t return: -864.461 \t ep_len: 861.333\n",
      "epoch: 288 \t loss: -1010.298 \t return: -945.892 \t ep_len: 942.500\n",
      "epoch: 289 \t loss: -1006.979 \t return: -910.879 \t ep_len: 907.167\n",
      "epoch: 290 \t loss: -1053.577 \t return: -1000.583 \t ep_len: 996.667\n",
      "epoch: 291 \t loss: -967.839 \t return: -898.004 \t ep_len: 894.500\n",
      "epoch: 292 \t loss: -956.709 \t return: -852.524 \t ep_len: 848.857\n",
      "epoch: 293 \t loss: -1024.535 \t return: -987.833 \t ep_len: 983.500\n",
      "epoch: 294 \t loss: -1006.968 \t return: -975.479 \t ep_len: 971.167\n",
      "epoch: 295 \t loss: -992.551 \t return: -894.627 \t ep_len: 890.833\n",
      "epoch: 296 \t loss: -985.789 \t return: -900.616 \t ep_len: 896.500\n",
      "epoch: 297 \t loss: -925.568 \t return: -859.037 \t ep_len: 855.500\n",
      "epoch: 298 \t loss: -968.806 \t return: -904.414 \t ep_len: 900.167\n",
      "epoch: 299 \t loss: -957.718 \t return: -835.806 \t ep_len: 832.714\n",
      "epoch: 300 \t loss: -1031.571 \t return: -1004.591 \t ep_len: 1000.000\n",
      "epoch: 301 \t loss: -1028.047 \t return: -1004.702 \t ep_len: 1000.000\n",
      "epoch: 302 \t loss: -922.139 \t return: -878.424 \t ep_len: 874.667\n",
      "epoch: 303 \t loss: -912.178 \t return: -814.909 \t ep_len: 811.286\n",
      "epoch: 304 \t loss: -1003.913 \t return: -1003.309 \t ep_len: 998.500\n",
      "epoch: 305 \t loss: -965.562 \t return: -864.098 \t ep_len: 860.167\n",
      "epoch: 306 \t loss: -968.059 \t return: -916.819 \t ep_len: 912.500\n",
      "epoch: 307 \t loss: -978.635 \t return: -954.263 \t ep_len: 949.667\n",
      "epoch: 308 \t loss: -936.299 \t return: -855.021 \t ep_len: 851.167\n",
      "epoch: 309 \t loss: -1028.859 \t return: -1004.785 \t ep_len: 1000.000\n",
      "epoch: 310 \t loss: -1028.783 \t return: -1004.754 \t ep_len: 1000.000\n",
      "epoch: 311 \t loss: -956.774 \t return: -894.597 \t ep_len: 890.667\n",
      "epoch: 312 \t loss: -987.299 \t return: -946.427 \t ep_len: 942.000\n",
      "epoch: 313 \t loss: -968.855 \t return: -913.115 \t ep_len: 909.333\n",
      "epoch: 314 \t loss: -1010.740 \t return: -913.214 \t ep_len: 909.500\n",
      "epoch: 315 \t loss: -1011.130 \t return: -911.747 \t ep_len: 908.167\n",
      "epoch: 316 \t loss: -1020.914 \t return: -954.260 \t ep_len: 950.500\n",
      "epoch: 317 \t loss: -1030.582 \t return: -970.864 \t ep_len: 967.167\n",
      "epoch: 318 \t loss: -1027.315 \t return: -894.977 \t ep_len: 891.833\n",
      "epoch: 319 \t loss: -1004.348 \t return: -875.816 \t ep_len: 873.000\n",
      "epoch: 320 \t loss: -990.584 \t return: -843.934 \t ep_len: 841.333\n",
      "epoch: 321 \t loss: -991.481 \t return: -860.195 \t ep_len: 857.833\n",
      "epoch: 322 \t loss: -1085.165 \t return: -1001.486 \t ep_len: 998.333\n",
      "epoch: 323 \t loss: -1011.350 \t return: -856.934 \t ep_len: 854.667\n",
      "epoch: 324 \t loss: -1007.224 \t return: -849.567 \t ep_len: 847.286\n",
      "epoch: 325 \t loss: -1037.420 \t return: -888.552 \t ep_len: 885.667\n",
      "epoch: 326 \t loss: -1010.244 \t return: -800.113 \t ep_len: 797.857\n",
      "epoch: 327 \t loss: -1006.802 \t return: -897.254 \t ep_len: 894.833\n",
      "epoch: 328 \t loss: -990.647 \t return: -873.865 \t ep_len: 871.333\n",
      "epoch: 329 \t loss: -1024.900 \t return: -916.862 \t ep_len: 914.333\n",
      "epoch: 330 \t loss: -1083.069 \t return: -1003.184 \t ep_len: 1000.000\n",
      "epoch: 331 \t loss: -941.276 \t return: -786.143 \t ep_len: 784.000\n",
      "epoch: 332 \t loss: -1055.291 \t return: -976.590 \t ep_len: 973.500\n",
      "epoch: 333 \t loss: -971.261 \t return: -762.623 \t ep_len: 760.429\n",
      "epoch: 334 \t loss: -994.735 \t return: -852.215 \t ep_len: 849.833\n",
      "epoch: 335 \t loss: -1047.201 \t return: -958.415 \t ep_len: 955.500\n",
      "epoch: 336 \t loss: -981.118 \t return: -814.659 \t ep_len: 812.286\n",
      "epoch: 337 \t loss: -972.656 \t return: -848.943 \t ep_len: 846.667\n",
      "epoch: 338 \t loss: -1000.002 \t return: -806.808 \t ep_len: 804.429\n",
      "epoch: 339 \t loss: -1090.869 \t return: -1002.981 \t ep_len: 1000.000\n",
      "epoch: 340 \t loss: -972.136 \t return: -877.304 \t ep_len: 875.000\n",
      "epoch: 341 \t loss: -887.279 \t return: -761.469 \t ep_len: 759.714\n",
      "epoch: 342 \t loss: -1040.016 \t return: -931.163 \t ep_len: 928.667\n",
      "epoch: 343 \t loss: -1008.192 \t return: -867.081 \t ep_len: 865.167\n",
      "epoch: 344 \t loss: -1073.034 \t return: -978.213 \t ep_len: 975.833\n",
      "epoch: 345 \t loss: -1041.733 \t return: -927.125 \t ep_len: 925.000\n",
      "epoch: 346 \t loss: -1028.316 \t return: -869.516 \t ep_len: 867.500\n",
      "epoch: 347 \t loss: -919.255 \t return: -789.901 \t ep_len: 788.429\n",
      "epoch: 348 \t loss: -1046.293 \t return: -884.125 \t ep_len: 882.000\n",
      "epoch: 349 \t loss: -947.038 \t return: -798.597 \t ep_len: 796.714\n",
      "epoch: 350 \t loss: -1047.646 \t return: -946.054 \t ep_len: 943.667\n",
      "epoch: 351 \t loss: -1033.411 \t return: -852.435 \t ep_len: 850.667\n",
      "epoch: 352 \t loss: -1042.580 \t return: -909.540 \t ep_len: 907.667\n",
      "epoch: 353 \t loss: -1085.175 \t return: -989.955 \t ep_len: 987.833\n",
      "epoch: 354 \t loss: -910.518 \t return: -636.832 \t ep_len: 636.000\n",
      "epoch: 355 \t loss: -985.741 \t return: -844.013 \t ep_len: 842.667\n",
      "epoch: 356 \t loss: -1077.259 \t return: -982.335 \t ep_len: 980.333\n",
      "epoch: 357 \t loss: -933.084 \t return: -806.865 \t ep_len: 805.429\n",
      "epoch: 358 \t loss: -946.874 \t return: -756.587 \t ep_len: 755.143\n",
      "epoch: 359 \t loss: -988.123 \t return: -839.619 \t ep_len: 838.167\n",
      "epoch: 360 \t loss: -791.653 \t return: -626.082 \t ep_len: 625.333\n",
      "epoch: 361 \t loss: -1001.524 \t return: -855.219 \t ep_len: 853.500\n",
      "epoch: 362 \t loss: -952.502 \t return: -769.214 \t ep_len: 768.000\n",
      "epoch: 363 \t loss: -1058.604 \t return: -959.457 \t ep_len: 957.333\n",
      "epoch: 364 \t loss: -1036.047 \t return: -854.260 \t ep_len: 852.667\n",
      "epoch: 365 \t loss: -1010.830 \t return: -891.032 \t ep_len: 889.333\n",
      "epoch: 366 \t loss: -930.074 \t return: -774.832 \t ep_len: 773.714\n",
      "epoch: 367 \t loss: -990.698 \t return: -856.571 \t ep_len: 855.000\n",
      "epoch: 368 \t loss: -951.843 \t return: -846.955 \t ep_len: 845.571\n",
      "epoch: 369 \t loss: -1022.284 \t return: -907.820 \t ep_len: 906.167\n",
      "epoch: 370 \t loss: -1015.121 \t return: -886.711 \t ep_len: 885.167\n",
      "epoch: 371 \t loss: -904.062 \t return: -683.144 \t ep_len: 682.375\n",
      "epoch: 372 \t loss: -844.328 \t return: -654.025 \t ep_len: 653.250\n",
      "epoch: 373 \t loss: -905.649 \t return: -727.008 \t ep_len: 726.143\n",
      "epoch: 374 \t loss: -1051.704 \t return: -896.779 \t ep_len: 895.167\n",
      "epoch: 375 \t loss: -926.311 \t return: -790.747 \t ep_len: 789.857\n",
      "epoch: 376 \t loss: -1014.846 \t return: -898.520 \t ep_len: 897.167\n",
      "epoch: 377 \t loss: -932.725 \t return: -726.716 \t ep_len: 725.875\n",
      "epoch: 378 \t loss: -1047.253 \t return: -915.800 \t ep_len: 914.167\n",
      "epoch: 379 \t loss: -999.820 \t return: -883.402 \t ep_len: 882.333\n",
      "epoch: 380 \t loss: -1097.208 \t return: -1001.944 \t ep_len: 1000.000\n",
      "epoch: 381 \t loss: -1039.934 \t return: -903.686 \t ep_len: 902.333\n",
      "epoch: 382 \t loss: -1051.243 \t return: -895.612 \t ep_len: 894.000\n",
      "epoch: 383 \t loss: -875.831 \t return: -662.047 \t ep_len: 661.375\n",
      "epoch: 384 \t loss: -1010.391 \t return: -864.648 \t ep_len: 863.333\n",
      "epoch: 385 \t loss: -1057.038 \t return: -878.153 \t ep_len: 876.667\n",
      "epoch: 386 \t loss: -951.589 \t return: -765.155 \t ep_len: 764.143\n",
      "epoch: 387 \t loss: -901.027 \t return: -701.377 \t ep_len: 700.625\n",
      "epoch: 388 \t loss: -776.245 \t return: -562.016 \t ep_len: 561.778\n",
      "epoch: 389 \t loss: -978.386 \t return: -742.634 \t ep_len: 741.625\n",
      "epoch: 390 \t loss: -1016.711 \t return: -907.753 \t ep_len: 906.667\n",
      "epoch: 391 \t loss: -909.458 \t return: -641.107 \t ep_len: 640.556\n",
      "epoch: 392 \t loss: -957.157 \t return: -740.830 \t ep_len: 740.000\n",
      "epoch: 393 \t loss: -1046.377 \t return: -905.415 \t ep_len: 904.000\n",
      "epoch: 394 \t loss: -985.731 \t return: -882.670 \t ep_len: 881.667\n",
      "epoch: 395 \t loss: -1003.372 \t return: -845.233 \t ep_len: 844.143\n",
      "epoch: 396 \t loss: -973.092 \t return: -806.481 \t ep_len: 805.429\n",
      "epoch: 397 \t loss: -1017.824 \t return: -854.610 \t ep_len: 853.286\n",
      "epoch: 398 \t loss: -915.374 \t return: -705.380 \t ep_len: 704.625\n",
      "epoch: 399 \t loss: -863.031 \t return: -701.205 \t ep_len: 700.625\n",
      "epoch: 400 \t loss: -866.465 \t return: -705.070 \t ep_len: 704.375\n",
      "epoch: 401 \t loss: -961.557 \t return: -818.757 \t ep_len: 817.857\n",
      "epoch: 402 \t loss: -1045.359 \t return: -919.489 \t ep_len: 918.000\n",
      "epoch: 403 \t loss: -967.609 \t return: -831.194 \t ep_len: 830.143\n",
      "epoch: 404 \t loss: -1047.546 \t return: -886.136 \t ep_len: 884.833\n",
      "epoch: 405 \t loss: -940.779 \t return: -775.406 \t ep_len: 774.571\n",
      "epoch: 406 \t loss: -918.568 \t return: -740.122 \t ep_len: 739.286\n",
      "epoch: 407 \t loss: -1086.663 \t return: -996.852 \t ep_len: 995.167\n",
      "epoch: 408 \t loss: -935.319 \t return: -761.452 \t ep_len: 760.714\n",
      "epoch: 409 \t loss: -887.792 \t return: -722.352 \t ep_len: 721.750\n",
      "epoch: 410 \t loss: -850.089 \t return: -681.258 \t ep_len: 680.750\n",
      "epoch: 411 \t loss: -918.636 \t return: -645.400 \t ep_len: 644.875\n",
      "epoch: 412 \t loss: -799.087 \t return: -621.627 \t ep_len: 621.444\n",
      "epoch: 413 \t loss: -1013.615 \t return: -880.939 \t ep_len: 879.833\n",
      "epoch: 414 \t loss: -1043.623 \t return: -888.376 \t ep_len: 887.167\n",
      "epoch: 415 \t loss: -987.572 \t return: -856.865 \t ep_len: 855.857\n",
      "epoch: 416 \t loss: -932.533 \t return: -799.259 \t ep_len: 798.429\n",
      "epoch: 417 \t loss: -943.422 \t return: -719.025 \t ep_len: 718.500\n",
      "epoch: 418 \t loss: -828.462 \t return: -596.750 \t ep_len: 596.556\n",
      "epoch: 419 \t loss: -890.913 \t return: -745.321 \t ep_len: 744.857\n",
      "epoch: 420 \t loss: -1011.539 \t return: -904.970 \t ep_len: 903.833\n",
      "epoch: 421 \t loss: -1002.110 \t return: -852.809 \t ep_len: 851.714\n",
      "epoch: 422 \t loss: -960.214 \t return: -819.272 \t ep_len: 818.429\n",
      "epoch: 423 \t loss: -1047.401 \t return: -948.246 \t ep_len: 947.000\n",
      "epoch: 424 \t loss: -958.714 \t return: -765.644 \t ep_len: 764.857\n",
      "epoch: 425 \t loss: -975.154 \t return: -840.518 \t ep_len: 839.571\n",
      "epoch: 426 \t loss: -1007.492 \t return: -852.965 \t ep_len: 852.000\n",
      "epoch: 427 \t loss: -863.252 \t return: -673.513 \t ep_len: 673.000\n",
      "epoch: 428 \t loss: -1029.593 \t return: -858.281 \t ep_len: 857.143\n",
      "epoch: 429 \t loss: -1029.290 \t return: -883.338 \t ep_len: 882.333\n",
      "epoch: 430 \t loss: -901.976 \t return: -721.913 \t ep_len: 721.500\n",
      "epoch: 431 \t loss: -1003.630 \t return: -818.756 \t ep_len: 817.714\n",
      "epoch: 432 \t loss: -992.029 \t return: -791.833 \t ep_len: 791.000\n",
      "epoch: 433 \t loss: -885.002 \t return: -659.265 \t ep_len: 659.000\n",
      "epoch: 434 \t loss: -1042.818 \t return: -942.181 \t ep_len: 941.000\n",
      "epoch: 435 \t loss: -967.355 \t return: -848.929 \t ep_len: 848.167\n",
      "epoch: 436 \t loss: -994.107 \t return: -833.498 \t ep_len: 832.714\n",
      "epoch: 437 \t loss: -806.210 \t return: -651.111 \t ep_len: 650.875\n",
      "epoch: 438 \t loss: -951.909 \t return: -730.697 \t ep_len: 730.125\n",
      "epoch: 439 \t loss: -1012.240 \t return: -908.116 \t ep_len: 907.167\n",
      "epoch: 440 \t loss: -967.490 \t return: -807.459 \t ep_len: 807.000\n",
      "epoch: 441 \t loss: -973.785 \t return: -804.771 \t ep_len: 804.286\n",
      "epoch: 442 \t loss: -806.472 \t return: -571.016 \t ep_len: 571.000\n",
      "epoch: 443 \t loss: -976.003 \t return: -834.321 \t ep_len: 833.667\n",
      "epoch: 444 \t loss: -1014.034 \t return: -884.020 \t ep_len: 883.167\n",
      "epoch: 445 \t loss: -994.424 \t return: -804.395 \t ep_len: 803.857\n",
      "epoch: 446 \t loss: -875.565 \t return: -745.590 \t ep_len: 745.286\n",
      "epoch: 447 \t loss: -857.889 \t return: -748.256 \t ep_len: 748.000\n",
      "epoch: 448 \t loss: -898.285 \t return: -668.346 \t ep_len: 668.000\n",
      "epoch: 449 \t loss: -802.330 \t return: -720.101 \t ep_len: 720.000\n",
      "epoch: 450 \t loss: -892.015 \t return: -593.641 \t ep_len: 593.556\n",
      "epoch: 451 \t loss: -872.750 \t return: -733.479 \t ep_len: 733.143\n",
      "epoch: 452 \t loss: -932.389 \t return: -808.287 \t ep_len: 807.714\n",
      "epoch: 453 \t loss: -923.627 \t return: -657.165 \t ep_len: 657.000\n",
      "epoch: 454 \t loss: -950.511 \t return: -812.031 \t ep_len: 811.571\n",
      "epoch: 455 \t loss: -694.976 \t return: -513.368 \t ep_len: 513.700\n",
      "epoch: 456 \t loss: -844.950 \t return: -721.541 \t ep_len: 721.429\n",
      "epoch: 457 \t loss: -778.467 \t return: -631.658 \t ep_len: 631.875\n",
      "epoch: 458 \t loss: -853.059 \t return: -584.157 \t ep_len: 584.300\n",
      "epoch: 459 \t loss: -956.451 \t return: -765.266 \t ep_len: 765.000\n",
      "epoch: 460 \t loss: -776.403 \t return: -560.750 \t ep_len: 561.111\n",
      "epoch: 461 \t loss: -853.831 \t return: -666.391 \t ep_len: 666.625\n",
      "epoch: 462 \t loss: -925.849 \t return: -872.619 \t ep_len: 872.667\n",
      "epoch: 463 \t loss: -752.173 \t return: -546.599 \t ep_len: 547.100\n",
      "epoch: 464 \t loss: -854.420 \t return: -661.652 \t ep_len: 662.111\n",
      "epoch: 465 \t loss: -637.480 \t return: -517.359 \t ep_len: 518.200\n",
      "epoch: 466 \t loss: -752.732 \t return: -645.933 \t ep_len: 646.625\n",
      "epoch: 467 \t loss: -562.694 \t return: -469.187 \t ep_len: 470.091\n",
      "epoch: 468 \t loss: -797.363 \t return: -678.889 \t ep_len: 679.375\n",
      "epoch: 469 \t loss: -933.144 \t return: -938.802 \t ep_len: 939.000\n",
      "epoch: 470 \t loss: -716.260 \t return: -579.723 \t ep_len: 580.556\n",
      "epoch: 471 \t loss: -771.379 \t return: -657.649 \t ep_len: 658.222\n",
      "epoch: 472 \t loss: -757.281 \t return: -629.393 \t ep_len: 630.125\n",
      "epoch: 473 \t loss: -627.562 \t return: -459.110 \t ep_len: 459.909\n",
      "epoch: 474 \t loss: -670.459 \t return: -442.172 \t ep_len: 443.000\n",
      "epoch: 475 \t loss: -642.027 \t return: -522.067 \t ep_len: 522.909\n",
      "epoch: 476 \t loss: -611.683 \t return: -460.423 \t ep_len: 461.273\n",
      "epoch: 477 \t loss: -473.548 \t return: -444.889 \t ep_len: 445.833\n",
      "epoch: 478 \t loss: -374.303 \t return: -258.136 \t ep_len: 259.150\n",
      "epoch: 479 \t loss: -566.606 \t return: -499.280 \t ep_len: 500.200\n",
      "epoch: 480 \t loss: -498.553 \t return: -493.619 \t ep_len: 494.545\n",
      "epoch: 481 \t loss: -486.011 \t return: -504.528 \t ep_len: 505.500\n",
      "epoch: 482 \t loss: -513.899 \t return: -460.291 \t ep_len: 461.091\n",
      "epoch: 483 \t loss: -444.579 \t return: -415.169 \t ep_len: 416.071\n",
      "epoch: 484 \t loss: -399.281 \t return: -376.529 \t ep_len: 377.429\n",
      "epoch: 485 \t loss: -475.021 \t return: -515.811 \t ep_len: 516.500\n",
      "epoch: 486 \t loss: -443.808 \t return: -522.482 \t ep_len: 523.400\n",
      "epoch: 487 \t loss: -420.771 \t return: -363.072 \t ep_len: 364.000\n",
      "epoch: 488 \t loss: -450.330 \t return: -446.114 \t ep_len: 446.917\n",
      "epoch: 489 \t loss: -465.958 \t return: -475.627 \t ep_len: 476.333\n",
      "epoch: 490 \t loss: -540.745 \t return: -672.588 \t ep_len: 673.000\n",
      "epoch: 491 \t loss: -488.607 \t return: -564.612 \t ep_len: 565.222\n",
      "epoch: 492 \t loss: -351.986 \t return: -430.676 \t ep_len: 431.417\n",
      "epoch: 493 \t loss: -350.713 \t return: -338.743 \t ep_len: 339.625\n",
      "epoch: 494 \t loss: -401.824 \t return: -422.702 \t ep_len: 423.500\n",
      "epoch: 495 \t loss: -386.914 \t return: -470.895 \t ep_len: 471.727\n",
      "epoch: 496 \t loss: -388.812 \t return: -407.985 \t ep_len: 408.769\n",
      "epoch: 497 \t loss: -396.438 \t return: -429.135 \t ep_len: 430.000\n",
      "epoch: 498 \t loss: -516.166 \t return: -471.110 \t ep_len: 471.833\n",
      "epoch: 499 \t loss: -518.806 \t return: -540.770 \t ep_len: 541.600\n",
      "epoch: 500 \t loss: -548.262 \t return: -558.429 \t ep_len: 559.000\n",
      "epoch: 501 \t loss: -471.724 \t return: -470.701 \t ep_len: 471.545\n",
      "epoch: 502 \t loss: -484.380 \t return: -418.295 \t ep_len: 419.167\n",
      "epoch: 503 \t loss: -397.050 \t return: -427.081 \t ep_len: 428.000\n",
      "epoch: 504 \t loss: -471.982 \t return: -467.280 \t ep_len: 468.091\n",
      "epoch: 505 \t loss: -425.232 \t return: -471.426 \t ep_len: 472.167\n",
      "epoch: 506 \t loss: -355.702 \t return: -321.996 \t ep_len: 322.875\n",
      "epoch: 507 \t loss: -425.129 \t return: -416.477 \t ep_len: 417.250\n",
      "epoch: 508 \t loss: -437.879 \t return: -437.254 \t ep_len: 437.923\n",
      "epoch: 509 \t loss: -430.292 \t return: -466.950 \t ep_len: 467.727\n",
      "epoch: 510 \t loss: -310.483 \t return: -335.798 \t ep_len: 336.667\n",
      "epoch: 511 \t loss: -273.036 \t return: -264.603 \t ep_len: 265.474\n",
      "epoch: 512 \t loss: -422.358 \t return: -468.739 \t ep_len: 469.364\n",
      "epoch: 513 \t loss: -448.386 \t return: -516.458 \t ep_len: 517.200\n",
      "epoch: 514 \t loss: -378.229 \t return: -452.592 \t ep_len: 453.250\n",
      "epoch: 515 \t loss: -331.773 \t return: -363.826 \t ep_len: 364.571\n",
      "epoch: 516 \t loss: -344.764 \t return: -405.521 \t ep_len: 406.214\n",
      "epoch: 517 \t loss: -287.137 \t return: -399.926 \t ep_len: 400.692\n",
      "epoch: 518 \t loss: -296.414 \t return: -359.262 \t ep_len: 360.071\n",
      "epoch: 519 \t loss: -349.702 \t return: -420.420 \t ep_len: 421.154\n",
      "epoch: 520 \t loss: -359.228 \t return: -470.922 \t ep_len: 471.455\n",
      "epoch: 521 \t loss: -290.315 \t return: -296.214 \t ep_len: 297.111\n",
      "epoch: 522 \t loss: -425.153 \t return: -418.636 \t ep_len: 419.333\n",
      "epoch: 523 \t loss: -327.123 \t return: -358.894 \t ep_len: 359.643\n",
      "epoch: 524 \t loss: -310.581 \t return: -385.945 \t ep_len: 386.615\n",
      "epoch: 525 \t loss: -416.277 \t return: -430.789 \t ep_len: 431.333\n",
      "epoch: 526 \t loss: -321.343 \t return: -391.128 \t ep_len: 391.846\n",
      "epoch: 527 \t loss: -289.730 \t return: -300.247 \t ep_len: 301.053\n",
      "epoch: 528 \t loss: -358.662 \t return: -426.291 \t ep_len: 427.000\n",
      "epoch: 529 \t loss: -291.573 \t return: -332.728 \t ep_len: 333.467\n",
      "epoch: 530 \t loss: -352.178 \t return: -424.479 \t ep_len: 425.167\n",
      "epoch: 531 \t loss: -369.783 \t return: -339.435 \t ep_len: 340.200\n",
      "epoch: 532 \t loss: -321.289 \t return: -404.054 \t ep_len: 404.923\n",
      "epoch: 533 \t loss: -352.647 \t return: -314.472 \t ep_len: 315.250\n",
      "epoch: 534 \t loss: -355.430 \t return: -370.024 \t ep_len: 370.857\n",
      "epoch: 535 \t loss: -379.321 \t return: -442.441 \t ep_len: 443.333\n",
      "epoch: 536 \t loss: -364.620 \t return: -405.345 \t ep_len: 406.154\n",
      "epoch: 537 \t loss: -333.226 \t return: -355.503 \t ep_len: 356.400\n",
      "epoch: 538 \t loss: -391.255 \t return: -474.457 \t ep_len: 475.250\n",
      "epoch: 539 \t loss: -372.450 \t return: -365.259 \t ep_len: 366.143\n",
      "epoch: 540 \t loss: -318.848 \t return: -354.150 \t ep_len: 355.062\n",
      "epoch: 541 \t loss: -325.941 \t return: -342.956 \t ep_len: 343.867\n",
      "epoch: 542 \t loss: -307.646 \t return: -361.984 \t ep_len: 362.875\n",
      "epoch: 543 \t loss: -346.366 \t return: -418.491 \t ep_len: 419.417\n",
      "epoch: 544 \t loss: -401.174 \t return: -358.925 \t ep_len: 359.714\n",
      "epoch: 545 \t loss: -509.627 \t return: -500.656 \t ep_len: 501.300\n",
      "epoch: 546 \t loss: -375.898 \t return: -357.675 \t ep_len: 358.533\n",
      "epoch: 547 \t loss: -411.216 \t return: -384.156 \t ep_len: 385.077\n",
      "epoch: 548 \t loss: -251.965 \t return: -295.102 \t ep_len: 296.056\n",
      "epoch: 549 \t loss: -383.265 \t return: -471.682 \t ep_len: 472.500\n",
      "epoch: 550 \t loss: -317.095 \t return: -280.075 \t ep_len: 280.944\n",
      "epoch: 551 \t loss: -495.704 \t return: -446.074 \t ep_len: 446.750\n",
      "epoch: 552 \t loss: -401.840 \t return: -472.332 \t ep_len: 473.167\n",
      "epoch: 553 \t loss: -381.143 \t return: -387.048 \t ep_len: 387.846\n",
      "epoch: 554 \t loss: -322.252 \t return: -339.704 \t ep_len: 340.600\n",
      "epoch: 555 \t loss: -278.615 \t return: -356.961 \t ep_len: 357.857\n",
      "epoch: 556 \t loss: -427.917 \t return: -487.933 \t ep_len: 488.500\n",
      "epoch: 557 \t loss: -388.204 \t return: -398.748 \t ep_len: 399.538\n",
      "epoch: 558 \t loss: -312.960 \t return: -338.220 \t ep_len: 339.125\n",
      "epoch: 559 \t loss: -318.868 \t return: -362.616 \t ep_len: 363.429\n",
      "epoch: 560 \t loss: -400.484 \t return: -502.386 \t ep_len: 503.182\n",
      "epoch: 561 \t loss: -492.488 \t return: -567.276 \t ep_len: 567.889\n",
      "epoch: 562 \t loss: -312.100 \t return: -342.608 \t ep_len: 343.467\n",
      "epoch: 563 \t loss: -308.718 \t return: -389.331 \t ep_len: 390.077\n",
      "epoch: 564 \t loss: -337.817 \t return: -389.342 \t ep_len: 390.077\n",
      "epoch: 565 \t loss: -381.962 \t return: -387.254 \t ep_len: 388.000\n",
      "epoch: 566 \t loss: -346.864 \t return: -372.716 \t ep_len: 373.500\n",
      "epoch: 567 \t loss: -266.729 \t return: -292.796 \t ep_len: 293.667\n",
      "epoch: 568 \t loss: -322.928 \t return: -455.414 \t ep_len: 456.182\n",
      "epoch: 569 \t loss: -369.824 \t return: -426.177 \t ep_len: 426.857\n",
      "epoch: 570 \t loss: -368.597 \t return: -534.268 \t ep_len: 535.000\n",
      "epoch: 571 \t loss: -279.547 \t return: -399.693 \t ep_len: 400.462\n",
      "epoch: 572 \t loss: -294.785 \t return: -372.423 \t ep_len: 373.214\n",
      "epoch: 573 \t loss: -320.624 \t return: -401.914 \t ep_len: 402.571\n",
      "epoch: 574 \t loss: -367.201 \t return: -534.084 \t ep_len: 534.545\n",
      "epoch: 575 \t loss: -240.202 \t return: -345.368 \t ep_len: 346.133\n",
      "epoch: 576 \t loss: -285.086 \t return: -364.799 \t ep_len: 365.500\n",
      "epoch: 577 \t loss: -314.254 \t return: -345.264 \t ep_len: 345.933\n",
      "epoch: 578 \t loss: -321.564 \t return: -360.213 \t ep_len: 360.929\n",
      "epoch: 579 \t loss: -263.031 \t return: -364.238 \t ep_len: 365.067\n",
      "epoch: 580 \t loss: -197.666 \t return: -278.212 \t ep_len: 279.056\n",
      "epoch: 581 \t loss: -233.155 \t return: -338.138 \t ep_len: 338.933\n",
      "epoch: 582 \t loss: -374.823 \t return: -570.099 \t ep_len: 570.444\n",
      "epoch: 583 \t loss: -361.912 \t return: -500.000 \t ep_len: 500.600\n",
      "epoch: 584 \t loss: -282.444 \t return: -429.160 \t ep_len: 429.846\n",
      "epoch: 585 \t loss: -291.659 \t return: -394.046 \t ep_len: 394.714\n",
      "epoch: 586 \t loss: -276.454 \t return: -363.441 \t ep_len: 364.214\n",
      "epoch: 587 \t loss: -403.372 \t return: -587.835 \t ep_len: 588.222\n",
      "epoch: 588 \t loss: -326.243 \t return: -335.672 \t ep_len: 336.467\n",
      "epoch: 589 \t loss: -419.544 \t return: -576.616 \t ep_len: 577.000\n",
      "epoch: 590 \t loss: -274.172 \t return: -304.676 \t ep_len: 305.444\n",
      "epoch: 591 \t loss: -337.622 \t return: -416.206 \t ep_len: 416.769\n",
      "epoch: 592 \t loss: -363.686 \t return: -648.759 \t ep_len: 649.125\n",
      "epoch: 593 \t loss: -350.339 \t return: -405.794 \t ep_len: 406.385\n",
      "epoch: 594 \t loss: -363.498 \t return: -559.867 \t ep_len: 560.222\n",
      "epoch: 595 \t loss: -197.317 \t return: -250.163 \t ep_len: 251.000\n",
      "epoch: 596 \t loss: -299.842 \t return: -352.149 \t ep_len: 353.000\n",
      "epoch: 597 \t loss: -423.002 \t return: -421.636 \t ep_len: 422.333\n",
      "epoch: 598 \t loss: -435.240 \t return: -501.694 \t ep_len: 502.364\n",
      "epoch: 599 \t loss: -236.264 \t return: -276.358 \t ep_len: 277.263\n",
      "epoch: 600 \t loss: -308.422 \t return: -288.993 \t ep_len: 289.850\n",
      "epoch: 601 \t loss: -413.787 \t return: -387.043 \t ep_len: 387.769\n",
      "epoch: 602 \t loss: -284.299 \t return: -273.330 \t ep_len: 274.211\n",
      "epoch: 603 \t loss: -329.249 \t return: -359.176 \t ep_len: 360.000\n",
      "epoch: 604 \t loss: -318.202 \t return: -312.634 \t ep_len: 313.471\n",
      "epoch: 605 \t loss: -248.202 \t return: -258.629 \t ep_len: 259.450\n",
      "epoch: 606 \t loss: -377.982 \t return: -509.532 \t ep_len: 510.100\n",
      "epoch: 607 \t loss: -348.085 \t return: -361.499 \t ep_len: 362.267\n",
      "epoch: 608 \t loss: -375.342 \t return: -502.408 \t ep_len: 503.100\n",
      "epoch: 609 \t loss: -318.602 \t return: -306.180 \t ep_len: 307.000\n",
      "epoch: 610 \t loss: -458.873 \t return: -534.973 \t ep_len: 535.636\n",
      "epoch: 611 \t loss: -437.117 \t return: -551.501 \t ep_len: 552.300\n",
      "epoch: 612 \t loss: -354.440 \t return: -336.783 \t ep_len: 337.667\n",
      "epoch: 613 \t loss: -503.544 \t return: -632.444 \t ep_len: 633.375\n",
      "epoch: 614 \t loss: -407.163 \t return: -397.399 \t ep_len: 398.385\n",
      "epoch: 615 \t loss: -329.709 \t return: -364.131 \t ep_len: 365.143\n",
      "epoch: 616 \t loss: -496.830 \t return: -397.983 \t ep_len: 398.867\n",
      "epoch: 617 \t loss: -551.914 \t return: -422.519 \t ep_len: 423.500\n",
      "epoch: 618 \t loss: -539.222 \t return: -384.655 \t ep_len: 385.615\n",
      "epoch: 619 \t loss: -467.833 \t return: -349.303 \t ep_len: 350.267\n",
      "epoch: 620 \t loss: -424.899 \t return: -372.241 \t ep_len: 373.286\n",
      "epoch: 621 \t loss: -446.995 \t return: -377.752 \t ep_len: 378.733\n",
      "epoch: 622 \t loss: -507.025 \t return: -438.958 \t ep_len: 439.833\n",
      "epoch: 623 \t loss: -522.862 \t return: -505.936 \t ep_len: 507.000\n",
      "epoch: 624 \t loss: -638.564 \t return: -665.682 \t ep_len: 666.500\n",
      "epoch: 625 \t loss: -423.099 \t return: -371.235 \t ep_len: 372.214\n",
      "epoch: 626 \t loss: -580.412 \t return: -417.371 \t ep_len: 418.167\n",
      "epoch: 627 \t loss: -578.272 \t return: -486.044 \t ep_len: 487.000\n",
      "epoch: 628 \t loss: -561.858 \t return: -472.970 \t ep_len: 473.833\n",
      "epoch: 629 \t loss: -495.962 \t return: -370.286 \t ep_len: 371.267\n",
      "epoch: 630 \t loss: -572.536 \t return: -485.542 \t ep_len: 486.500\n",
      "epoch: 631 \t loss: -506.138 \t return: -385.825 \t ep_len: 386.692\n",
      "epoch: 632 \t loss: -591.589 \t return: -560.389 \t ep_len: 561.333\n",
      "epoch: 633 \t loss: -532.032 \t return: -418.180 \t ep_len: 419.143\n",
      "epoch: 634 \t loss: -689.526 \t return: -536.957 \t ep_len: 537.727\n",
      "epoch: 635 \t loss: -709.785 \t return: -644.199 \t ep_len: 644.875\n",
      "epoch: 636 \t loss: -571.127 \t return: -487.427 \t ep_len: 488.364\n",
      "epoch: 637 \t loss: -582.485 \t return: -433.731 \t ep_len: 434.583\n",
      "epoch: 638 \t loss: -621.857 \t return: -422.609 \t ep_len: 423.417\n",
      "epoch: 639 \t loss: -555.940 \t return: -435.220 \t ep_len: 436.154\n",
      "epoch: 640 \t loss: -670.990 \t return: -468.445 \t ep_len: 469.182\n",
      "epoch: 641 \t loss: -567.061 \t return: -466.332 \t ep_len: 467.273\n",
      "epoch: 642 \t loss: -648.446 \t return: -579.282 \t ep_len: 580.222\n",
      "epoch: 643 \t loss: -622.328 \t return: -438.417 \t ep_len: 439.250\n",
      "epoch: 644 \t loss: -689.715 \t return: -509.815 \t ep_len: 510.545\n",
      "epoch: 645 \t loss: -682.770 \t return: -564.363 \t ep_len: 565.111\n",
      "epoch: 646 \t loss: -597.907 \t return: -473.259 \t ep_len: 474.000\n",
      "epoch: 647 \t loss: -678.069 \t return: -500.002 \t ep_len: 500.600\n",
      "epoch: 648 \t loss: -813.573 \t return: -523.672 \t ep_len: 524.000\n",
      "epoch: 649 \t loss: -841.534 \t return: -597.258 \t ep_len: 597.400\n",
      "epoch: 650 \t loss: -778.708 \t return: -593.973 \t ep_len: 594.333\n",
      "epoch: 651 \t loss: -854.629 \t return: -637.852 \t ep_len: 637.889\n",
      "epoch: 652 \t loss: -905.945 \t return: -673.870 \t ep_len: 673.750\n",
      "epoch: 653 \t loss: -1053.202 \t return: -974.925 \t ep_len: 973.833\n",
      "epoch: 654 \t loss: -885.263 \t return: -701.472 \t ep_len: 701.125\n",
      "epoch: 655 \t loss: -819.016 \t return: -600.640 \t ep_len: 600.444\n",
      "epoch: 656 \t loss: -1031.056 \t return: -867.859 \t ep_len: 866.833\n",
      "epoch: 657 \t loss: -895.274 \t return: -653.672 \t ep_len: 653.125\n",
      "epoch: 658 \t loss: -973.646 \t return: -799.248 \t ep_len: 798.286\n",
      "epoch: 659 \t loss: -914.188 \t return: -724.368 \t ep_len: 723.571\n",
      "epoch: 660 \t loss: -922.245 \t return: -757.534 \t ep_len: 756.714\n",
      "epoch: 661 \t loss: -936.265 \t return: -764.063 \t ep_len: 762.714\n",
      "epoch: 662 \t loss: -832.953 \t return: -627.568 \t ep_len: 626.667\n",
      "epoch: 663 \t loss: -948.388 \t return: -842.065 \t ep_len: 840.500\n",
      "epoch: 664 \t loss: -976.063 \t return: -862.249 \t ep_len: 860.833\n",
      "epoch: 665 \t loss: -1005.027 \t return: -773.000 \t ep_len: 771.714\n",
      "epoch: 666 \t loss: -900.625 \t return: -734.102 \t ep_len: 733.000\n",
      "epoch: 667 \t loss: -913.075 \t return: -708.129 \t ep_len: 707.125\n",
      "epoch: 668 \t loss: -917.770 \t return: -705.676 \t ep_len: 704.625\n",
      "epoch: 669 \t loss: -924.670 \t return: -757.310 \t ep_len: 756.000\n",
      "epoch: 670 \t loss: -931.166 \t return: -713.137 \t ep_len: 711.875\n",
      "epoch: 671 \t loss: -894.739 \t return: -735.480 \t ep_len: 734.429\n",
      "epoch: 672 \t loss: -891.572 \t return: -715.719 \t ep_len: 714.625\n",
      "epoch: 673 \t loss: -969.350 \t return: -815.453 \t ep_len: 814.000\n",
      "epoch: 674 \t loss: -946.466 \t return: -732.249 \t ep_len: 731.143\n",
      "epoch: 675 \t loss: -773.345 \t return: -561.194 \t ep_len: 560.800\n",
      "epoch: 676 \t loss: -884.105 \t return: -746.452 \t ep_len: 745.429\n",
      "epoch: 677 \t loss: -827.522 \t return: -557.602 \t ep_len: 557.111\n",
      "epoch: 678 \t loss: -994.570 \t return: -847.351 \t ep_len: 845.714\n",
      "epoch: 679 \t loss: -903.096 \t return: -760.152 \t ep_len: 759.000\n",
      "epoch: 680 \t loss: -1029.173 \t return: -909.121 \t ep_len: 907.167\n",
      "epoch: 681 \t loss: -970.528 \t return: -788.822 \t ep_len: 787.286\n",
      "epoch: 682 \t loss: -931.329 \t return: -814.030 \t ep_len: 812.286\n",
      "epoch: 683 \t loss: -994.932 \t return: -889.319 \t ep_len: 887.167\n",
      "epoch: 684 \t loss: -943.734 \t return: -810.310 \t ep_len: 808.714\n",
      "epoch: 685 \t loss: -899.193 \t return: -728.048 \t ep_len: 726.500\n",
      "epoch: 686 \t loss: -947.544 \t return: -798.397 \t ep_len: 796.857\n",
      "epoch: 687 \t loss: -873.586 \t return: -737.863 \t ep_len: 736.714\n",
      "epoch: 688 \t loss: -940.062 \t return: -760.814 \t ep_len: 759.714\n",
      "epoch: 689 \t loss: -915.034 \t return: -737.663 \t ep_len: 736.857\n",
      "epoch: 690 \t loss: -940.853 \t return: -759.399 \t ep_len: 758.571\n",
      "epoch: 691 \t loss: -956.858 \t return: -775.492 \t ep_len: 774.571\n",
      "epoch: 692 \t loss: -1087.426 \t return: -1000.720 \t ep_len: 999.000\n",
      "epoch: 693 \t loss: -920.542 \t return: -667.559 \t ep_len: 666.875\n",
      "epoch: 694 \t loss: -846.141 \t return: -710.886 \t ep_len: 710.375\n",
      "epoch: 695 \t loss: -816.841 \t return: -732.792 \t ep_len: 732.429\n",
      "epoch: 696 \t loss: -844.064 \t return: -641.597 \t ep_len: 641.250\n",
      "epoch: 697 \t loss: -878.895 \t return: -723.538 \t ep_len: 723.125\n",
      "epoch: 698 \t loss: -1006.187 \t return: -914.480 \t ep_len: 913.667\n",
      "epoch: 699 \t loss: -960.329 \t return: -841.710 \t ep_len: 841.000\n",
      "epoch: 700 \t loss: -908.994 \t return: -676.676 \t ep_len: 676.250\n",
      "epoch: 701 \t loss: -875.496 \t return: -646.671 \t ep_len: 646.250\n",
      "epoch: 702 \t loss: -1047.526 \t return: -950.907 \t ep_len: 949.667\n",
      "epoch: 703 \t loss: -859.525 \t return: -606.972 \t ep_len: 606.556\n",
      "epoch: 704 \t loss: -980.053 \t return: -828.630 \t ep_len: 827.714\n",
      "epoch: 705 \t loss: -928.678 \t return: -749.467 \t ep_len: 748.857\n",
      "epoch: 706 \t loss: -858.395 \t return: -694.294 \t ep_len: 693.750\n",
      "epoch: 707 \t loss: -904.485 \t return: -732.602 \t ep_len: 731.857\n",
      "epoch: 708 \t loss: -956.449 \t return: -779.751 \t ep_len: 778.714\n",
      "epoch: 709 \t loss: -974.028 \t return: -779.078 \t ep_len: 778.143\n",
      "epoch: 710 \t loss: -943.074 \t return: -843.394 \t ep_len: 842.167\n",
      "epoch: 711 \t loss: -999.906 \t return: -836.346 \t ep_len: 834.833\n",
      "epoch: 712 \t loss: -928.178 \t return: -825.302 \t ep_len: 824.143\n",
      "epoch: 713 \t loss: -897.610 \t return: -690.666 \t ep_len: 689.750\n",
      "epoch: 714 \t loss: -971.845 \t return: -871.643 \t ep_len: 870.000\n",
      "epoch: 715 \t loss: -985.363 \t return: -790.620 \t ep_len: 789.143\n",
      "epoch: 716 \t loss: -912.105 \t return: -736.057 \t ep_len: 734.875\n",
      "epoch: 717 \t loss: -866.487 \t return: -633.351 \t ep_len: 632.625\n",
      "epoch: 718 \t loss: -854.411 \t return: -709.169 \t ep_len: 708.125\n",
      "epoch: 719 \t loss: -1062.290 \t return: -856.460 \t ep_len: 854.167\n",
      "epoch: 720 \t loss: -1089.396 \t return: -1002.681 \t ep_len: 1000.000\n",
      "epoch: 721 \t loss: -978.147 \t return: -830.553 \t ep_len: 828.857\n",
      "epoch: 722 \t loss: -864.679 \t return: -677.238 \t ep_len: 676.125\n",
      "epoch: 723 \t loss: -1013.001 \t return: -834.114 \t ep_len: 832.286\n",
      "epoch: 724 \t loss: -922.074 \t return: -724.550 \t ep_len: 723.143\n",
      "epoch: 725 \t loss: -897.713 \t return: -701.387 \t ep_len: 700.125\n",
      "epoch: 726 \t loss: -1018.765 \t return: -776.099 \t ep_len: 774.286\n",
      "epoch: 727 \t loss: -968.161 \t return: -857.997 \t ep_len: 855.857\n",
      "epoch: 728 \t loss: -917.955 \t return: -709.680 \t ep_len: 708.125\n",
      "epoch: 729 \t loss: -1027.198 \t return: -938.522 \t ep_len: 936.000\n",
      "epoch: 730 \t loss: -1011.156 \t return: -844.615 \t ep_len: 842.000\n",
      "epoch: 731 \t loss: -1084.752 \t return: -1003.036 \t ep_len: 1000.000\n",
      "epoch: 732 \t loss: -946.538 \t return: -826.941 \t ep_len: 824.714\n",
      "epoch: 733 \t loss: -1068.402 \t return: -1003.718 \t ep_len: 1000.000\n",
      "epoch: 734 \t loss: -1028.811 \t return: -965.001 \t ep_len: 961.333\n",
      "epoch: 735 \t loss: -865.635 \t return: -744.980 \t ep_len: 742.750\n",
      "epoch: 736 \t loss: -1003.928 \t return: -952.488 \t ep_len: 948.500\n",
      "epoch: 737 \t loss: -1040.347 \t return: -1004.352 \t ep_len: 1000.000\n",
      "epoch: 738 \t loss: -1020.012 \t return: -980.834 \t ep_len: 976.667\n",
      "epoch: 739 \t loss: -951.078 \t return: -835.830 \t ep_len: 832.000\n",
      "epoch: 740 \t loss: -943.222 \t return: -924.953 \t ep_len: 920.500\n",
      "epoch: 741 \t loss: -998.680 \t return: -1005.449 \t ep_len: 1000.000\n",
      "epoch: 742 \t loss: -963.072 \t return: -918.960 \t ep_len: 914.500\n",
      "epoch: 743 \t loss: -1010.444 \t return: -1005.112 \t ep_len: 1000.000\n",
      "epoch: 744 \t loss: -883.358 \t return: -748.356 \t ep_len: 745.000\n",
      "epoch: 745 \t loss: -1007.897 \t return: -1005.241 \t ep_len: 1000.000\n",
      "epoch: 746 \t loss: -921.793 \t return: -858.670 \t ep_len: 855.000\n",
      "epoch: 747 \t loss: -988.715 \t return: -929.904 \t ep_len: 925.833\n",
      "epoch: 748 \t loss: -903.588 \t return: -852.473 \t ep_len: 849.333\n",
      "epoch: 749 \t loss: -1053.533 \t return: -1000.422 \t ep_len: 996.500\n",
      "epoch: 750 \t loss: -1036.038 \t return: -974.623 \t ep_len: 971.333\n",
      "epoch: 751 \t loss: -990.025 \t return: -884.650 \t ep_len: 881.833\n",
      "epoch: 752 \t loss: -1002.063 \t return: -914.146 \t ep_len: 911.833\n",
      "epoch: 753 \t loss: -1048.705 \t return: -968.607 \t ep_len: 966.000\n",
      "epoch: 754 \t loss: -885.654 \t return: -761.337 \t ep_len: 760.000\n",
      "epoch: 755 \t loss: -836.168 \t return: -701.794 \t ep_len: 700.750\n",
      "epoch: 756 \t loss: -885.920 \t return: -717.701 \t ep_len: 716.500\n",
      "epoch: 757 \t loss: -962.317 \t return: -809.807 \t ep_len: 808.286\n",
      "epoch: 758 \t loss: -854.112 \t return: -734.408 \t ep_len: 733.429\n",
      "epoch: 759 \t loss: -866.538 \t return: -645.353 \t ep_len: 644.625\n",
      "epoch: 760 \t loss: -937.554 \t return: -756.711 \t ep_len: 755.571\n",
      "epoch: 761 \t loss: -976.391 \t return: -736.284 \t ep_len: 735.250\n",
      "epoch: 762 \t loss: -749.154 \t return: -576.697 \t ep_len: 576.333\n",
      "epoch: 763 \t loss: -822.888 \t return: -626.622 \t ep_len: 626.125\n",
      "epoch: 764 \t loss: -722.275 \t return: -560.505 \t ep_len: 560.333\n",
      "epoch: 765 \t loss: -895.621 \t return: -743.413 \t ep_len: 742.571\n",
      "epoch: 766 \t loss: -844.971 \t return: -704.734 \t ep_len: 704.375\n",
      "epoch: 767 \t loss: -1028.870 \t return: -941.640 \t ep_len: 940.500\n",
      "epoch: 768 \t loss: -791.901 \t return: -633.931 \t ep_len: 633.750\n",
      "epoch: 769 \t loss: -819.951 \t return: -635.848 \t ep_len: 635.667\n",
      "epoch: 770 \t loss: -873.155 \t return: -634.543 \t ep_len: 634.125\n",
      "epoch: 771 \t loss: -905.256 \t return: -749.174 \t ep_len: 748.714\n",
      "epoch: 772 \t loss: -892.631 \t return: -781.118 \t ep_len: 780.429\n",
      "epoch: 773 \t loss: -876.058 \t return: -679.895 \t ep_len: 679.625\n",
      "epoch: 774 \t loss: -817.253 \t return: -674.762 \t ep_len: 674.750\n",
      "epoch: 775 \t loss: -568.785 \t return: -399.314 \t ep_len: 399.846\n",
      "epoch: 776 \t loss: -910.158 \t return: -746.017 \t ep_len: 746.000\n",
      "epoch: 777 \t loss: -868.543 \t return: -638.986 \t ep_len: 638.889\n",
      "epoch: 778 \t loss: -954.496 \t return: -812.745 \t ep_len: 812.571\n",
      "epoch: 779 \t loss: -773.522 \t return: -501.553 \t ep_len: 501.800\n",
      "epoch: 780 \t loss: -788.605 \t return: -597.358 \t ep_len: 597.444\n",
      "epoch: 781 \t loss: -814.861 \t return: -650.110 \t ep_len: 650.250\n",
      "epoch: 782 \t loss: -776.255 \t return: -609.997 \t ep_len: 610.222\n",
      "epoch: 783 \t loss: -885.984 \t return: -732.374 \t ep_len: 732.429\n",
      "epoch: 784 \t loss: -837.201 \t return: -629.869 \t ep_len: 629.875\n",
      "epoch: 785 \t loss: -740.381 \t return: -506.038 \t ep_len: 506.400\n",
      "epoch: 786 \t loss: -728.260 \t return: -570.203 \t ep_len: 570.500\n",
      "epoch: 787 \t loss: -878.597 \t return: -730.111 \t ep_len: 730.000\n",
      "epoch: 788 \t loss: -790.567 \t return: -592.582 \t ep_len: 592.667\n",
      "epoch: 789 \t loss: -778.630 \t return: -516.880 \t ep_len: 517.273\n",
      "epoch: 790 \t loss: -835.878 \t return: -638.827 \t ep_len: 639.125\n",
      "epoch: 791 \t loss: -761.278 \t return: -584.385 \t ep_len: 584.667\n",
      "epoch: 792 \t loss: -919.524 \t return: -749.473 \t ep_len: 749.429\n",
      "epoch: 793 \t loss: -924.198 \t return: -775.781 \t ep_len: 775.571\n",
      "epoch: 794 \t loss: -988.937 \t return: -791.453 \t ep_len: 791.000\n",
      "epoch: 795 \t loss: -737.483 \t return: -583.693 \t ep_len: 584.111\n",
      "epoch: 796 \t loss: -924.685 \t return: -626.927 \t ep_len: 626.875\n",
      "epoch: 797 \t loss: -787.935 \t return: -665.338 \t ep_len: 665.625\n",
      "epoch: 798 \t loss: -716.528 \t return: -471.281 \t ep_len: 471.667\n",
      "epoch: 799 \t loss: -853.698 \t return: -736.774 \t ep_len: 736.857\n",
      "epoch: 800 \t loss: -902.182 \t return: -735.690 \t ep_len: 735.375\n",
      "epoch: 801 \t loss: -887.111 \t return: -750.495 \t ep_len: 750.429\n",
      "epoch: 802 \t loss: -810.561 \t return: -599.038 \t ep_len: 599.222\n",
      "epoch: 803 \t loss: -752.725 \t return: -548.259 \t ep_len: 548.600\n",
      "epoch: 804 \t loss: -825.939 \t return: -632.412 \t ep_len: 632.375\n",
      "epoch: 805 \t loss: -869.984 \t return: -654.381 \t ep_len: 654.333\n",
      "epoch: 806 \t loss: -741.948 \t return: -592.190 \t ep_len: 592.444\n",
      "epoch: 807 \t loss: -795.826 \t return: -595.220 \t ep_len: 595.444\n",
      "epoch: 808 \t loss: -839.506 \t return: -556.886 \t ep_len: 557.111\n",
      "epoch: 809 \t loss: -851.664 \t return: -581.538 \t ep_len: 581.700\n",
      "epoch: 810 \t loss: -833.082 \t return: -651.832 \t ep_len: 652.000\n",
      "epoch: 811 \t loss: -864.542 \t return: -691.506 \t ep_len: 691.625\n",
      "epoch: 812 \t loss: -657.946 \t return: -470.689 \t ep_len: 471.273\n",
      "epoch: 813 \t loss: -811.330 \t return: -673.709 \t ep_len: 673.875\n",
      "epoch: 814 \t loss: -825.503 \t return: -633.815 \t ep_len: 634.000\n",
      "epoch: 815 \t loss: -744.738 \t return: -548.971 \t ep_len: 549.400\n",
      "epoch: 816 \t loss: -862.460 \t return: -630.282 \t ep_len: 630.500\n",
      "epoch: 817 \t loss: -841.064 \t return: -692.560 \t ep_len: 692.875\n",
      "epoch: 818 \t loss: -860.248 \t return: -687.160 \t ep_len: 687.375\n",
      "epoch: 819 \t loss: -753.552 \t return: -585.457 \t ep_len: 586.000\n",
      "epoch: 820 \t loss: -619.040 \t return: -481.360 \t ep_len: 482.000\n",
      "epoch: 821 \t loss: -617.258 \t return: -442.774 \t ep_len: 443.538\n",
      "epoch: 822 \t loss: -748.815 \t return: -507.393 \t ep_len: 508.000\n",
      "epoch: 823 \t loss: -835.415 \t return: -702.423 \t ep_len: 702.750\n",
      "epoch: 824 \t loss: -783.399 \t return: -613.166 \t ep_len: 613.556\n",
      "epoch: 825 \t loss: -672.567 \t return: -453.988 \t ep_len: 454.538\n",
      "epoch: 826 \t loss: -646.794 \t return: -438.731 \t ep_len: 439.462\n",
      "epoch: 827 \t loss: -800.681 \t return: -592.023 \t ep_len: 592.333\n",
      "epoch: 828 \t loss: -615.005 \t return: -449.548 \t ep_len: 450.333\n",
      "epoch: 829 \t loss: -755.495 \t return: -509.891 \t ep_len: 510.400\n",
      "epoch: 830 \t loss: -784.744 \t return: -633.878 \t ep_len: 634.375\n",
      "epoch: 831 \t loss: -775.959 \t return: -645.801 \t ep_len: 646.375\n",
      "epoch: 832 \t loss: -789.190 \t return: -574.755 \t ep_len: 575.222\n",
      "epoch: 833 \t loss: -655.878 \t return: -419.603 \t ep_len: 420.167\n",
      "epoch: 834 \t loss: -753.589 \t return: -571.068 \t ep_len: 571.444\n",
      "epoch: 835 \t loss: -901.761 \t return: -719.893 \t ep_len: 720.000\n",
      "epoch: 836 \t loss: -702.614 \t return: -537.389 \t ep_len: 537.800\n",
      "epoch: 837 \t loss: -589.174 \t return: -503.284 \t ep_len: 504.000\n",
      "epoch: 838 \t loss: -655.818 \t return: -527.396 \t ep_len: 528.000\n",
      "epoch: 839 \t loss: -752.856 \t return: -586.873 \t ep_len: 587.400\n",
      "epoch: 840 \t loss: -680.468 \t return: -584.762 \t ep_len: 585.500\n",
      "epoch: 841 \t loss: -679.557 \t return: -508.151 \t ep_len: 508.900\n",
      "epoch: 842 \t loss: -760.113 \t return: -588.820 \t ep_len: 589.400\n",
      "epoch: 843 \t loss: -700.793 \t return: -649.029 \t ep_len: 649.750\n",
      "epoch: 844 \t loss: -753.322 \t return: -558.123 \t ep_len: 558.667\n",
      "epoch: 845 \t loss: -699.217 \t return: -506.785 \t ep_len: 507.600\n",
      "epoch: 846 \t loss: -785.972 \t return: -609.972 \t ep_len: 610.556\n",
      "epoch: 847 \t loss: -796.562 \t return: -645.782 \t ep_len: 646.375\n",
      "epoch: 848 \t loss: -536.543 \t return: -368.331 \t ep_len: 369.143\n",
      "epoch: 849 \t loss: -694.305 \t return: -523.369 \t ep_len: 524.100\n",
      "epoch: 850 \t loss: -768.420 \t return: -656.881 \t ep_len: 657.556\n",
      "epoch: 851 \t loss: -643.170 \t return: -448.506 \t ep_len: 449.308\n",
      "epoch: 852 \t loss: -763.090 \t return: -519.287 \t ep_len: 519.900\n",
      "epoch: 853 \t loss: -713.504 \t return: -530.625 \t ep_len: 531.364\n",
      "epoch: 854 \t loss: -629.230 \t return: -474.202 \t ep_len: 474.909\n",
      "epoch: 855 \t loss: -527.953 \t return: -486.997 \t ep_len: 487.818\n",
      "epoch: 856 \t loss: -776.632 \t return: -502.036 \t ep_len: 502.500\n",
      "epoch: 857 \t loss: -731.619 \t return: -544.393 \t ep_len: 544.909\n",
      "epoch: 858 \t loss: -953.190 \t return: -788.628 \t ep_len: 788.571\n",
      "epoch: 859 \t loss: -663.031 \t return: -512.093 \t ep_len: 512.818\n",
      "epoch: 860 \t loss: -670.816 \t return: -456.614 \t ep_len: 457.364\n",
      "epoch: 861 \t loss: -504.661 \t return: -391.776 \t ep_len: 392.692\n",
      "epoch: 862 \t loss: -516.956 \t return: -343.827 \t ep_len: 344.688\n",
      "epoch: 863 \t loss: -569.996 \t return: -403.462 \t ep_len: 404.385\n",
      "epoch: 864 \t loss: -587.084 \t return: -424.373 \t ep_len: 425.250\n",
      "epoch: 865 \t loss: -563.930 \t return: -357.150 \t ep_len: 358.067\n",
      "epoch: 866 \t loss: -569.022 \t return: -424.059 \t ep_len: 425.000\n",
      "epoch: 867 \t loss: -556.830 \t return: -361.560 \t ep_len: 362.500\n",
      "epoch: 868 \t loss: -647.859 \t return: -481.936 \t ep_len: 482.750\n",
      "epoch: 869 \t loss: -537.951 \t return: -349.191 \t ep_len: 350.133\n",
      "epoch: 870 \t loss: -569.957 \t return: -422.095 \t ep_len: 423.083\n",
      "epoch: 871 \t loss: -519.572 \t return: -348.486 \t ep_len: 349.467\n",
      "epoch: 872 \t loss: -393.147 \t return: -278.628 \t ep_len: 279.667\n",
      "epoch: 873 \t loss: -421.357 \t return: -313.155 \t ep_len: 314.188\n",
      "epoch: 874 \t loss: -623.564 \t return: -436.836 \t ep_len: 437.846\n",
      "epoch: 875 \t loss: -484.381 \t return: -352.628 \t ep_len: 353.667\n",
      "epoch: 876 \t loss: -503.061 \t return: -399.767 \t ep_len: 400.786\n",
      "epoch: 877 \t loss: -499.343 \t return: -326.185 \t ep_len: 327.125\n",
      "epoch: 878 \t loss: -765.122 \t return: -643.358 \t ep_len: 644.250\n",
      "epoch: 879 \t loss: -555.243 \t return: -482.993 \t ep_len: 483.909\n",
      "epoch: 880 \t loss: -666.225 \t return: -453.082 \t ep_len: 453.917\n",
      "epoch: 881 \t loss: -546.851 \t return: -314.112 \t ep_len: 315.062\n",
      "epoch: 882 \t loss: -681.672 \t return: -446.731 \t ep_len: 447.667\n",
      "epoch: 883 \t loss: -617.910 \t return: -476.458 \t ep_len: 477.455\n",
      "epoch: 884 \t loss: -590.319 \t return: -384.711 \t ep_len: 385.571\n",
      "epoch: 885 \t loss: -637.415 \t return: -393.412 \t ep_len: 394.286\n",
      "epoch: 886 \t loss: -681.920 \t return: -511.896 \t ep_len: 512.545\n",
      "epoch: 887 \t loss: -642.670 \t return: -544.137 \t ep_len: 544.727\n",
      "epoch: 888 \t loss: -602.586 \t return: -457.766 \t ep_len: 458.364\n",
      "epoch: 889 \t loss: -858.021 \t return: -768.213 \t ep_len: 768.286\n",
      "epoch: 890 \t loss: -743.722 \t return: -519.120 \t ep_len: 519.455\n",
      "epoch: 891 \t loss: -801.818 \t return: -578.381 \t ep_len: 578.333\n",
      "epoch: 892 \t loss: -745.264 \t return: -620.948 \t ep_len: 620.889\n",
      "epoch: 893 \t loss: -658.106 \t return: -492.986 \t ep_len: 493.182\n",
      "epoch: 894 \t loss: -758.567 \t return: -598.513 \t ep_len: 598.333\n",
      "epoch: 895 \t loss: -720.673 \t return: -563.630 \t ep_len: 563.444\n",
      "epoch: 896 \t loss: -775.259 \t return: -591.993 \t ep_len: 591.778\n",
      "epoch: 897 \t loss: -693.953 \t return: -583.608 \t ep_len: 583.222\n",
      "epoch: 898 \t loss: -877.629 \t return: -688.105 \t ep_len: 687.000\n",
      "epoch: 899 \t loss: -940.311 \t return: -897.027 \t ep_len: 895.167\n",
      "epoch: 900 \t loss: -954.548 \t return: -916.424 \t ep_len: 913.667\n",
      "epoch: 901 \t loss: -981.394 \t return: -969.858 \t ep_len: 966.833\n",
      "epoch: 902 \t loss: -925.057 \t return: -837.074 \t ep_len: 834.429\n",
      "epoch: 903 \t loss: -842.830 \t return: -727.175 \t ep_len: 725.429\n",
      "epoch: 904 \t loss: -856.106 \t return: -775.371 \t ep_len: 773.143\n",
      "epoch: 905 \t loss: -1001.756 \t return: -1001.107 \t ep_len: 997.833\n",
      "epoch: 906 \t loss: -911.414 \t return: -786.228 \t ep_len: 784.000\n",
      "epoch: 907 \t loss: -938.105 \t return: -917.705 \t ep_len: 915.000\n",
      "epoch: 908 \t loss: -804.218 \t return: -702.705 \t ep_len: 701.125\n",
      "epoch: 909 \t loss: -893.581 \t return: -785.286 \t ep_len: 783.286\n",
      "epoch: 910 \t loss: -763.907 \t return: -622.867 \t ep_len: 621.667\n",
      "epoch: 911 \t loss: -852.939 \t return: -725.104 \t ep_len: 723.857\n",
      "epoch: 912 \t loss: -761.511 \t return: -647.403 \t ep_len: 646.750\n",
      "epoch: 913 \t loss: -755.175 \t return: -587.397 \t ep_len: 587.100\n",
      "epoch: 914 \t loss: -630.122 \t return: -481.945 \t ep_len: 482.182\n",
      "epoch: 915 \t loss: -704.806 \t return: -569.378 \t ep_len: 569.556\n",
      "epoch: 916 \t loss: -606.679 \t return: -456.322 \t ep_len: 457.000\n",
      "epoch: 917 \t loss: -596.210 \t return: -511.991 \t ep_len: 512.600\n",
      "epoch: 918 \t loss: -518.545 \t return: -363.470 \t ep_len: 364.286\n",
      "epoch: 919 \t loss: -654.613 \t return: -464.614 \t ep_len: 465.333\n",
      "epoch: 920 \t loss: -698.966 \t return: -390.605 \t ep_len: 391.385\n",
      "epoch: 921 \t loss: -590.547 \t return: -397.172 \t ep_len: 398.077\n",
      "epoch: 922 \t loss: -686.337 \t return: -523.827 \t ep_len: 524.600\n",
      "epoch: 923 \t loss: -673.788 \t return: -525.543 \t ep_len: 526.400\n",
      "epoch: 924 \t loss: -804.471 \t return: -682.598 \t ep_len: 683.375\n",
      "epoch: 925 \t loss: -584.106 \t return: -407.379 \t ep_len: 408.385\n",
      "epoch: 926 \t loss: -791.295 \t return: -561.924 \t ep_len: 562.667\n",
      "epoch: 927 \t loss: -515.985 \t return: -342.765 \t ep_len: 343.800\n",
      "epoch: 928 \t loss: -606.380 \t return: -368.463 \t ep_len: 369.500\n",
      "epoch: 929 \t loss: -633.580 \t return: -499.774 \t ep_len: 500.818\n",
      "epoch: 930 \t loss: -762.029 \t return: -639.010 \t ep_len: 639.875\n",
      "epoch: 931 \t loss: -575.984 \t return: -423.459 \t ep_len: 424.333\n",
      "epoch: 932 \t loss: -544.915 \t return: -459.847 \t ep_len: 460.818\n",
      "epoch: 933 \t loss: -761.661 \t return: -555.873 \t ep_len: 556.667\n",
      "epoch: 934 \t loss: -655.412 \t return: -502.436 \t ep_len: 503.455\n",
      "epoch: 935 \t loss: -627.900 \t return: -423.113 \t ep_len: 424.077\n",
      "epoch: 936 \t loss: -606.608 \t return: -422.874 \t ep_len: 423.750\n",
      "epoch: 937 \t loss: -569.605 \t return: -353.893 \t ep_len: 354.733\n",
      "epoch: 938 \t loss: -627.705 \t return: -445.861 \t ep_len: 446.583\n",
      "epoch: 939 \t loss: -758.133 \t return: -630.838 \t ep_len: 631.500\n",
      "epoch: 940 \t loss: -747.762 \t return: -524.804 \t ep_len: 525.500\n",
      "epoch: 941 \t loss: -735.091 \t return: -602.307 \t ep_len: 602.889\n",
      "epoch: 942 \t loss: -644.832 \t return: -527.449 \t ep_len: 528.100\n",
      "epoch: 943 \t loss: -690.338 \t return: -601.141 \t ep_len: 601.556\n",
      "epoch: 944 \t loss: -713.679 \t return: -549.253 \t ep_len: 549.500\n",
      "epoch: 945 \t loss: -745.236 \t return: -593.996 \t ep_len: 593.889\n",
      "epoch: 946 \t loss: -691.865 \t return: -485.007 \t ep_len: 485.273\n",
      "epoch: 947 \t loss: -784.825 \t return: -623.012 \t ep_len: 622.778\n",
      "epoch: 948 \t loss: -813.001 \t return: -642.844 \t ep_len: 642.500\n",
      "epoch: 949 \t loss: -901.762 \t return: -738.324 \t ep_len: 737.250\n",
      "epoch: 950 \t loss: -784.999 \t return: -745.910 \t ep_len: 745.143\n",
      "epoch: 951 \t loss: -641.784 \t return: -560.064 \t ep_len: 559.667\n",
      "epoch: 952 \t loss: -803.147 \t return: -718.178 \t ep_len: 717.125\n",
      "epoch: 953 \t loss: -760.016 \t return: -544.048 \t ep_len: 543.455\n",
      "epoch: 954 \t loss: -828.121 \t return: -718.460 \t ep_len: 717.500\n",
      "epoch: 955 \t loss: -819.933 \t return: -740.282 \t ep_len: 739.571\n",
      "epoch: 956 \t loss: -606.945 \t return: -464.211 \t ep_len: 464.273\n",
      "epoch: 957 \t loss: -833.123 \t return: -783.836 \t ep_len: 783.000\n",
      "epoch: 958 \t loss: -745.458 \t return: -576.034 \t ep_len: 575.600\n",
      "epoch: 959 \t loss: -777.089 \t return: -650.136 \t ep_len: 649.625\n",
      "epoch: 960 \t loss: -859.081 \t return: -764.657 \t ep_len: 763.571\n",
      "epoch: 961 \t loss: -760.416 \t return: -566.988 \t ep_len: 566.700\n",
      "epoch: 962 \t loss: -766.322 \t return: -525.434 \t ep_len: 525.500\n",
      "epoch: 963 \t loss: -560.426 \t return: -443.325 \t ep_len: 443.833\n",
      "epoch: 964 \t loss: -765.300 \t return: -632.184 \t ep_len: 632.500\n",
      "epoch: 965 \t loss: -750.600 \t return: -517.238 \t ep_len: 517.600\n",
      "epoch: 966 \t loss: -742.261 \t return: -492.668 \t ep_len: 493.333\n",
      "epoch: 967 \t loss: -781.342 \t return: -530.443 \t ep_len: 531.000\n",
      "epoch: 968 \t loss: -716.789 \t return: -641.581 \t ep_len: 642.444\n",
      "epoch: 969 \t loss: -679.071 \t return: -627.454 \t ep_len: 628.444\n",
      "epoch: 970 \t loss: -733.535 \t return: -497.145 \t ep_len: 497.909\n",
      "epoch: 971 \t loss: -700.255 \t return: -572.742 \t ep_len: 573.667\n",
      "epoch: 972 \t loss: -708.254 \t return: -526.341 \t ep_len: 527.200\n",
      "epoch: 973 \t loss: -776.758 \t return: -504.751 \t ep_len: 505.636\n",
      "epoch: 974 \t loss: -654.249 \t return: -456.683 \t ep_len: 457.545\n",
      "epoch: 975 \t loss: -790.391 \t return: -527.512 \t ep_len: 528.364\n",
      "epoch: 976 \t loss: -524.466 \t return: -338.724 \t ep_len: 339.733\n",
      "epoch: 977 \t loss: -537.155 \t return: -383.891 \t ep_len: 384.923\n",
      "epoch: 978 \t loss: -861.643 \t return: -646.500 \t ep_len: 647.250\n",
      "epoch: 979 \t loss: -555.174 \t return: -389.960 \t ep_len: 390.923\n",
      "epoch: 980 \t loss: -815.964 \t return: -528.209 \t ep_len: 529.100\n",
      "epoch: 981 \t loss: -751.104 \t return: -460.725 \t ep_len: 461.667\n",
      "epoch: 982 \t loss: -592.860 \t return: -474.871 \t ep_len: 475.909\n",
      "epoch: 983 \t loss: -808.377 \t return: -560.040 \t ep_len: 560.800\n",
      "epoch: 984 \t loss: -571.461 \t return: -359.147 \t ep_len: 360.214\n",
      "epoch: 985 \t loss: -538.057 \t return: -393.619 \t ep_len: 394.769\n",
      "epoch: 986 \t loss: -532.103 \t return: -386.597 \t ep_len: 387.692\n",
      "epoch: 987 \t loss: -481.149 \t return: -371.437 \t ep_len: 372.643\n",
      "epoch: 988 \t loss: -527.654 \t return: -369.405 \t ep_len: 370.533\n",
      "epoch: 989 \t loss: -546.649 \t return: -411.740 \t ep_len: 413.000\n",
      "epoch: 990 \t loss: -495.307 \t return: -339.751 \t ep_len: 341.000\n",
      "epoch: 991 \t loss: -445.890 \t return: -299.450 \t ep_len: 300.647\n",
      "epoch: 992 \t loss: -587.522 \t return: -390.712 \t ep_len: 391.846\n",
      "epoch: 993 \t loss: -714.557 \t return: -521.938 \t ep_len: 523.300\n",
      "epoch: 994 \t loss: -577.283 \t return: -378.179 \t ep_len: 379.500\n",
      "epoch: 995 \t loss: -592.279 \t return: -356.670 \t ep_len: 357.929\n",
      "epoch: 996 \t loss: -447.852 \t return: -357.652 \t ep_len: 359.000\n",
      "epoch: 997 \t loss: -541.171 \t return: -425.104 \t ep_len: 426.500\n",
      "epoch: 998 \t loss: -482.357 \t return: -333.864 \t ep_len: 335.200\n",
      "epoch: 999 \t loss: -439.163 \t return: -298.950 \t ep_len: 300.211\n",
      "epoch: 1000 \t loss: -592.863 \t return: -368.254 \t ep_len: 369.500\n",
      "epoch: 1001 \t loss: -420.655 \t return: -321.896 \t ep_len: 323.235\n",
      "epoch: 1002 \t loss: -651.485 \t return: -475.655 \t ep_len: 477.091\n",
      "epoch: 1003 \t loss: -500.308 \t return: -444.417 \t ep_len: 445.917\n",
      "epoch: 1004 \t loss: -556.610 \t return: -561.114 \t ep_len: 562.700\n",
      "epoch: 1005 \t loss: -330.562 \t return: -259.790 \t ep_len: 261.095\n",
      "epoch: 1006 \t loss: -447.549 \t return: -313.995 \t ep_len: 315.375\n",
      "epoch: 1007 \t loss: -570.255 \t return: -427.561 \t ep_len: 428.917\n",
      "epoch: 1008 \t loss: -392.590 \t return: -312.404 \t ep_len: 313.750\n",
      "epoch: 1009 \t loss: -359.617 \t return: -315.971 \t ep_len: 317.375\n",
      "epoch: 1010 \t loss: -338.602 \t return: -277.639 \t ep_len: 278.950\n",
      "epoch: 1011 \t loss: -345.546 \t return: -262.421 \t ep_len: 263.737\n",
      "epoch: 1012 \t loss: -337.295 \t return: -279.994 \t ep_len: 281.316\n",
      "epoch: 1013 \t loss: -481.571 \t return: -338.365 \t ep_len: 339.667\n",
      "epoch: 1014 \t loss: -479.534 \t return: -319.993 \t ep_len: 321.278\n",
      "epoch: 1015 \t loss: -388.955 \t return: -356.736 \t ep_len: 358.143\n",
      "epoch: 1016 \t loss: -283.119 \t return: -251.942 \t ep_len: 253.250\n",
      "epoch: 1017 \t loss: -371.582 \t return: -357.328 \t ep_len: 358.714\n",
      "epoch: 1018 \t loss: -368.002 \t return: -290.508 \t ep_len: 291.833\n",
      "epoch: 1019 \t loss: -368.657 \t return: -277.150 \t ep_len: 278.389\n",
      "epoch: 1020 \t loss: -221.113 \t return: -194.519 \t ep_len: 195.731\n",
      "epoch: 1021 \t loss: -350.319 \t return: -312.971 \t ep_len: 314.312\n",
      "epoch: 1022 \t loss: -321.984 \t return: -278.221 \t ep_len: 279.526\n",
      "epoch: 1023 \t loss: -395.659 \t return: -259.910 \t ep_len: 261.143\n",
      "epoch: 1024 \t loss: -351.901 \t return: -277.554 \t ep_len: 278.778\n",
      "epoch: 1025 \t loss: -231.564 \t return: -207.792 \t ep_len: 209.000\n",
      "epoch: 1026 \t loss: -243.425 \t return: -282.273 \t ep_len: 283.579\n",
      "epoch: 1027 \t loss: -293.203 \t return: -237.960 \t ep_len: 239.130\n",
      "epoch: 1028 \t loss: -269.459 \t return: -263.566 \t ep_len: 264.842\n",
      "epoch: 1029 \t loss: -212.349 \t return: -226.796 \t ep_len: 228.000\n",
      "epoch: 1030 \t loss: -325.499 \t return: -312.222 \t ep_len: 313.529\n",
      "epoch: 1031 \t loss: -304.293 \t return: -249.591 \t ep_len: 250.762\n",
      "epoch: 1032 \t loss: -307.878 \t return: -250.265 \t ep_len: 251.500\n",
      "epoch: 1033 \t loss: -317.522 \t return: -296.371 \t ep_len: 297.588\n",
      "epoch: 1034 \t loss: -331.561 \t return: -338.289 \t ep_len: 339.533\n",
      "epoch: 1035 \t loss: -423.055 \t return: -457.270 \t ep_len: 458.455\n",
      "epoch: 1036 \t loss: -311.244 \t return: -297.938 \t ep_len: 299.118\n",
      "epoch: 1037 \t loss: -212.199 \t return: -265.529 \t ep_len: 266.684\n",
      "epoch: 1038 \t loss: -284.904 \t return: -288.342 \t ep_len: 289.444\n",
      "epoch: 1039 \t loss: -248.401 \t return: -254.213 \t ep_len: 255.350\n",
      "epoch: 1040 \t loss: -209.818 \t return: -260.022 \t ep_len: 261.150\n",
      "epoch: 1041 \t loss: -209.101 \t return: -264.896 \t ep_len: 266.000\n",
      "epoch: 1042 \t loss: -237.186 \t return: -318.443 \t ep_len: 319.500\n",
      "epoch: 1043 \t loss: -217.499 \t return: -243.400 \t ep_len: 244.429\n",
      "epoch: 1044 \t loss: -225.839 \t return: -322.994 \t ep_len: 324.059\n",
      "epoch: 1045 \t loss: -303.985 \t return: -327.375 \t ep_len: 328.412\n",
      "epoch: 1046 \t loss: -256.466 \t return: -293.399 \t ep_len: 294.389\n",
      "epoch: 1047 \t loss: -215.341 \t return: -308.404 \t ep_len: 309.471\n",
      "epoch: 1048 \t loss: -217.390 \t return: -301.240 \t ep_len: 302.294\n",
      "epoch: 1049 \t loss: -145.538 \t return: -205.933 \t ep_len: 206.960\n",
      "epoch: 1050 \t loss: -267.897 \t return: -318.630 \t ep_len: 319.688\n",
      "epoch: 1051 \t loss: -230.314 \t return: -356.425 \t ep_len: 357.429\n",
      "epoch: 1052 \t loss: -200.879 \t return: -267.745 \t ep_len: 268.789\n",
      "epoch: 1053 \t loss: -217.965 \t return: -279.003 \t ep_len: 280.000\n",
      "epoch: 1054 \t loss: -257.337 \t return: -293.793 \t ep_len: 294.882\n",
      "epoch: 1055 \t loss: -213.680 \t return: -284.908 \t ep_len: 285.944\n",
      "epoch: 1056 \t loss: -168.481 \t return: -224.461 \t ep_len: 225.522\n",
      "epoch: 1057 \t loss: -253.743 \t return: -299.801 \t ep_len: 300.824\n",
      "epoch: 1058 \t loss: -204.830 \t return: -227.645 \t ep_len: 228.727\n",
      "epoch: 1059 \t loss: -287.118 \t return: -296.026 \t ep_len: 297.000\n",
      "epoch: 1060 \t loss: -194.016 \t return: -231.289 \t ep_len: 232.318\n",
      "epoch: 1061 \t loss: -230.405 \t return: -250.370 \t ep_len: 251.350\n",
      "epoch: 1062 \t loss: -243.573 \t return: -294.399 \t ep_len: 295.529\n",
      "epoch: 1063 \t loss: -226.008 \t return: -219.693 \t ep_len: 220.696\n",
      "epoch: 1064 \t loss: -360.414 \t return: -393.960 \t ep_len: 394.857\n",
      "epoch: 1065 \t loss: -204.114 \t return: -219.371 \t ep_len: 220.480\n",
      "epoch: 1066 \t loss: -226.196 \t return: -254.651 \t ep_len: 255.850\n",
      "epoch: 1067 \t loss: -242.010 \t return: -278.583 \t ep_len: 279.789\n",
      "epoch: 1068 \t loss: -238.957 \t return: -244.299 \t ep_len: 245.476\n",
      "epoch: 1069 \t loss: -244.781 \t return: -222.154 \t ep_len: 223.304\n",
      "epoch: 1070 \t loss: -248.471 \t return: -238.386 \t ep_len: 239.520\n",
      "epoch: 1071 \t loss: -268.469 \t return: -257.969 \t ep_len: 259.100\n",
      "epoch: 1072 \t loss: -222.321 \t return: -251.805 \t ep_len: 253.050\n",
      "epoch: 1073 \t loss: -271.279 \t return: -253.996 \t ep_len: 255.238\n",
      "epoch: 1074 \t loss: -256.447 \t return: -296.039 \t ep_len: 297.278\n",
      "epoch: 1075 \t loss: -181.220 \t return: -202.823 \t ep_len: 204.000\n",
      "epoch: 1076 \t loss: -246.729 \t return: -207.247 \t ep_len: 208.423\n",
      "epoch: 1077 \t loss: -175.584 \t return: -181.428 \t ep_len: 182.586\n",
      "epoch: 1078 \t loss: -162.356 \t return: -196.853 \t ep_len: 198.038\n",
      "epoch: 1079 \t loss: -263.457 \t return: -246.087 \t ep_len: 247.286\n",
      "epoch: 1080 \t loss: -338.002 \t return: -282.256 \t ep_len: 283.429\n",
      "epoch: 1081 \t loss: -172.338 \t return: -201.843 \t ep_len: 203.000\n",
      "epoch: 1082 \t loss: -337.094 \t return: -315.999 \t ep_len: 317.250\n",
      "epoch: 1083 \t loss: -326.415 \t return: -309.637 \t ep_len: 310.941\n",
      "epoch: 1084 \t loss: -153.934 \t return: -171.380 \t ep_len: 172.517\n",
      "epoch: 1085 \t loss: -367.858 \t return: -272.656 \t ep_len: 273.842\n",
      "epoch: 1086 \t loss: -245.005 \t return: -277.113 \t ep_len: 278.333\n",
      "epoch: 1087 \t loss: -218.197 \t return: -249.871 \t ep_len: 251.091\n",
      "epoch: 1088 \t loss: -249.607 \t return: -251.716 \t ep_len: 252.900\n",
      "epoch: 1089 \t loss: -253.899 \t return: -292.430 \t ep_len: 293.667\n",
      "epoch: 1090 \t loss: -304.049 \t return: -259.552 \t ep_len: 260.700\n",
      "epoch: 1091 \t loss: -248.003 \t return: -258.002 \t ep_len: 259.238\n",
      "epoch: 1092 \t loss: -319.768 \t return: -293.438 \t ep_len: 294.588\n",
      "epoch: 1093 \t loss: -263.572 \t return: -239.619 \t ep_len: 240.818\n",
      "epoch: 1094 \t loss: -186.426 \t return: -186.073 \t ep_len: 187.222\n",
      "epoch: 1095 \t loss: -192.440 \t return: -208.857 \t ep_len: 210.040\n",
      "epoch: 1096 \t loss: -288.749 \t return: -333.567 \t ep_len: 334.867\n",
      "epoch: 1097 \t loss: -214.745 \t return: -252.722 \t ep_len: 253.950\n",
      "epoch: 1098 \t loss: -198.538 \t return: -216.154 \t ep_len: 217.320\n",
      "epoch: 1099 \t loss: -306.595 \t return: -249.608 \t ep_len: 250.800\n",
      "epoch: 1100 \t loss: -286.958 \t return: -293.208 \t ep_len: 294.471\n",
      "epoch: 1101 \t loss: -223.333 \t return: -236.973 \t ep_len: 238.190\n",
      "epoch: 1102 \t loss: -196.618 \t return: -201.466 \t ep_len: 202.654\n",
      "epoch: 1103 \t loss: -231.691 \t return: -201.997 \t ep_len: 203.200\n",
      "epoch: 1104 \t loss: -211.485 \t return: -219.128 \t ep_len: 220.348\n",
      "epoch: 1105 \t loss: -233.667 \t return: -250.749 \t ep_len: 252.000\n",
      "epoch: 1106 \t loss: -228.403 \t return: -238.615 \t ep_len: 239.864\n",
      "epoch: 1107 \t loss: -212.304 \t return: -201.445 \t ep_len: 202.680\n",
      "epoch: 1108 \t loss: -246.775 \t return: -239.525 \t ep_len: 240.739\n",
      "epoch: 1109 \t loss: -236.577 \t return: -217.585 \t ep_len: 218.792\n",
      "epoch: 1110 \t loss: -355.602 \t return: -267.895 \t ep_len: 269.100\n",
      "epoch: 1111 \t loss: -303.584 \t return: -292.769 \t ep_len: 294.111\n",
      "epoch: 1112 \t loss: -233.763 \t return: -218.447 \t ep_len: 219.652\n",
      "epoch: 1113 \t loss: -292.089 \t return: -237.297 \t ep_len: 238.524\n",
      "epoch: 1114 \t loss: -217.090 \t return: -225.841 \t ep_len: 227.043\n",
      "epoch: 1115 \t loss: -303.503 \t return: -311.236 \t ep_len: 312.529\n",
      "epoch: 1116 \t loss: -278.723 \t return: -249.503 \t ep_len: 250.762\n",
      "epoch: 1117 \t loss: -190.179 \t return: -185.366 \t ep_len: 186.556\n",
      "epoch: 1118 \t loss: -390.429 \t return: -318.938 \t ep_len: 320.125\n",
      "epoch: 1119 \t loss: -258.545 \t return: -266.498 \t ep_len: 267.762\n",
      "epoch: 1120 \t loss: -294.321 \t return: -253.805 \t ep_len: 255.000\n",
      "epoch: 1121 \t loss: -223.476 \t return: -251.427 \t ep_len: 252.650\n",
      "epoch: 1122 \t loss: -382.995 \t return: -361.286 \t ep_len: 362.533\n",
      "epoch: 1123 \t loss: -208.594 \t return: -238.714 \t ep_len: 239.952\n",
      "epoch: 1124 \t loss: -220.535 \t return: -232.689 \t ep_len: 233.909\n",
      "epoch: 1125 \t loss: -312.412 \t return: -261.867 \t ep_len: 263.150\n",
      "epoch: 1126 \t loss: -272.995 \t return: -286.944 \t ep_len: 288.263\n",
      "epoch: 1127 \t loss: -302.733 \t return: -317.698 \t ep_len: 319.000\n",
      "epoch: 1128 \t loss: -205.605 \t return: -226.814 \t ep_len: 228.045\n",
      "epoch: 1129 \t loss: -174.959 \t return: -175.871 \t ep_len: 177.067\n",
      "epoch: 1130 \t loss: -263.220 \t return: -244.836 \t ep_len: 246.095\n",
      "epoch: 1131 \t loss: -257.756 \t return: -273.664 \t ep_len: 274.950\n",
      "epoch: 1132 \t loss: -257.540 \t return: -242.850 \t ep_len: 244.143\n",
      "epoch: 1133 \t loss: -262.638 \t return: -209.552 \t ep_len: 210.750\n",
      "epoch: 1134 \t loss: -263.939 \t return: -265.214 \t ep_len: 266.500\n",
      "epoch: 1135 \t loss: -220.401 \t return: -233.681 \t ep_len: 234.955\n",
      "epoch: 1136 \t loss: -260.489 \t return: -222.513 \t ep_len: 223.783\n",
      "epoch: 1137 \t loss: -281.753 \t return: -266.600 \t ep_len: 267.895\n",
      "epoch: 1138 \t loss: -333.818 \t return: -296.046 \t ep_len: 297.278\n",
      "epoch: 1139 \t loss: -203.307 \t return: -224.053 \t ep_len: 225.304\n",
      "epoch: 1140 \t loss: -174.135 \t return: -197.324 \t ep_len: 198.538\n",
      "epoch: 1141 \t loss: -212.389 \t return: -237.333 \t ep_len: 238.591\n",
      "epoch: 1142 \t loss: -199.115 \t return: -219.058 \t ep_len: 220.292\n",
      "epoch: 1143 \t loss: -219.476 \t return: -218.175 \t ep_len: 219.391\n",
      "epoch: 1144 \t loss: -168.537 \t return: -173.182 \t ep_len: 174.367\n",
      "epoch: 1145 \t loss: -284.155 \t return: -250.453 \t ep_len: 251.650\n",
      "epoch: 1146 \t loss: -351.250 \t return: -322.479 \t ep_len: 323.750\n",
      "epoch: 1147 \t loss: -295.160 \t return: -293.426 \t ep_len: 294.647\n",
      "epoch: 1148 \t loss: -321.543 \t return: -280.907 \t ep_len: 282.111\n",
      "epoch: 1149 \t loss: -212.386 \t return: -259.526 \t ep_len: 260.750\n",
      "epoch: 1150 \t loss: -285.229 \t return: -273.797 \t ep_len: 275.000\n",
      "epoch: 1151 \t loss: -248.238 \t return: -286.531 \t ep_len: 287.722\n",
      "epoch: 1152 \t loss: -286.271 \t return: -263.955 \t ep_len: 265.150\n",
      "epoch: 1153 \t loss: -221.794 \t return: -255.391 \t ep_len: 256.550\n",
      "epoch: 1154 \t loss: -218.420 \t return: -227.545 \t ep_len: 228.727\n",
      "epoch: 1155 \t loss: -210.360 \t return: -203.518 \t ep_len: 204.680\n",
      "epoch: 1156 \t loss: -262.017 \t return: -257.240 \t ep_len: 258.450\n",
      "epoch: 1157 \t loss: -194.949 \t return: -253.733 \t ep_len: 254.900\n",
      "epoch: 1158 \t loss: -210.009 \t return: -227.586 \t ep_len: 228.739\n",
      "epoch: 1159 \t loss: -351.552 \t return: -354.515 \t ep_len: 355.733\n",
      "epoch: 1160 \t loss: -306.438 \t return: -271.031 \t ep_len: 272.263\n",
      "epoch: 1161 \t loss: -267.756 \t return: -268.769 \t ep_len: 269.895\n",
      "epoch: 1162 \t loss: -278.292 \t return: -298.972 \t ep_len: 300.211\n",
      "epoch: 1163 \t loss: -286.252 \t return: -280.651 \t ep_len: 281.900\n",
      "epoch: 1164 \t loss: -286.293 \t return: -281.495 \t ep_len: 282.667\n",
      "epoch: 1165 \t loss: -132.876 \t return: -162.951 \t ep_len: 164.065\n",
      "epoch: 1166 \t loss: -159.617 \t return: -217.377 \t ep_len: 218.542\n",
      "epoch: 1167 \t loss: -235.059 \t return: -244.602 \t ep_len: 245.762\n",
      "epoch: 1168 \t loss: -333.737 \t return: -330.994 \t ep_len: 332.111\n",
      "epoch: 1169 \t loss: -234.964 \t return: -240.735 \t ep_len: 241.905\n",
      "epoch: 1170 \t loss: -284.718 \t return: -298.730 \t ep_len: 299.941\n",
      "epoch: 1171 \t loss: -240.083 \t return: -314.622 \t ep_len: 315.824\n",
      "epoch: 1172 \t loss: -271.521 \t return: -280.065 \t ep_len: 281.278\n",
      "epoch: 1173 \t loss: -308.449 \t return: -277.620 \t ep_len: 278.857\n",
      "epoch: 1174 \t loss: -172.984 \t return: -254.601 \t ep_len: 255.762\n",
      "epoch: 1175 \t loss: -175.943 \t return: -189.930 \t ep_len: 191.037\n",
      "epoch: 1176 \t loss: -206.384 \t return: -242.088 \t ep_len: 243.238\n",
      "epoch: 1177 \t loss: -254.354 \t return: -282.029 \t ep_len: 283.222\n",
      "epoch: 1178 \t loss: -226.529 \t return: -272.990 \t ep_len: 274.150\n",
      "epoch: 1179 \t loss: -233.454 \t return: -266.166 \t ep_len: 267.316\n",
      "epoch: 1180 \t loss: -270.038 \t return: -299.961 \t ep_len: 301.167\n",
      "epoch: 1181 \t loss: -217.158 \t return: -295.908 \t ep_len: 297.111\n",
      "epoch: 1182 \t loss: -287.374 \t return: -268.058 \t ep_len: 269.263\n",
      "epoch: 1183 \t loss: -294.124 \t return: -238.730 \t ep_len: 239.870\n",
      "epoch: 1184 \t loss: -246.908 \t return: -288.553 \t ep_len: 289.778\n",
      "epoch: 1185 \t loss: -203.517 \t return: -240.132 \t ep_len: 241.381\n",
      "epoch: 1186 \t loss: -196.183 \t return: -235.800 \t ep_len: 237.043\n",
      "epoch: 1187 \t loss: -253.453 \t return: -238.233 \t ep_len: 239.476\n",
      "epoch: 1188 \t loss: -193.365 \t return: -184.621 \t ep_len: 185.815\n",
      "epoch: 1189 \t loss: -241.926 \t return: -222.632 \t ep_len: 223.870\n",
      "epoch: 1190 \t loss: -228.011 \t return: -245.589 \t ep_len: 246.857\n",
      "epoch: 1191 \t loss: -200.134 \t return: -203.363 \t ep_len: 204.600\n",
      "epoch: 1192 \t loss: -277.770 \t return: -252.492 \t ep_len: 253.750\n",
      "epoch: 1193 \t loss: -212.734 \t return: -202.243 \t ep_len: 203.480\n",
      "epoch: 1194 \t loss: -365.871 \t return: -278.507 \t ep_len: 279.833\n",
      "epoch: 1195 \t loss: -183.099 \t return: -193.909 \t ep_len: 195.115\n",
      "epoch: 1196 \t loss: -292.096 \t return: -253.258 \t ep_len: 254.550\n",
      "epoch: 1197 \t loss: -233.527 \t return: -263.846 \t ep_len: 265.105\n",
      "epoch: 1198 \t loss: -213.769 \t return: -237.756 \t ep_len: 239.043\n",
      "epoch: 1199 \t loss: -181.165 \t return: -181.025 \t ep_len: 182.214\n",
      "epoch: 1200 \t loss: -291.063 \t return: -267.133 \t ep_len: 268.421\n",
      "epoch: 1201 \t loss: -217.353 \t return: -242.625 \t ep_len: 243.857\n",
      "epoch: 1202 \t loss: -247.958 \t return: -217.600 \t ep_len: 218.826\n",
      "epoch: 1203 \t loss: -314.004 \t return: -300.409 \t ep_len: 301.765\n",
      "epoch: 1204 \t loss: -302.859 \t return: -262.870 \t ep_len: 264.158\n",
      "epoch: 1205 \t loss: -213.570 \t return: -237.036 \t ep_len: 238.286\n",
      "epoch: 1206 \t loss: -194.759 \t return: -228.458 \t ep_len: 229.739\n",
      "epoch: 1207 \t loss: -356.548 \t return: -286.221 \t ep_len: 287.444\n",
      "epoch: 1208 \t loss: -259.712 \t return: -213.716 \t ep_len: 214.885\n",
      "epoch: 1209 \t loss: -281.701 \t return: -298.087 \t ep_len: 299.412\n",
      "epoch: 1210 \t loss: -220.840 \t return: -226.705 \t ep_len: 227.955\n",
      "epoch: 1211 \t loss: -188.739 \t return: -177.687 \t ep_len: 178.833\n",
      "epoch: 1212 \t loss: -150.874 \t return: -192.477 \t ep_len: 193.654\n",
      "epoch: 1213 \t loss: -225.239 \t return: -227.627 \t ep_len: 228.818\n",
      "epoch: 1214 \t loss: -248.306 \t return: -254.558 \t ep_len: 255.800\n",
      "epoch: 1215 \t loss: -319.865 \t return: -276.088 \t ep_len: 277.316\n",
      "epoch: 1216 \t loss: -216.993 \t return: -211.101 \t ep_len: 212.292\n",
      "epoch: 1217 \t loss: -194.893 \t return: -201.244 \t ep_len: 202.440\n",
      "epoch: 1218 \t loss: -232.308 \t return: -226.676 \t ep_len: 227.909\n",
      "epoch: 1219 \t loss: -242.876 \t return: -220.374 \t ep_len: 221.583\n",
      "epoch: 1220 \t loss: -193.094 \t return: -226.204 \t ep_len: 227.409\n",
      "epoch: 1221 \t loss: -237.645 \t return: -252.440 \t ep_len: 253.700\n",
      "epoch: 1222 \t loss: -249.808 \t return: -237.659 \t ep_len: 238.864\n",
      "epoch: 1223 \t loss: -189.998 \t return: -241.175 \t ep_len: 242.381\n",
      "epoch: 1224 \t loss: -214.900 \t return: -214.702 \t ep_len: 215.917\n",
      "epoch: 1225 \t loss: -203.286 \t return: -245.643 \t ep_len: 246.857\n",
      "epoch: 1226 \t loss: -255.091 \t return: -197.401 \t ep_len: 198.571\n",
      "epoch: 1227 \t loss: -292.263 \t return: -314.017 \t ep_len: 315.353\n",
      "epoch: 1228 \t loss: -162.918 \t return: -175.811 \t ep_len: 176.967\n",
      "epoch: 1229 \t loss: -315.256 \t return: -301.874 \t ep_len: 303.167\n",
      "epoch: 1230 \t loss: -264.100 \t return: -191.211 \t ep_len: 192.385\n",
      "epoch: 1231 \t loss: -229.739 \t return: -211.126 \t ep_len: 212.346\n",
      "epoch: 1232 \t loss: -231.424 \t return: -275.568 \t ep_len: 276.842\n",
      "epoch: 1233 \t loss: -188.093 \t return: -167.598 \t ep_len: 168.742\n",
      "epoch: 1234 \t loss: -327.273 \t return: -229.221 \t ep_len: 230.435\n",
      "epoch: 1235 \t loss: -200.683 \t return: -218.376 \t ep_len: 219.565\n",
      "epoch: 1236 \t loss: -278.130 \t return: -275.193 \t ep_len: 276.474\n",
      "epoch: 1237 \t loss: -337.822 \t return: -371.480 \t ep_len: 372.857\n",
      "epoch: 1238 \t loss: -170.007 \t return: -192.347 \t ep_len: 193.519\n",
      "epoch: 1239 \t loss: -265.365 \t return: -243.430 \t ep_len: 244.571\n",
      "epoch: 1240 \t loss: -245.019 \t return: -218.167 \t ep_len: 219.304\n",
      "epoch: 1241 \t loss: -230.244 \t return: -195.268 \t ep_len: 196.423\n",
      "epoch: 1242 \t loss: -254.074 \t return: -269.775 \t ep_len: 271.000\n",
      "epoch: 1243 \t loss: -215.550 \t return: -209.824 \t ep_len: 210.960\n",
      "epoch: 1244 \t loss: -196.737 \t return: -201.201 \t ep_len: 202.360\n",
      "epoch: 1245 \t loss: -164.789 \t return: -199.290 \t ep_len: 200.440\n",
      "epoch: 1246 \t loss: -183.873 \t return: -218.885 \t ep_len: 220.043\n",
      "epoch: 1247 \t loss: -150.308 \t return: -179.783 \t ep_len: 180.893\n",
      "epoch: 1248 \t loss: -282.579 \t return: -319.572 \t ep_len: 320.706\n",
      "epoch: 1249 \t loss: -144.644 \t return: -209.427 \t ep_len: 210.500\n",
      "epoch: 1250 \t loss: -274.984 \t return: -383.846 \t ep_len: 384.929\n",
      "epoch: 1251 \t loss: -190.853 \t return: -216.726 \t ep_len: 217.792\n",
      "epoch: 1252 \t loss: -300.399 \t return: -429.562 \t ep_len: 430.500\n",
      "epoch: 1253 \t loss: -213.200 \t return: -239.484 \t ep_len: 240.524\n",
      "epoch: 1254 \t loss: -179.515 \t return: -274.590 \t ep_len: 275.632\n",
      "epoch: 1255 \t loss: -151.980 \t return: -200.696 \t ep_len: 201.760\n",
      "epoch: 1256 \t loss: -162.818 \t return: -217.776 \t ep_len: 218.783\n",
      "epoch: 1257 \t loss: -167.111 \t return: -212.128 \t ep_len: 213.167\n",
      "epoch: 1258 \t loss: -279.810 \t return: -423.418 \t ep_len: 424.417\n",
      "epoch: 1259 \t loss: -185.252 \t return: -223.308 \t ep_len: 224.292\n",
      "epoch: 1260 \t loss: -290.789 \t return: -392.827 \t ep_len: 393.643\n",
      "epoch: 1261 \t loss: -223.033 \t return: -322.695 \t ep_len: 323.562\n",
      "epoch: 1262 \t loss: -234.064 \t return: -359.771 \t ep_len: 360.643\n",
      "epoch: 1263 \t loss: -197.621 \t return: -297.085 \t ep_len: 298.059\n",
      "epoch: 1264 \t loss: -226.347 \t return: -301.303 \t ep_len: 302.235\n",
      "epoch: 1265 \t loss: -162.913 \t return: -238.894 \t ep_len: 239.955\n",
      "epoch: 1266 \t loss: -205.082 \t return: -267.138 \t ep_len: 268.211\n",
      "epoch: 1267 \t loss: -215.993 \t return: -224.674 \t ep_len: 225.708\n",
      "epoch: 1268 \t loss: -225.083 \t return: -270.724 \t ep_len: 271.789\n",
      "epoch: 1269 \t loss: -196.427 \t return: -242.089 \t ep_len: 243.190\n",
      "epoch: 1270 \t loss: -149.191 \t return: -186.017 \t ep_len: 187.111\n",
      "epoch: 1271 \t loss: -168.193 \t return: -200.598 \t ep_len: 201.680\n",
      "epoch: 1272 \t loss: -202.709 \t return: -242.419 \t ep_len: 243.571\n",
      "epoch: 1273 \t loss: -154.247 \t return: -193.325 \t ep_len: 194.423\n",
      "epoch: 1274 \t loss: -228.407 \t return: -211.840 \t ep_len: 212.958\n",
      "epoch: 1275 \t loss: -158.003 \t return: -226.502 \t ep_len: 227.636\n",
      "epoch: 1276 \t loss: -217.543 \t return: -265.562 \t ep_len: 266.737\n",
      "epoch: 1277 \t loss: -220.844 \t return: -273.207 \t ep_len: 274.421\n",
      "epoch: 1278 \t loss: -215.130 \t return: -222.685 \t ep_len: 223.826\n",
      "epoch: 1279 \t loss: -171.613 \t return: -202.957 \t ep_len: 204.120\n",
      "epoch: 1280 \t loss: -226.210 \t return: -227.680 \t ep_len: 228.909\n",
      "epoch: 1281 \t loss: -199.613 \t return: -235.185 \t ep_len: 236.409\n",
      "epoch: 1282 \t loss: -283.586 \t return: -277.498 \t ep_len: 278.778\n",
      "epoch: 1283 \t loss: -251.472 \t return: -269.549 \t ep_len: 270.800\n",
      "epoch: 1284 \t loss: -212.061 \t return: -215.493 \t ep_len: 216.667\n",
      "epoch: 1285 \t loss: -291.998 \t return: -237.519 \t ep_len: 238.667\n",
      "epoch: 1286 \t loss: -240.427 \t return: -250.412 \t ep_len: 251.650\n",
      "epoch: 1287 \t loss: -212.367 \t return: -223.792 \t ep_len: 224.957\n",
      "epoch: 1288 \t loss: -239.108 \t return: -238.972 \t ep_len: 240.143\n",
      "epoch: 1289 \t loss: -209.796 \t return: -202.593 \t ep_len: 203.760\n",
      "epoch: 1290 \t loss: -181.756 \t return: -229.779 \t ep_len: 230.957\n",
      "epoch: 1291 \t loss: -268.982 \t return: -256.983 \t ep_len: 258.200\n",
      "epoch: 1292 \t loss: -249.810 \t return: -242.383 \t ep_len: 243.619\n",
      "epoch: 1293 \t loss: -238.487 \t return: -244.808 \t ep_len: 246.000\n",
      "epoch: 1294 \t loss: -394.930 \t return: -342.802 \t ep_len: 344.000\n",
      "epoch: 1295 \t loss: -283.353 \t return: -279.659 \t ep_len: 280.889\n",
      "epoch: 1296 \t loss: -205.024 \t return: -232.477 \t ep_len: 233.682\n",
      "epoch: 1297 \t loss: -196.095 \t return: -217.735 \t ep_len: 218.913\n",
      "epoch: 1298 \t loss: -308.280 \t return: -277.614 \t ep_len: 278.789\n",
      "epoch: 1299 \t loss: -273.731 \t return: -279.280 \t ep_len: 280.500\n",
      "epoch: 1300 \t loss: -178.847 \t return: -191.682 \t ep_len: 192.808\n",
      "epoch: 1301 \t loss: -231.556 \t return: -207.388 \t ep_len: 208.571\n",
      "epoch: 1302 \t loss: -207.286 \t return: -241.342 \t ep_len: 242.571\n",
      "epoch: 1303 \t loss: -217.069 \t return: -204.692 \t ep_len: 205.880\n",
      "epoch: 1304 \t loss: -212.986 \t return: -207.927 \t ep_len: 209.125\n",
      "epoch: 1305 \t loss: -236.710 \t return: -207.856 \t ep_len: 209.042\n",
      "epoch: 1306 \t loss: -152.233 \t return: -193.031 \t ep_len: 194.231\n",
      "epoch: 1307 \t loss: -263.327 \t return: -218.905 \t ep_len: 220.174\n",
      "epoch: 1308 \t loss: -164.283 \t return: -166.066 \t ep_len: 167.226\n",
      "epoch: 1309 \t loss: -241.388 \t return: -249.120 \t ep_len: 250.350\n",
      "epoch: 1310 \t loss: -191.645 \t return: -217.170 \t ep_len: 218.391\n",
      "epoch: 1311 \t loss: -243.497 \t return: -212.335 \t ep_len: 213.500\n",
      "epoch: 1312 \t loss: -176.371 \t return: -172.935 \t ep_len: 174.103\n",
      "epoch: 1313 \t loss: -146.154 \t return: -143.424 \t ep_len: 144.571\n",
      "epoch: 1314 \t loss: -187.211 \t return: -188.512 \t ep_len: 189.714\n",
      "epoch: 1315 \t loss: -252.755 \t return: -227.208 \t ep_len: 228.455\n",
      "epoch: 1316 \t loss: -151.652 \t return: -151.225 \t ep_len: 152.394\n",
      "epoch: 1317 \t loss: -243.901 \t return: -257.120 \t ep_len: 258.450\n",
      "epoch: 1318 \t loss: -171.079 \t return: -207.252 \t ep_len: 208.458\n",
      "epoch: 1319 \t loss: -152.383 \t return: -160.517 \t ep_len: 161.677\n",
      "epoch: 1320 \t loss: -193.556 \t return: -208.679 \t ep_len: 209.917\n",
      "epoch: 1321 \t loss: -199.954 \t return: -196.271 \t ep_len: 197.481\n",
      "epoch: 1322 \t loss: -188.332 \t return: -198.144 \t ep_len: 199.346\n",
      "epoch: 1323 \t loss: -236.034 \t return: -173.453 \t ep_len: 174.586\n",
      "epoch: 1324 \t loss: -177.894 \t return: -160.344 \t ep_len: 161.516\n",
      "epoch: 1325 \t loss: -190.447 \t return: -184.422 \t ep_len: 185.630\n",
      "epoch: 1326 \t loss: -241.060 \t return: -234.842 \t ep_len: 236.125\n",
      "epoch: 1327 \t loss: -252.214 \t return: -193.850 \t ep_len: 195.074\n",
      "epoch: 1328 \t loss: -206.726 \t return: -212.436 \t ep_len: 213.667\n",
      "epoch: 1329 \t loss: -217.793 \t return: -203.281 \t ep_len: 204.480\n",
      "epoch: 1330 \t loss: -251.931 \t return: -230.467 \t ep_len: 231.727\n",
      "epoch: 1331 \t loss: -282.372 \t return: -268.973 \t ep_len: 270.316\n",
      "epoch: 1332 \t loss: -170.873 \t return: -151.893 \t ep_len: 153.059\n",
      "epoch: 1333 \t loss: -215.633 \t return: -220.585 \t ep_len: 221.840\n",
      "epoch: 1334 \t loss: -173.218 \t return: -189.450 \t ep_len: 190.679\n",
      "epoch: 1335 \t loss: -271.032 \t return: -229.691 \t ep_len: 230.955\n",
      "epoch: 1336 \t loss: -228.798 \t return: -208.502 \t ep_len: 209.750\n",
      "epoch: 1337 \t loss: -231.366 \t return: -198.185 \t ep_len: 199.423\n",
      "epoch: 1338 \t loss: -213.222 \t return: -237.290 \t ep_len: 238.571\n",
      "epoch: 1339 \t loss: -267.988 \t return: -227.977 \t ep_len: 229.250\n",
      "epoch: 1340 \t loss: -169.833 \t return: -184.019 \t ep_len: 185.222\n",
      "epoch: 1341 \t loss: -287.353 \t return: -288.735 \t ep_len: 290.053\n",
      "epoch: 1342 \t loss: -168.586 \t return: -210.343 \t ep_len: 211.583\n",
      "epoch: 1343 \t loss: -177.788 \t return: -186.587 \t ep_len: 187.815\n",
      "epoch: 1344 \t loss: -219.438 \t return: -185.410 \t ep_len: 186.630\n",
      "epoch: 1345 \t loss: -250.810 \t return: -231.962 \t ep_len: 233.227\n",
      "epoch: 1346 \t loss: -209.146 \t return: -237.568 \t ep_len: 238.810\n",
      "epoch: 1347 \t loss: -236.063 \t return: -261.411 \t ep_len: 262.650\n",
      "epoch: 1348 \t loss: -172.380 \t return: -209.833 \t ep_len: 211.040\n",
      "epoch: 1349 \t loss: -202.501 \t return: -201.493 \t ep_len: 202.680\n",
      "epoch: 1350 \t loss: -276.928 \t return: -211.663 \t ep_len: 212.840\n",
      "epoch: 1351 \t loss: -221.845 \t return: -218.682 \t ep_len: 219.913\n",
      "epoch: 1352 \t loss: -203.124 \t return: -208.668 \t ep_len: 209.875\n",
      "epoch: 1353 \t loss: -198.319 \t return: -205.103 \t ep_len: 206.308\n",
      "epoch: 1354 \t loss: -185.189 \t return: -200.137 \t ep_len: 201.360\n",
      "epoch: 1355 \t loss: -299.792 \t return: -248.754 \t ep_len: 249.955\n",
      "epoch: 1356 \t loss: -198.401 \t return: -190.445 \t ep_len: 191.667\n",
      "epoch: 1357 \t loss: -276.463 \t return: -237.786 \t ep_len: 239.000\n",
      "epoch: 1358 \t loss: -160.251 \t return: -186.187 \t ep_len: 187.370\n",
      "epoch: 1359 \t loss: -236.494 \t return: -285.741 \t ep_len: 287.000\n",
      "epoch: 1360 \t loss: -175.718 \t return: -217.680 \t ep_len: 218.875\n",
      "epoch: 1361 \t loss: -168.631 \t return: -192.825 \t ep_len: 194.038\n",
      "epoch: 1362 \t loss: -187.973 \t return: -223.760 \t ep_len: 224.957\n",
      "epoch: 1363 \t loss: -237.394 \t return: -224.563 \t ep_len: 225.739\n",
      "epoch: 1364 \t loss: -178.945 \t return: -222.869 \t ep_len: 224.043\n",
      "epoch: 1365 \t loss: -168.343 \t return: -199.157 \t ep_len: 200.360\n",
      "epoch: 1366 \t loss: -256.503 \t return: -271.088 \t ep_len: 272.316\n",
      "epoch: 1367 \t loss: -196.727 \t return: -221.679 \t ep_len: 222.913\n",
      "epoch: 1368 \t loss: -217.112 \t return: -204.956 \t ep_len: 206.115\n",
      "epoch: 1369 \t loss: -146.103 \t return: -191.843 \t ep_len: 193.000\n",
      "epoch: 1370 \t loss: -219.694 \t return: -227.898 \t ep_len: 229.045\n",
      "epoch: 1371 \t loss: -291.172 \t return: -281.884 \t ep_len: 283.111\n",
      "epoch: 1372 \t loss: -161.535 \t return: -246.161 \t ep_len: 247.286\n",
      "epoch: 1373 \t loss: -168.361 \t return: -237.642 \t ep_len: 238.818\n",
      "epoch: 1374 \t loss: -213.620 \t return: -259.946 \t ep_len: 261.100\n",
      "epoch: 1375 \t loss: -216.474 \t return: -243.064 \t ep_len: 244.238\n",
      "epoch: 1376 \t loss: -219.544 \t return: -227.311 \t ep_len: 228.500\n",
      "epoch: 1377 \t loss: -229.171 \t return: -295.047 \t ep_len: 296.235\n",
      "epoch: 1378 \t loss: -206.751 \t return: -244.907 \t ep_len: 246.095\n",
      "epoch: 1379 \t loss: -210.788 \t return: -252.609 \t ep_len: 253.850\n",
      "epoch: 1380 \t loss: -191.556 \t return: -254.417 \t ep_len: 255.650\n",
      "epoch: 1381 \t loss: -167.837 \t return: -179.238 \t ep_len: 180.393\n",
      "epoch: 1382 \t loss: -216.413 \t return: -265.546 \t ep_len: 266.737\n",
      "epoch: 1383 \t loss: -154.188 \t return: -200.844 \t ep_len: 202.000\n",
      "epoch: 1384 \t loss: -122.285 \t return: -152.075 \t ep_len: 153.235\n",
      "epoch: 1385 \t loss: -220.437 \t return: -264.235 \t ep_len: 265.526\n",
      "epoch: 1386 \t loss: -244.700 \t return: -218.360 \t ep_len: 219.560\n",
      "epoch: 1387 \t loss: -178.808 \t return: -219.870 \t ep_len: 221.130\n",
      "epoch: 1388 \t loss: -118.674 \t return: -142.766 \t ep_len: 143.892\n",
      "epoch: 1389 \t loss: -182.449 \t return: -196.676 \t ep_len: 197.923\n",
      "epoch: 1390 \t loss: -163.893 \t return: -207.572 \t ep_len: 208.792\n",
      "epoch: 1391 \t loss: -165.362 \t return: -184.473 \t ep_len: 185.714\n",
      "epoch: 1392 \t loss: -117.022 \t return: -151.215 \t ep_len: 152.394\n",
      "epoch: 1393 \t loss: -186.641 \t return: -180.024 \t ep_len: 181.258\n",
      "epoch: 1394 \t loss: -114.422 \t return: -147.728 \t ep_len: 148.882\n",
      "epoch: 1395 \t loss: -169.107 \t return: -185.111 \t ep_len: 186.333\n",
      "epoch: 1396 \t loss: -178.146 \t return: -175.477 \t ep_len: 176.724\n",
      "epoch: 1397 \t loss: -171.141 \t return: -228.547 \t ep_len: 229.864\n",
      "epoch: 1398 \t loss: -188.993 \t return: -207.934 \t ep_len: 209.208\n",
      "epoch: 1399 \t loss: -132.909 \t return: -173.634 \t ep_len: 174.828\n",
      "epoch: 1400 \t loss: -201.399 \t return: -167.621 \t ep_len: 168.800\n",
      "epoch: 1401 \t loss: -252.035 \t return: -228.079 \t ep_len: 229.348\n",
      "epoch: 1402 \t loss: -182.195 \t return: -190.025 \t ep_len: 191.259\n",
      "epoch: 1403 \t loss: -239.142 \t return: -241.456 \t ep_len: 242.762\n",
      "epoch: 1404 \t loss: -220.635 \t return: -225.446 \t ep_len: 226.739\n",
      "epoch: 1405 \t loss: -222.085 \t return: -226.595 \t ep_len: 227.864\n",
      "epoch: 1406 \t loss: -185.209 \t return: -227.477 \t ep_len: 228.783\n",
      "epoch: 1407 \t loss: -244.461 \t return: -224.466 \t ep_len: 225.708\n",
      "epoch: 1408 \t loss: -101.660 \t return: -140.491 \t ep_len: 141.649\n",
      "epoch: 1409 \t loss: -270.666 \t return: -263.299 \t ep_len: 264.632\n",
      "epoch: 1410 \t loss: -206.913 \t return: -211.687 \t ep_len: 212.958\n",
      "epoch: 1411 \t loss: -173.962 \t return: -213.814 \t ep_len: 215.083\n",
      "epoch: 1412 \t loss: -193.305 \t return: -195.893 \t ep_len: 197.107\n",
      "epoch: 1413 \t loss: -118.700 \t return: -157.959 \t ep_len: 159.156\n",
      "epoch: 1414 \t loss: -181.633 \t return: -178.533 \t ep_len: 179.786\n",
      "epoch: 1415 \t loss: -186.092 \t return: -207.206 \t ep_len: 208.500\n",
      "epoch: 1416 \t loss: -195.139 \t return: -254.979 \t ep_len: 256.300\n",
      "epoch: 1417 \t loss: -146.736 \t return: -180.226 \t ep_len: 181.467\n",
      "epoch: 1418 \t loss: -168.215 \t return: -229.859 \t ep_len: 231.130\n",
      "epoch: 1419 \t loss: -148.299 \t return: -146.388 \t ep_len: 147.588\n",
      "epoch: 1420 \t loss: -178.643 \t return: -177.817 \t ep_len: 179.000\n",
      "epoch: 1421 \t loss: -141.370 \t return: -153.225 \t ep_len: 154.424\n",
      "epoch: 1422 \t loss: -158.971 \t return: -151.479 \t ep_len: 152.667\n",
      "epoch: 1423 \t loss: -195.090 \t return: -200.484 \t ep_len: 201.760\n",
      "epoch: 1424 \t loss: -130.510 \t return: -166.453 \t ep_len: 167.667\n",
      "epoch: 1425 \t loss: -148.861 \t return: -196.235 \t ep_len: 197.500\n",
      "epoch: 1426 \t loss: -161.502 \t return: -164.853 \t ep_len: 166.062\n",
      "epoch: 1427 \t loss: -258.507 \t return: -262.232 \t ep_len: 263.579\n",
      "epoch: 1428 \t loss: -224.574 \t return: -240.653 \t ep_len: 242.000\n",
      "epoch: 1429 \t loss: -149.961 \t return: -171.367 \t ep_len: 172.621\n",
      "epoch: 1430 \t loss: -154.683 \t return: -182.695 \t ep_len: 183.964\n",
      "epoch: 1431 \t loss: -183.673 \t return: -216.850 \t ep_len: 218.130\n",
      "epoch: 1432 \t loss: -159.583 \t return: -183.403 \t ep_len: 184.643\n",
      "epoch: 1433 \t loss: -138.661 \t return: -192.151 \t ep_len: 193.423\n",
      "epoch: 1434 \t loss: -190.486 \t return: -207.100 \t ep_len: 208.417\n",
      "epoch: 1435 \t loss: -121.834 \t return: -162.249 \t ep_len: 163.452\n",
      "epoch: 1436 \t loss: -175.862 \t return: -207.259 \t ep_len: 208.560\n",
      "epoch: 1437 \t loss: -147.635 \t return: -152.278 \t ep_len: 153.485\n",
      "epoch: 1438 \t loss: -134.377 \t return: -180.812 \t ep_len: 182.036\n",
      "epoch: 1439 \t loss: -118.506 \t return: -160.509 \t ep_len: 161.719\n",
      "epoch: 1440 \t loss: -143.902 \t return: -152.538 \t ep_len: 153.765\n",
      "epoch: 1441 \t loss: -123.341 \t return: -180.352 \t ep_len: 181.607\n",
      "epoch: 1442 \t loss: -136.570 \t return: -185.793 \t ep_len: 187.069\n",
      "epoch: 1443 \t loss: -226.808 \t return: -218.602 \t ep_len: 219.870\n",
      "epoch: 1444 \t loss: -165.152 \t return: -209.223 \t ep_len: 210.500\n",
      "epoch: 1445 \t loss: -174.640 \t return: -201.016 \t ep_len: 202.280\n",
      "epoch: 1446 \t loss: -139.562 \t return: -180.073 \t ep_len: 181.286\n",
      "epoch: 1447 \t loss: -115.707 \t return: -147.527 \t ep_len: 148.750\n",
      "epoch: 1448 \t loss: -150.317 \t return: -175.608 \t ep_len: 176.833\n",
      "epoch: 1449 \t loss: -155.285 \t return: -205.180 \t ep_len: 206.423\n",
      "epoch: 1450 \t loss: -135.065 \t return: -165.747 \t ep_len: 167.000\n",
      "epoch: 1451 \t loss: -168.873 \t return: -228.805 \t ep_len: 230.136\n",
      "epoch: 1452 \t loss: -114.478 \t return: -144.057 \t ep_len: 145.286\n",
      "epoch: 1453 \t loss: -116.933 \t return: -144.991 \t ep_len: 146.189\n",
      "epoch: 1454 \t loss: -160.498 \t return: -217.450 \t ep_len: 218.783\n",
      "epoch: 1455 \t loss: -121.977 \t return: -159.334 \t ep_len: 160.562\n",
      "epoch: 1456 \t loss: -150.739 \t return: -166.301 \t ep_len: 167.500\n",
      "epoch: 1457 \t loss: -146.353 \t return: -187.577 \t ep_len: 188.815\n",
      "epoch: 1458 \t loss: -146.977 \t return: -186.453 \t ep_len: 187.690\n",
      "epoch: 1459 \t loss: -198.240 \t return: -226.574 \t ep_len: 227.909\n",
      "epoch: 1460 \t loss: -104.140 \t return: -141.773 \t ep_len: 142.943\n",
      "epoch: 1461 \t loss: -131.534 \t return: -204.732 \t ep_len: 205.960\n",
      "epoch: 1462 \t loss: -103.138 \t return: -177.713 \t ep_len: 178.862\n",
      "epoch: 1463 \t loss: -137.264 \t return: -194.125 \t ep_len: 195.357\n",
      "epoch: 1464 \t loss: -185.083 \t return: -200.827 \t ep_len: 201.960\n",
      "epoch: 1465 \t loss: -237.940 \t return: -342.986 \t ep_len: 344.250\n",
      "epoch: 1466 \t loss: -182.444 \t return: -226.643 \t ep_len: 227.864\n",
      "epoch: 1467 \t loss: -129.016 \t return: -186.848 \t ep_len: 188.071\n",
      "epoch: 1468 \t loss: -142.754 \t return: -194.184 \t ep_len: 195.423\n",
      "epoch: 1469 \t loss: -217.642 \t return: -305.486 \t ep_len: 306.706\n",
      "epoch: 1470 \t loss: -229.258 \t return: -226.552 \t ep_len: 227.783\n",
      "epoch: 1471 \t loss: -136.393 \t return: -204.108 \t ep_len: 205.385\n",
      "epoch: 1472 \t loss: -120.886 \t return: -175.778 \t ep_len: 177.000\n",
      "epoch: 1473 \t loss: -130.591 \t return: -203.782 \t ep_len: 204.960\n",
      "epoch: 1474 \t loss: -177.064 \t return: -282.890 \t ep_len: 284.222\n",
      "epoch: 1475 \t loss: -148.432 \t return: -217.425 \t ep_len: 218.609\n",
      "epoch: 1476 \t loss: -132.522 \t return: -216.968 \t ep_len: 218.130\n",
      "epoch: 1477 \t loss: -148.720 \t return: -189.813 \t ep_len: 191.074\n",
      "epoch: 1478 \t loss: -166.612 \t return: -211.143 \t ep_len: 212.417\n",
      "epoch: 1479 \t loss: -231.468 \t return: -226.923 \t ep_len: 228.273\n",
      "epoch: 1480 \t loss: -136.062 \t return: -171.074 \t ep_len: 172.290\n",
      "epoch: 1481 \t loss: -126.089 \t return: -165.726 \t ep_len: 166.935\n",
      "epoch: 1482 \t loss: -160.345 \t return: -191.786 \t ep_len: 193.000\n",
      "epoch: 1483 \t loss: -134.965 \t return: -202.987 \t ep_len: 204.320\n",
      "epoch: 1484 \t loss: -108.733 \t return: -178.074 \t ep_len: 179.379\n",
      "epoch: 1485 \t loss: -106.336 \t return: -167.393 \t ep_len: 168.600\n",
      "epoch: 1486 \t loss: -154.781 \t return: -207.943 \t ep_len: 209.167\n",
      "epoch: 1487 \t loss: -133.582 \t return: -178.161 \t ep_len: 179.448\n",
      "epoch: 1488 \t loss: -120.599 \t return: -165.597 \t ep_len: 166.871\n",
      "epoch: 1489 \t loss: -154.376 \t return: -208.991 \t ep_len: 210.333\n",
      "epoch: 1490 \t loss: -111.626 \t return: -168.173 \t ep_len: 169.400\n",
      "epoch: 1491 \t loss: -130.704 \t return: -160.359 \t ep_len: 161.645\n",
      "epoch: 1492 \t loss: -90.060 \t return: -127.650 \t ep_len: 128.846\n",
      "epoch: 1493 \t loss: -119.336 \t return: -144.064 \t ep_len: 145.314\n",
      "epoch: 1494 \t loss: -115.232 \t return: -161.130 \t ep_len: 162.387\n",
      "epoch: 1495 \t loss: -105.407 \t return: -136.095 \t ep_len: 137.316\n",
      "epoch: 1496 \t loss: -136.991 \t return: -184.624 \t ep_len: 185.964\n",
      "epoch: 1497 \t loss: -97.555 \t return: -129.332 \t ep_len: 130.538\n",
      "epoch: 1498 \t loss: -121.572 \t return: -146.490 \t ep_len: 147.686\n",
      "epoch: 1499 \t loss: -144.162 \t return: -155.099 \t ep_len: 156.312\n"
     ]
    }
   ],
   "source": [
    "policy = train(env_name = env_wrapped_rew, hidden_sizes = [32], lr = 1e-2, epochs = 1500, batch_size = 5000,a = 0.01,  render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=147, out_features=32, bias=True)\n",
       "  (1): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.88888888888889"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStart(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)   # ← still a dict here\n",
    "        base = self.unwrapped                  # MiniGridEnv\n",
    "\n",
    "        # sample a free floor tile\n",
    "        while True:\n",
    "            x = base.np_random.integers(1, base.width  - 1)\n",
    "            y = base.np_random.integers(1, base.height - 1)\n",
    "            if base.grid.get(x, y) is None:\n",
    "                base.agent_pos = (x, y)\n",
    "                base.agent_dir = base.np_random.integers(0, 4)\n",
    "                break\n",
    "\n",
    "        # regenerate dict-obs; *do not* flatten here\n",
    "        obs = base.gen_obs()\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_base   = LargeSimpleEnv(render_mode=\"human\")\n",
    "env_rs     = RandomStart(env_base)       # randomise first\n",
    "env_flat   = MiniGridFlatImg(env_rs)     # then flatten\n",
    "env_wr     = MiniGridReward(env_flat, goal_states=[(8, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_base   = SimpleEnv(render_mode=\"human\")\n",
    "env_rs     = RandomStart(env_base)       # randomise first\n",
    "env_flat   = MiniGridFlatImg(env_rs)     # then flatten\n",
    "env_standard   = MiniGridReward(env_flat, goal_states=[(8, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to always start at the behinning at the start of each episode\n",
    "# # create the base env *with* a render mode\n",
    "# env_base = SimpleEnv(render_mode=\"human\")      # window pops up\n",
    "# # or render_mode=\"rgb_array\"  # returns an image you can display in a notebook\n",
    "\n",
    "# # wrap exactly as before\n",
    "# env_flat_vis  = MiniGridFlatImg(env_base)\n",
    "# env_wrapped_rew_vis = MiniGridReward(env_flat_vis, goal_states=[(8, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward: -69\n",
      "Episode 2 finished with reward: -16\n",
      "Episode 3 finished with reward: -107\n",
      "Episode 4 finished with reward: -369\n",
      "Episode 5 finished with reward: -88\n",
      "Episode 6 finished with reward: -107\n",
      "Episode 7 finished with reward: -43\n",
      "Episode 8 finished with reward: -6\n",
      "Episode 9 finished with reward: -152\n",
      "Episode 10 finished with reward: 0\n"
     ]
    }
   ],
   "source": [
    "#takes policy network and returns action distribution\n",
    "def get_policy(obs):\n",
    "    logits = policy(obs)\n",
    "    return Categorical(logits = logits)\n",
    "\n",
    "#samples actions from the action distrubution from the policy network\n",
    "def get_action(obs):\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "\n",
    "\n",
    "def play_policy(env, policy, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Play the policy in the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        ep_rews = []\n",
    "        while not done:\n",
    "            action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _, _ = env.step(action)\n",
    "            ep_rews.append(rew)\n",
    "            env.render()\n",
    "        print(f\"Episode {episode + 1} finished with reward: {sum(ep_rews)}\")\n",
    "\n",
    "play_policy(env_standard, policy, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = env.unwrapped.grid.get(1, 8)\n",
    "cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LargeSimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "            self, \n",
    "            size=20, \n",
    "            agent_start_pos=(1, 8), \n",
    "            agent_start_dir=0, \n",
    "            max_steps=1000, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.goal_pos = (8, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the shortest path\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        #create gird\n",
    "        self.grid = Grid(width, height)\n",
    "        #place barrier\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "        #place goal\n",
    "        self.put_obj(Goal(), 8, 1)\n",
    "        #place walls horizontal walls\n",
    "        for i in range(1, width // 2):\n",
    "            self.grid.set(i, width - 4, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 7, Wall())\n",
    "            self.grid.set(i, width - 10, Wall())\n",
    "            self.grid.set(i + width // 2 - 1, width - 13, Wall())\n",
    "        # place vertical walls\n",
    "        for i in range(1, 6 ):            # three vertical walls\n",
    "                self.grid.set(4, i, Wall())\n",
    "                self.grid.set(12, i, Wall())\n",
    "        for i in range(1, 4):\n",
    "            self.grid.set(12, i+ 15, Wall())\n",
    "        #place agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos #check this\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"find the shortest path\"\n",
    "    \n",
    "    def count_states(self):\n",
    "        free_cells = sum(1 for x in range(self.grid.width)\n",
    "                      for y in range(self.grid.height)\n",
    "                      if not self.grid.get(x, y)) * 4\n",
    "        return free_cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image': array([[[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8),\n",
       "  'direction': 0,\n",
       "  'mission': 'find the shortest path'},\n",
       " {})"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large = LargeSimpleEnv(render_mode=\"human\")\n",
    "large.reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
